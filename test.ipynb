{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import CNFFormula\n",
    "from models.rbm import clauseRBM\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# with open(\"wcnfdata/ram_k3_n18.ra0.wcnf\", 'r') as f:\n",
    "#     cnf_str = f.read()\n",
    "\n",
    "# with open(\"wcnfdata/brock200_3.clq.wcnf\", 'r') as f:\n",
    "#     cnf_str = f.read()\n",
    "\n",
    "with open(\"wcnfdata/ram_k3_n11.ra0.wcnf\", 'r') as f:\n",
    "    cnf_str = f.read()\n",
    "\n",
    "formula = CNFFormula(cnf_str)\n",
    "formula.padding()\n",
    "rbm = clauseRBM(formula.max_clause_length, device=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "'''\n",
    "# Global variable to store the result of unit propagation\n",
    "# unit_prop_result = None\n",
    "# last_v = None  # To store the last variable assignments\n",
    "# T_global = None  # To store the clauses\n",
    "\n",
    "\n",
    "def send_for_unit_propagation(v, mu):\n",
    "    global last_v, T_global, T\n",
    "    # Store the last variable assignments and clauses for unit propagation\n",
    "    last_v = v.clone()\n",
    "    T_global = T.clone()\n",
    "\n",
    "def fetch_unit_prop_result():\n",
    "    global last_v, T_global\n",
    "    # Perform unit propagation on the last variable assignments\n",
    "    # and return the new variable assignments\n",
    "    unit_prop_result = []\n",
    "    for v_i in last_v:\n",
    "        v_i_new = v_i.clone()\n",
    "        changed = True\n",
    "        while changed:\n",
    "            changed = False\n",
    "            # For each clause\n",
    "            for clause in T_global:\n",
    "                unassigned_literals = []\n",
    "                clause_satisfied = False\n",
    "                for lit in clause:\n",
    "                    if lit == 0:\n",
    "                        continue  # Skip padding zeros\n",
    "                    var_idx = abs(lit) - 1\n",
    "                    var_value = v_i_new[var_idx].item()\n",
    "                    if var_value == -1:\n",
    "                        unassigned_literals.append(lit)\n",
    "                    elif (lit > 0 and var_value == 1) or (lit < 0 and var_value == 0):\n",
    "                        clause_satisfied = True\n",
    "                        break  # Clause is satisfied\n",
    "                if not clause_satisfied and len(unassigned_literals) == 1:\n",
    "                    # Unit clause found; assign the unit literal\n",
    "                    lit = unassigned_literals[0]\n",
    "                    var_idx = abs(lit) - 1\n",
    "                    v_i_new[var_idx] = 1 if lit > 0 else 0\n",
    "                    changed = True  # Changes occurred; need to recheck\n",
    "        unit_prop_result.append(v_i_new)\n",
    "    unit_prop_result = torch.stack(unit_prop_result)\n",
    "    return unit_prop_result\n",
    "'''\n",
    "\n",
    "def gather_and_count(v, Q, polarity):\n",
    "    c = torch.einsum('bv,ckv->bck', v, Q) # (batch_size, num_clause, max_len_clause)\n",
    "    return c, (((polarity + c) == 2) + ((polarity + c) == -1)).any(dim=-1).sum(dim=-1)\n",
    "\n",
    "def time_remaining(start_time, time_limit):\n",
    "    if time.time() - start_time > time_limit:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def construct_Q(T, num_var, device):\n",
    "    Q = torch.zeros(T.shape[0], T.shape[1], num_var, device=device) #(num_clause, len_clause, num_var)\n",
    "    for clause_idx, clause in enumerate(T):\n",
    "        for lit_idx, lit in enumerate(clause):\n",
    "            if lit != 0:\n",
    "                Q[clause_idx, lit_idx, torch.abs(lit)-1] = 1.0\n",
    "    return Q\n",
    "\n",
    "def rbmsat(formula, W_c, b_c, B, seed, time_limit, upp=100, upw=1, alpha=0.1): # B is batch size, N is the total number of variables\n",
    "    T = torch.tensor(formula.clauses, device=device)\n",
    "    N = formula.num_vars\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Initialize variables\n",
    "    s_max = 0 # initially 0 clauses are satisfied\n",
    "    v = torch.bernoulli(torch.ones(B, N, device=W_c.device) * 0.5)  # sample random inits \n",
    "    # mu = 0.25 * torch.ones_like(v) \n",
    "    # Setup tensors\n",
    "    polarity = torch.sign(T)  # strictly {1, -1}, not 0 (-1 2 -3) -> (-1 1 -1)\n",
    "    Q = construct_Q(T, N, device) # C * K * N    (num_clause, max_len_clause, num_var)\n",
    "    W = torch.einsum('ck,kh->ckh', polarity, W_c) # polarity.unsqueeze(1) * W_c.unsqueeze(2) # element-wise multiplication\n",
    "    # print(W)\n",
    "    b = b_c.repeat([T.shape[0], 1]) + torch.mm((1 - polarity) / 2, W_c)\n",
    "    \n",
    "    # t, d = 1, -1\n",
    "    step = 0\n",
    "    s_max_list = []\n",
    "    best_v = torch.ones(N, device=W_c.device)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        while True:\n",
    "            if time_remaining(start_time, time_limit):\n",
    "                step += 1\n",
    "                # if step == 3:\n",
    "                #     break\n",
    "                #     # print(f\"step {step}, {s_max_list[-1]}\")\n",
    "                \n",
    "                if upp > 0 and d == 0:\n",
    "                   v_u = torch.vstack([v, fetch_unit_prop_result()])\n",
    "                   _, s_u = gather_and_count(v_u, Q, polarity)\n",
    "                   ranks = torch.argsort(s_u)\n",
    "                   v = v_u[ranks[-B:]]\n",
    "                   mu = torch.vstack([mu, mu])[ranks[-B:]]\n",
    "                if upp > 0 and t % upp == 0:\n",
    "                    d = upw\n",
    "                    send_for_unit_propagation(v, mu)\n",
    "                    \n",
    "                c, s = gather_and_count(v, Q, polarity)\n",
    "                s_max_step, idx_max = s.max(dim=0)\n",
    "                # print(s_max_step.item())\n",
    "                # print(f\"v:{v[idx_max]}\")\n",
    "                if s_max_step.item() > s_max:\n",
    "                    s_max = s_max_step.item()\n",
    "                    best_v = v[idx_max].clone()\n",
    "                    s_max_list.append((step, s_max))\n",
    "                    print(f\"new s_max found {s_max} at step {step}\")\n",
    "\n",
    "                h_logits = b + torch.einsum('bck,ckh->bch', c, W) # 'bck,chk->bhk'  bck,ckh->bch\n",
    "                \n",
    "                h = torch.bernoulli(torch.sigmoid(h_logits))\n",
    "                # print(h[0,0:10])\n",
    "                ro = torch.sigmoid(torch.einsum('bch,ckh,ckv->bv', h, W, Q)) # 'bhk,chk,ckv->bv'\n",
    "                # print(f\"ro:{ro}\")\n",
    "                # mu = (1 - alpha) * mu + alpha * ro * (1 - ro)\n",
    "                v = torch.bernoulli(ro)\n",
    "                \n",
    "                # t, d = t + 1, max(d - 1, -1)\n",
    "            else:\n",
    "                s_max_list.append((step, s_max_list[-1][-1]))\n",
    "                break\n",
    "        \n",
    "    return s_max_list, best_v\n",
    "\n",
    "# bath_size = 128\n",
    "# seed = 42\n",
    "# timeout = 300\n",
    "\n",
    "# alpha, upp, upw = 0.1, 100, 1\n",
    "\n",
    "# s_max, best_v = rbmsat(formula, rbm.W, rbm.b, bath_size, seed, timeout, upp, upw, alpha)\n",
    "# print(best_v)\n",
    "# x_ax, y_ax = zip(*s_max)\n",
    "# plt.step(x_ax,y_ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brock200_3.clq.wcnf\n",
      "new s_max found 803 at step 1\n",
      "new s_max found 804 at step 397\n",
      "new s_max found 805 at step 592\n",
      "new s_max found 806 at step 766\n",
      "new s_max found 807 at step 1029\n",
      "new s_max found 811 at step 1560\n",
      "new s_max found 812 at step 6315\n",
      "new s_max found 813 at step 9824\n",
      "new s_max found 815 at step 18707\n",
      "new s_max found 817 at step 21471\n",
      "new s_max found 819 at step 31340\n",
      "new s_max found 821 at step 88583\n",
      "new s_max found 822 at step 89522\n",
      "brock200_3.clq.wcnf, 822\n",
      "maxcut-140-630-0.7-12.wcnf\n",
      "new s_max found 974 at step 1\n",
      "new s_max found 993 at step 2\n",
      "new s_max found 1005 at step 3\n",
      "new s_max found 1011 at step 4\n",
      "new s_max found 1012 at step 6\n",
      "new s_max found 1015 at step 7\n",
      "new s_max found 1016 at step 8\n",
      "new s_max found 1021 at step 9\n",
      "new s_max found 1022 at step 11\n",
      "new s_max found 1025 at step 12\n",
      "new s_max found 1027 at step 13\n",
      "new s_max found 1030 at step 14\n",
      "new s_max found 1032 at step 15\n",
      "new s_max found 1037 at step 16\n",
      "new s_max found 1040 at step 17\n",
      "new s_max found 1041 at step 22\n",
      "new s_max found 1047 at step 25\n",
      "new s_max found 1048 at step 31\n",
      "new s_max found 1049 at step 32\n",
      "new s_max found 1052 at step 35\n",
      "new s_max found 1053 at step 37\n",
      "new s_max found 1054 at step 45\n",
      "new s_max found 1055 at step 61\n",
      "new s_max found 1062 at step 62\n",
      "new s_max found 1065 at step 63\n",
      "new s_max found 1066 at step 64\n",
      "new s_max found 1068 at step 77\n",
      "new s_max found 1069 at step 128\n",
      "new s_max found 1070 at step 129\n",
      "new s_max found 1074 at step 166\n",
      "new s_max found 1075 at step 300\n",
      "new s_max found 1076 at step 351\n",
      "new s_max found 1077 at step 485\n",
      "new s_max found 1078 at step 530\n",
      "new s_max found 1079 at step 564\n",
      "new s_max found 1080 at step 576\n",
      "new s_max found 1082 at step 585\n",
      "new s_max found 1084 at step 1198\n",
      "new s_max found 1085 at step 1481\n",
      "new s_max found 1086 at step 1526\n",
      "new s_max found 1089 at step 2070\n",
      "new s_max found 1090 at step 2449\n",
      "new s_max found 1092 at step 4390\n",
      "new s_max found 1093 at step 5491\n",
      "maxcut-140-630-0.7-12.wcnf, 1093\n",
      "maxcut-140-630-0.7-20.wcnf\n",
      "new s_max found 974 at step 1\n",
      "new s_max found 986 at step 2\n",
      "new s_max found 998 at step 3\n",
      "new s_max found 1005 at step 4\n",
      "new s_max found 1011 at step 5\n",
      "new s_max found 1013 at step 6\n",
      "new s_max found 1015 at step 7\n",
      "new s_max found 1022 at step 8\n",
      "new s_max found 1025 at step 9\n",
      "new s_max found 1029 at step 10\n",
      "new s_max found 1030 at step 12\n",
      "new s_max found 1033 at step 14\n",
      "new s_max found 1034 at step 17\n",
      "new s_max found 1036 at step 19\n",
      "new s_max found 1039 at step 20\n",
      "new s_max found 1041 at step 21\n",
      "new s_max found 1043 at step 23\n",
      "new s_max found 1048 at step 27\n",
      "new s_max found 1049 at step 36\n",
      "new s_max found 1050 at step 41\n",
      "new s_max found 1051 at step 42\n",
      "new s_max found 1053 at step 46\n",
      "new s_max found 1054 at step 50\n",
      "new s_max found 1056 at step 57\n",
      "new s_max found 1057 at step 62\n",
      "new s_max found 1062 at step 63\n",
      "new s_max found 1063 at step 106\n",
      "new s_max found 1064 at step 109\n",
      "new s_max found 1066 at step 114\n",
      "new s_max found 1067 at step 126\n",
      "new s_max found 1068 at step 138\n",
      "new s_max found 1069 at step 156\n",
      "new s_max found 1070 at step 157\n",
      "new s_max found 1071 at step 174\n",
      "new s_max found 1072 at step 177\n",
      "new s_max found 1074 at step 198\n",
      "new s_max found 1077 at step 219\n",
      "new s_max found 1078 at step 220\n",
      "new s_max found 1079 at step 261\n",
      "new s_max found 1080 at step 264\n",
      "new s_max found 1082 at step 269\n",
      "new s_max found 1084 at step 529\n",
      "new s_max found 1085 at step 906\n",
      "new s_max found 1086 at step 2390\n",
      "new s_max found 1087 at step 4575\n",
      "new s_max found 1088 at step 4576\n",
      "new s_max found 1089 at step 7227\n",
      "new s_max found 1090 at step 11758\n",
      "new s_max found 1091 at step 22928\n",
      "maxcut-140-630-0.7-20.wcnf, 1091\n",
      "maxcut-140-630-0.7-28.wcnf\n",
      "new s_max found 989 at step 1\n",
      "new s_max found 991 at step 2\n",
      "new s_max found 999 at step 3\n",
      "new s_max found 1005 at step 4\n",
      "new s_max found 1009 at step 5\n",
      "new s_max found 1011 at step 6\n",
      "new s_max found 1015 at step 7\n",
      "new s_max found 1028 at step 9\n",
      "new s_max found 1033 at step 10\n",
      "new s_max found 1037 at step 12\n",
      "new s_max found 1038 at step 15\n",
      "new s_max found 1041 at step 17\n",
      "new s_max found 1042 at step 21\n",
      "new s_max found 1044 at step 22\n",
      "new s_max found 1049 at step 27\n",
      "new s_max found 1050 at step 28\n",
      "new s_max found 1053 at step 32\n",
      "new s_max found 1055 at step 33\n",
      "new s_max found 1056 at step 48\n",
      "new s_max found 1059 at step 51\n",
      "new s_max found 1061 at step 58\n",
      "new s_max found 1065 at step 59\n",
      "new s_max found 1066 at step 60\n",
      "new s_max found 1067 at step 61\n",
      "new s_max found 1068 at step 95\n",
      "new s_max found 1069 at step 128\n",
      "new s_max found 1071 at step 133\n",
      "new s_max found 1072 at step 135\n",
      "new s_max found 1073 at step 169\n",
      "new s_max found 1074 at step 202\n",
      "new s_max found 1075 at step 203\n",
      "new s_max found 1076 at step 205\n",
      "new s_max found 1078 at step 327\n",
      "new s_max found 1080 at step 339\n",
      "new s_max found 1081 at step 377\n",
      "new s_max found 1083 at step 382\n",
      "new s_max found 1084 at step 443\n",
      "new s_max found 1085 at step 528\n",
      "new s_max found 1087 at step 1158\n",
      "new s_max found 1088 at step 1611\n",
      "new s_max found 1089 at step 2041\n",
      "new s_max found 1090 at step 2434\n",
      "new s_max found 1091 at step 2862\n",
      "new s_max found 1092 at step 2985\n",
      "new s_max found 1093 at step 107587\n",
      "maxcut-140-630-0.7-28.wcnf, 1093\n",
      "maxcut-140-630-0.7-4.wcnf\n",
      "new s_max found 981 at step 1\n",
      "new s_max found 1008 at step 2\n",
      "new s_max found 1009 at step 3\n",
      "new s_max found 1013 at step 4\n",
      "new s_max found 1017 at step 5\n",
      "new s_max found 1021 at step 7\n",
      "new s_max found 1022 at step 10\n",
      "new s_max found 1023 at step 11\n",
      "new s_max found 1025 at step 12\n",
      "new s_max found 1026 at step 14\n",
      "new s_max found 1027 at step 15\n",
      "new s_max found 1029 at step 17\n",
      "new s_max found 1031 at step 18\n",
      "new s_max found 1036 at step 19\n",
      "new s_max found 1038 at step 20\n",
      "new s_max found 1039 at step 23\n",
      "new s_max found 1043 at step 24\n",
      "new s_max found 1046 at step 25\n",
      "new s_max found 1048 at step 28\n",
      "new s_max found 1055 at step 31\n",
      "new s_max found 1056 at step 56\n",
      "new s_max found 1058 at step 59\n",
      "new s_max found 1060 at step 60\n",
      "new s_max found 1061 at step 62\n",
      "new s_max found 1062 at step 63\n",
      "new s_max found 1066 at step 78\n",
      "new s_max found 1067 at step 86\n",
      "new s_max found 1069 at step 109\n",
      "new s_max found 1070 at step 140\n",
      "new s_max found 1071 at step 154\n",
      "new s_max found 1072 at step 156\n",
      "new s_max found 1073 at step 216\n",
      "new s_max found 1074 at step 219\n",
      "new s_max found 1075 at step 290\n",
      "new s_max found 1076 at step 296\n",
      "new s_max found 1078 at step 300\n",
      "new s_max found 1079 at step 309\n",
      "new s_max found 1080 at step 312\n",
      "new s_max found 1082 at step 314\n",
      "new s_max found 1083 at step 331\n",
      "new s_max found 1085 at step 333\n",
      "new s_max found 1087 at step 783\n",
      "new s_max found 1088 at step 914\n",
      "new s_max found 1089 at step 940\n",
      "new s_max found 1090 at step 1772\n",
      "new s_max found 1092 at step 2767\n",
      "new s_max found 1093 at step 6622\n",
      "maxcut-140-630-0.7-4.wcnf, 1093\n",
      "maxcut-140-630-0.7-45.wcnf\n",
      "new s_max found 980 at step 1\n",
      "new s_max found 994 at step 2\n",
      "new s_max found 1005 at step 4\n",
      "new s_max found 1014 at step 5\n",
      "new s_max found 1017 at step 6\n",
      "new s_max found 1021 at step 7\n",
      "new s_max found 1024 at step 9\n",
      "new s_max found 1034 at step 10\n",
      "new s_max found 1035 at step 12\n",
      "new s_max found 1041 at step 13\n",
      "new s_max found 1045 at step 14\n",
      "new s_max found 1047 at step 15\n",
      "new s_max found 1048 at step 22\n",
      "new s_max found 1049 at step 25\n",
      "new s_max found 1051 at step 40\n",
      "new s_max found 1052 at step 41\n",
      "new s_max found 1053 at step 44\n",
      "new s_max found 1056 at step 49\n",
      "new s_max found 1057 at step 53\n",
      "new s_max found 1058 at step 55\n",
      "new s_max found 1059 at step 56\n",
      "new s_max found 1060 at step 57\n",
      "new s_max found 1062 at step 102\n",
      "new s_max found 1063 at step 104\n",
      "new s_max found 1064 at step 117\n",
      "new s_max found 1065 at step 129\n",
      "new s_max found 1066 at step 133\n",
      "new s_max found 1069 at step 142\n",
      "new s_max found 1070 at step 240\n",
      "new s_max found 1072 at step 260\n",
      "new s_max found 1073 at step 287\n",
      "new s_max found 1074 at step 350\n",
      "new s_max found 1075 at step 388\n",
      "new s_max found 1076 at step 445\n",
      "new s_max found 1077 at step 766\n",
      "new s_max found 1079 at step 782\n",
      "new s_max found 1080 at step 783\n",
      "new s_max found 1083 at step 1031\n",
      "new s_max found 1084 at step 1200\n",
      "new s_max found 1086 at step 1635\n",
      "new s_max found 1087 at step 2140\n",
      "new s_max found 1088 at step 3542\n",
      "new s_max found 1089 at step 4841\n",
      "new s_max found 1090 at step 6651\n",
      "new s_max found 1091 at step 15087\n",
      "new s_max found 1092 at step 17583\n",
      "maxcut-140-630-0.7-45.wcnf, 1092\n",
      "maxcut-140-630-0.7-6.wcnf\n",
      "new s_max found 975 at step 1\n",
      "new s_max found 996 at step 2\n",
      "new s_max found 1000 at step 3\n",
      "new s_max found 1007 at step 4\n",
      "new s_max found 1008 at step 5\n",
      "new s_max found 1013 at step 6\n",
      "new s_max found 1018 at step 7\n",
      "new s_max found 1025 at step 8\n",
      "new s_max found 1032 at step 9\n",
      "new s_max found 1037 at step 10\n",
      "new s_max found 1041 at step 15\n",
      "new s_max found 1044 at step 19\n",
      "new s_max found 1045 at step 20\n",
      "new s_max found 1047 at step 30\n",
      "new s_max found 1048 at step 34\n",
      "new s_max found 1051 at step 36\n",
      "new s_max found 1052 at step 37\n",
      "new s_max found 1054 at step 47\n",
      "new s_max found 1055 at step 48\n",
      "new s_max found 1059 at step 52\n",
      "new s_max found 1064 at step 63\n",
      "new s_max found 1065 at step 68\n",
      "new s_max found 1066 at step 70\n",
      "new s_max found 1067 at step 116\n",
      "new s_max found 1069 at step 117\n",
      "new s_max found 1070 at step 125\n",
      "new s_max found 1073 at step 126\n",
      "new s_max found 1076 at step 167\n",
      "new s_max found 1077 at step 179\n",
      "new s_max found 1078 at step 214\n",
      "new s_max found 1080 at step 216\n",
      "new s_max found 1083 at step 292\n",
      "new s_max found 1086 at step 410\n",
      "new s_max found 1087 at step 448\n",
      "new s_max found 1089 at step 464\n",
      "new s_max found 1090 at step 470\n",
      "new s_max found 1091 at step 475\n",
      "new s_max found 1094 at step 698\n",
      "new s_max found 1095 at step 1441\n",
      "new s_max found 1097 at step 2106\n",
      "new s_max found 1098 at step 4602\n",
      "maxcut-140-630-0.7-6.wcnf, 1098\n",
      "maxcut-140-630-0.8-1.wcnf\n",
      "new s_max found 970 at step 1\n",
      "new s_max found 990 at step 2\n",
      "new s_max found 1000 at step 3\n",
      "new s_max found 1012 at step 4\n",
      "new s_max found 1017 at step 6\n",
      "new s_max found 1024 at step 8\n",
      "new s_max found 1026 at step 10\n",
      "new s_max found 1028 at step 11\n",
      "new s_max found 1031 at step 13\n",
      "new s_max found 1034 at step 16\n",
      "new s_max found 1035 at step 17\n",
      "new s_max found 1036 at step 19\n",
      "new s_max found 1038 at step 20\n",
      "new s_max found 1044 at step 21\n",
      "new s_max found 1045 at step 29\n",
      "new s_max found 1049 at step 30\n",
      "new s_max found 1050 at step 40\n",
      "new s_max found 1051 at step 42\n",
      "new s_max found 1055 at step 43\n",
      "new s_max found 1056 at step 80\n",
      "new s_max found 1057 at step 85\n",
      "new s_max found 1058 at step 92\n",
      "new s_max found 1059 at step 112\n",
      "new s_max found 1061 at step 117\n",
      "new s_max found 1062 at step 121\n",
      "new s_max found 1064 at step 122\n",
      "new s_max found 1066 at step 127\n",
      "new s_max found 1068 at step 146\n",
      "new s_max found 1072 at step 148\n",
      "new s_max found 1074 at step 220\n",
      "new s_max found 1077 at step 322\n",
      "new s_max found 1079 at step 491\n",
      "new s_max found 1082 at step 495\n",
      "new s_max found 1083 at step 498\n",
      "new s_max found 1084 at step 637\n",
      "new s_max found 1085 at step 638\n",
      "new s_max found 1086 at step 892\n",
      "new s_max found 1087 at step 1149\n",
      "new s_max found 1089 at step 2271\n",
      "new s_max found 1090 at step 2927\n",
      "new s_max found 1091 at step 10512\n",
      "new s_max found 1092 at step 16219\n",
      "new s_max found 1093 at step 23153\n",
      "maxcut-140-630-0.8-1.wcnf, 1093\n",
      "maxcut-140-630-0.8-18.wcnf\n",
      "new s_max found 973 at step 1\n",
      "new s_max found 992 at step 2\n",
      "new s_max found 996 at step 3\n",
      "new s_max found 1009 at step 4\n",
      "new s_max found 1016 at step 5\n",
      "new s_max found 1020 at step 6\n",
      "new s_max found 1021 at step 7\n",
      "new s_max found 1024 at step 9\n",
      "new s_max found 1025 at step 10\n",
      "new s_max found 1026 at step 11\n",
      "new s_max found 1034 at step 12\n",
      "new s_max found 1035 at step 13\n",
      "new s_max found 1038 at step 20\n",
      "new s_max found 1040 at step 21\n",
      "new s_max found 1041 at step 22\n",
      "new s_max found 1043 at step 23\n",
      "new s_max found 1050 at step 24\n",
      "new s_max found 1052 at step 40\n",
      "new s_max found 1053 at step 42\n",
      "new s_max found 1056 at step 49\n",
      "new s_max found 1057 at step 53\n",
      "new s_max found 1062 at step 63\n",
      "new s_max found 1063 at step 73\n",
      "new s_max found 1067 at step 74\n",
      "new s_max found 1068 at step 109\n",
      "new s_max found 1069 at step 166\n",
      "new s_max found 1070 at step 167\n",
      "new s_max found 1072 at step 169\n",
      "new s_max found 1073 at step 209\n",
      "new s_max found 1075 at step 212\n",
      "new s_max found 1076 at step 234\n",
      "new s_max found 1079 at step 236\n",
      "new s_max found 1080 at step 328\n",
      "new s_max found 1081 at step 329\n",
      "new s_max found 1082 at step 343\n",
      "new s_max found 1084 at step 345\n",
      "new s_max found 1085 at step 544\n",
      "new s_max found 1086 at step 609\n",
      "new s_max found 1087 at step 624\n",
      "new s_max found 1088 at step 671\n",
      "new s_max found 1089 at step 775\n",
      "new s_max found 1090 at step 783\n",
      "new s_max found 1091 at step 1098\n",
      "new s_max found 1092 at step 1106\n",
      "new s_max found 1093 at step 2029\n",
      "new s_max found 1094 at step 5695\n",
      "new s_max found 1095 at step 9936\n",
      "maxcut-140-630-0.8-18.wcnf, 1095\n",
      "maxcut-140-630-0.8-26.wcnf\n",
      "new s_max found 968 at step 1\n",
      "new s_max found 995 at step 2\n",
      "new s_max found 998 at step 4\n",
      "new s_max found 1004 at step 5\n",
      "new s_max found 1006 at step 6\n",
      "new s_max found 1013 at step 7\n",
      "new s_max found 1018 at step 8\n",
      "new s_max found 1023 at step 9\n",
      "new s_max found 1024 at step 10\n",
      "new s_max found 1028 at step 11\n",
      "new s_max found 1032 at step 12\n",
      "new s_max found 1035 at step 13\n",
      "new s_max found 1036 at step 14\n",
      "new s_max found 1040 at step 22\n",
      "new s_max found 1043 at step 23\n",
      "new s_max found 1044 at step 29\n",
      "new s_max found 1045 at step 31\n",
      "new s_max found 1046 at step 41\n",
      "new s_max found 1048 at step 42\n",
      "new s_max found 1049 at step 44\n",
      "new s_max found 1050 at step 46\n",
      "new s_max found 1054 at step 47\n",
      "new s_max found 1059 at step 49\n",
      "new s_max found 1061 at step 71\n",
      "new s_max found 1063 at step 110\n",
      "new s_max found 1065 at step 119\n",
      "new s_max found 1066 at step 129\n",
      "new s_max found 1068 at step 132\n",
      "new s_max found 1071 at step 146\n",
      "new s_max found 1072 at step 198\n",
      "new s_max found 1074 at step 199\n",
      "new s_max found 1075 at step 357\n",
      "new s_max found 1076 at step 478\n",
      "new s_max found 1077 at step 549\n",
      "new s_max found 1078 at step 555\n",
      "new s_max found 1079 at step 574\n",
      "new s_max found 1080 at step 831\n",
      "new s_max found 1081 at step 832\n",
      "new s_max found 1082 at step 1058\n",
      "new s_max found 1083 at step 1306\n",
      "new s_max found 1084 at step 1317\n",
      "new s_max found 1085 at step 1887\n",
      "new s_max found 1086 at step 2017\n",
      "new s_max found 1088 at step 2026\n",
      "new s_max found 1089 at step 4010\n",
      "new s_max found 1090 at step 6154\n",
      "new s_max found 1091 at step 67754\n",
      "maxcut-140-630-0.8-26.wcnf, 1091\n",
      "maxcut-140-630-0.8-39.wcnf\n",
      "new s_max found 972 at step 1\n",
      "new s_max found 988 at step 2\n",
      "new s_max found 997 at step 3\n",
      "new s_max found 1003 at step 4\n",
      "new s_max found 1012 at step 5\n",
      "new s_max found 1016 at step 6\n",
      "new s_max found 1022 at step 8\n",
      "new s_max found 1027 at step 10\n",
      "new s_max found 1029 at step 12\n",
      "new s_max found 1032 at step 13\n",
      "new s_max found 1037 at step 14\n",
      "new s_max found 1039 at step 15\n",
      "new s_max found 1040 at step 16\n",
      "new s_max found 1041 at step 18\n",
      "new s_max found 1043 at step 22\n",
      "new s_max found 1044 at step 26\n",
      "new s_max found 1045 at step 27\n",
      "new s_max found 1046 at step 28\n",
      "new s_max found 1049 at step 32\n",
      "new s_max found 1050 at step 39\n",
      "new s_max found 1053 at step 47\n",
      "new s_max found 1054 at step 52\n",
      "new s_max found 1055 at step 56\n",
      "new s_max found 1057 at step 58\n",
      "new s_max found 1059 at step 59\n",
      "new s_max found 1061 at step 65\n",
      "new s_max found 1062 at step 79\n",
      "new s_max found 1063 at step 138\n",
      "new s_max found 1064 at step 141\n",
      "new s_max found 1066 at step 142\n",
      "new s_max found 1070 at step 150\n",
      "new s_max found 1071 at step 184\n",
      "new s_max found 1072 at step 212\n",
      "new s_max found 1075 at step 258\n",
      "new s_max found 1077 at step 360\n",
      "new s_max found 1079 at step 430\n",
      "new s_max found 1081 at step 434\n",
      "new s_max found 1082 at step 471\n",
      "new s_max found 1083 at step 1466\n",
      "new s_max found 1084 at step 2325\n",
      "new s_max found 1085 at step 4091\n",
      "new s_max found 1086 at step 8472\n",
      "new s_max found 1087 at step 47978\n",
      "new s_max found 1088 at step 153152\n",
      "maxcut-140-630-0.8-39.wcnf, 1088\n",
      "ram_k3_n11.ra0.wcnf\n",
      "new s_max found 483 at step 1\n",
      "new s_max found 485 at step 2\n",
      "new s_max found 486 at step 4\n",
      "new s_max found 487 at step 34082\n",
      "ram_k3_n11.ra0.wcnf, 487\n",
      "ram_k3_n14.ra0.wcnf\n",
      "new s_max found 1323 at step 1\n",
      "new s_max found 1327 at step 2\n",
      "new s_max found 1329 at step 3\n",
      "new s_max found 1330 at step 4\n",
      "new s_max found 1332 at step 14\n",
      "new s_max found 1333 at step 15\n",
      "ram_k3_n14.ra0.wcnf, 1333\n",
      "ram_k3_n15.ra0.wcnf\n",
      "new s_max found 1766 at step 1\n",
      "new s_max found 1770 at step 2\n",
      "new s_max found 1773 at step 3\n",
      "new s_max found 1776 at step 5\n",
      "new s_max found 1777 at step 54\n",
      "ram_k3_n15.ra0.wcnf, 1777\n",
      "ram_k3_n18.ra0.wcnf\n",
      "new s_max found 3761 at step 1\n",
      "new s_max found 3763 at step 2\n",
      "new s_max found 3764 at step 3\n",
      "new s_max found 3766 at step 5\n",
      "new s_max found 3767 at step 6\n",
      "new s_max found 3769 at step 8\n",
      "new s_max found 3770 at step 10\n",
      "new s_max found 3772 at step 13\n",
      "new s_max found 3774 at step 17\n",
      "new s_max found 3775 at step 41\n",
      "new s_max found 3777 at step 90\n",
      "ram_k3_n18.ra0.wcnf, 3777\n",
      "scpcyc09_maxsat.wcnf\n",
      "new s_max found 5522 at step 1\n",
      "new s_max found 5581 at step 2\n",
      "new s_max found 5596 at step 3\n",
      "new s_max found 5606 at step 4\n",
      "new s_max found 5615 at step 5\n",
      "new s_max found 5631 at step 6\n",
      "new s_max found 5649 at step 8\n",
      "new s_max found 5651 at step 11\n",
      "new s_max found 5653 at step 12\n",
      "new s_max found 5657 at step 19\n",
      "new s_max found 5661 at step 22\n",
      "new s_max found 5669 at step 29\n",
      "new s_max found 5671 at step 117\n",
      "new s_max found 5673 at step 174\n",
      "new s_max found 5674 at step 182\n",
      "new s_max found 5675 at step 318\n",
      "new s_max found 5676 at step 788\n",
      "new s_max found 5679 at step 790\n",
      "new s_max found 5681 at step 1141\n",
      "new s_max found 5690 at step 2450\n",
      "new s_max found 5691 at step 5415\n",
      "scpcyc09_maxsat.wcnf, 5691\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "bath_size = 128\n",
    "seed = 42\n",
    "timeout = 300\n",
    "\n",
    "alpha, upp, upw = 0.1, 100, 1\n",
    "\n",
    "best_num_satisfied_list = []\n",
    "# test all files in ../wcnfdata\n",
    "for file in os.listdir(\"./wcnfdata\"):\n",
    "    print(file)\n",
    "    with open(os.path.join(\"./wcnfdata\", file), 'r') as f:\n",
    "        formula_str = f.read()\n",
    "    \n",
    "    formula = CNFFormula(formula_str)\n",
    "    formula.padding()\n",
    "    rbm = clauseRBM(formula.max_clause_length, device=device)\n",
    "    s_max, best_v = rbmsat(formula, rbm.W, rbm.b, bath_size, seed, timeout, upp, upw, alpha)\n",
    "    \n",
    "    best_num_satisfied_list.append(s_max[-1][-1])\n",
    "\n",
    "    print(f\"{file}, {best_num_satisfied_list[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[822,\n",
       " 1093,\n",
       " 1091,\n",
       " 1093,\n",
       " 1093,\n",
       " 1092,\n",
       " 1098,\n",
       " 1093,\n",
       " 1095,\n",
       " 1091,\n",
       " 1088,\n",
       " 487,\n",
       " 1333,\n",
       " 1777,\n",
       " 3777,\n",
       " 5691]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_num_satisfied_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brock200_3.clq.wcnf\n",
      "maxcut-140-630-0.7-12.wcnf\n",
      "maxcut-140-630-0.7-20.wcnf\n",
      "maxcut-140-630-0.7-28.wcnf\n",
      "maxcut-140-630-0.7-4.wcnf\n",
      "maxcut-140-630-0.7-45.wcnf\n",
      "maxcut-140-630-0.7-6.wcnf\n",
      "maxcut-140-630-0.8-1.wcnf\n",
      "maxcut-140-630-0.8-18.wcnf\n",
      "maxcut-140-630-0.8-26.wcnf\n",
      "maxcut-140-630-0.8-39.wcnf\n",
      "ram_k3_n11.ra0.wcnf\n",
      "ram_k3_n14.ra0.wcnf\n",
      "ram_k3_n15.ra0.wcnf\n",
      "ram_k3_n18.ra0.wcnf\n",
      "scpcyc09_maxsat.wcnf\n",
      "[216, 167, 169, 167, 167, 168, 162, 165, 163, 167, 170, 8, 32, 43, 99, 1221]\n"
     ]
    }
   ],
   "source": [
    "best_num_sat = [822, 1093, 1091, 1093, 1093, 1092, 1098, 1093, 1095, 1091, 1088, 487, 1333, 1777, 3777, 5691]\n",
    "import os\n",
    "from utils import CNFFormula\n",
    "best_possible = []\n",
    "# test all files in ../wcnfdata\n",
    "for file in os.listdir(\"./wcnfdata\"):\n",
    "    print(file)\n",
    "    with open(os.path.join(\"./wcnfdata\", file), 'r') as f:\n",
    "        formula_str = f.read()\n",
    "    \n",
    "    formula = CNFFormula(formula_str)\n",
    "    best_possible.append(formula.num_clauses)\n",
    "\n",
    "diff = [0] * len(best_num_sat)\n",
    "for i in range(len(diff)):\n",
    "    diff[i] = best_possible[i] - best_num_sat[i]\n",
    "\n",
    "best_sol = [214, 167, 168, 167, 167, 168, 162, 165, 163, 167, 169, 7, 21, 30, 60, 823]\n",
    "print(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9077161583166862\n"
     ]
    }
   ],
   "source": [
    "ours = [216, 167, 169, 167, 167, 168, 162, 165, 163, 167, 170, 8, 32, 43, 99, 1221]\n",
    "satS = [214, 167, 169, 167, 167, 168, 162, 165, 163, 167, 169, 7, 21, 30, 60, 823]\n",
    "best = [214, 167, 168, 167, 167, 168, 162, 165, 163, 167, 169, 7, 21, 30, 60, 823]\n",
    "\n",
    "s_j = []\n",
    "for j in range(len(ours)):\n",
    "    s_j.append((best[j] + 1) / (ours[j] + 1))\n",
    "\n",
    "score = sum(s_j) / len(s_j)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assignment test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2682\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1.,\n",
      "         0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from utils import CNFFormula\n",
    "from main import count_satisfied_clauses\n",
    "v = torch.tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
    "        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1.,\n",
    "        0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
    "        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
    "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
    "        1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
    "\n",
    "# v_str = \"111111101111111001111111111111010111111111100011011100111011101101110001111111101111011011111111011110011111111111101110110110011110010\"\n",
    "\n",
    "# v = torch.tensor([[int(i) for i in v_str]])\n",
    "\n",
    "with open(\"wcnfdata/uaq-min-nr-nr50-np400-rpp5-nc0-rs0-t0-plb100-n9.wcnf\", 'r') as f:\n",
    "    cnf_str = f.read()\n",
    "formula = CNFFormula(cnf_str)\n",
    "\n",
    "best_v, best_num_satisfied = count_satisfied_clauses(\n",
    "    formula, \n",
    "    v\n",
    ")\n",
    "print(best_num_satisfied)\n",
    "print(best_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
      "         0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
      "         1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
      "         0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "v_str = \"1 -2 -3 -4 -5 -6 7 -8 -9 -10 -11 12 -13 -14 -15 -16 17 -18 -19 20 -21 -22 23 -24 -25 -26 27 -28 -29 30 -31 -32 -33 -34 -35 -36 -37 -38 39 -40 -41 42 -43 -44 45 46 -47 48 49 -50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 -155 156 157 -158 159 160 161 162 163 164 165 166 167 -168 169 170 171 172 173 174 175 -176 177 178 179 180 181 182 183 184 185 -186 187 188 189 190 191 192 -193 194 195 196 197 198 199 200 -201 202 -203 204 205 206 -207 208 209 210 211 212 213 -214 215 -216 -217 218 219 220 221 222 223 -224 225 -226 227 228 229 230 231 232 233 234 235 -236 237 238 -239 -240 241 242 243 244 245 246 -247 248 249 250 251 252 253 254 255 256 257 258 259 260 -261 -262 -263 264 265 266 267 268 269 270 271 272 -273 274 275 276 277 278 279 280 281 -282 283 284 285 286 287 288 289 290 -291 292 -293 294 295 296 -297 -298 299 -300 301 302 303 304 305 306 307 -308 -309 310 311 312 -313 -314 -315 316 317 318 319 320 321 322 323 324 325 326 -327 328 329 330 -331 332 333 334 335 336 337 338 339 340 341 342 343 -344 345 346 347 348 349 350 351 352 -353 354 355 356 357 -358 359 -360 361 362 363 364 -365 -366 367 368 369 -370 -371 372 373 374 375 376 -377 -378 -379 380 381 382 383 384 385 386 387 -388 389 390 391 392 393 -394 -395 -396 -397 398 -399 400 401 402 403 404 405 406 407 408 409 410 411 -412 413 -414 415 416 417 418 419 420 -421 422 423 424 -425 426 427 428 429 430 431 -432 -433 434 -435 -436 437 438 439 -440 441 442 443 444 -445 -446 447 -448 -449 -450\"\n",
    "v_list = v_str.split()\n",
    "v_temp = []\n",
    "for var in v_list:\n",
    "    if int(var) > 0:\n",
    "        v_temp.append(1.0)\n",
    "    else:\n",
    "        v_temp.append(0.0)\n",
    "\n",
    "v_torch = torch.tensor([v_temp])\n",
    "print(v_torch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brock200_3.clq.wcnf\n",
      "Step 100, Best: 813, current: 810, formula size: 1038, Elapsed Time: 16.84s\n",
      "Step 200, Best: 818, current: 803, formula size: 1038, Elapsed Time: 29.70s\n",
      "Step 300, Best: 818, current: 801, formula size: 1038, Elapsed Time: 44.76s\n",
      "Step 400, Best: 818, current: 801, formula size: 1038, Elapsed Time: 60.85s\n",
      "Step 500, Best: 818, current: 806, formula size: 1038, Elapsed Time: 77.98s\n",
      "Step 600, Best: 818, current: 802, formula size: 1038, Elapsed Time: 97.43s\n",
      "Step 700, Best: 818, current: 805, formula size: 1038, Elapsed Time: 110.33s\n",
      "Step 800, Best: 818, current: 802, formula size: 1038, Elapsed Time: 123.68s\n",
      "Step 900, Best: 818, current: 806, formula size: 1038, Elapsed Time: 136.62s\n",
      "Step 1000, Best: 818, current: 803, formula size: 1038, Elapsed Time: 149.38s\n",
      "Step 1100, Best: 818, current: 808, formula size: 1038, Elapsed Time: 162.05s\n",
      "Step 1200, Best: 818, current: 796, formula size: 1038, Elapsed Time: 174.84s\n",
      "Step 1300, Best: 818, current: 804, formula size: 1038, Elapsed Time: 187.38s\n",
      "Step 1400, Best: 818, current: 804, formula size: 1038, Elapsed Time: 199.90s\n",
      "Step 1500, Best: 818, current: 804, formula size: 1038, Elapsed Time: 212.75s\n",
      "Step 1600, Best: 818, current: 797, formula size: 1038, Elapsed Time: 226.16s\n",
      "Step 1700, Best: 818, current: 805, formula size: 1038, Elapsed Time: 239.29s\n",
      "Step 1800, Best: 818, current: 799, formula size: 1038, Elapsed Time: 252.24s\n",
      "Step 1900, Best: 818, current: 800, formula size: 1038, Elapsed Time: 276.34s\n",
      "Step 2000, Best: 818, current: 798, formula size: 1038, Elapsed Time: 302.01s\n",
      "Time limit reached: 300 seconds\n",
      "brock200_3.clq.wcnf, 818\n",
      "data.135.wcnf\n",
      "Step 100, Best: 3016, current: 2608, formula size: 3150, Elapsed Time: 85.12s\n",
      "Step 200, Best: 3016, current: 2605, formula size: 3150, Elapsed Time: 128.71s\n",
      "Step 300, Best: 3016, current: 2602, formula size: 3150, Elapsed Time: 177.22s\n",
      "Step 400, Best: 3016, current: 2605, formula size: 3150, Elapsed Time: 222.67s\n",
      "Step 500, Best: 3016, current: 2608, formula size: 3150, Elapsed Time: 266.49s\n",
      "Step 600, Best: 3016, current: 2607, formula size: 3150, Elapsed Time: 311.06s\n",
      "Time limit reached: 300 seconds\n",
      "data.135.wcnf, 3016\n",
      "data.243.wcnf\n",
      "Step 100, Best: 9801, current: 8712, formula size: 10044, Elapsed Time: 205.07s\n",
      "Time limit reached: 300 seconds\n",
      "data.243.wcnf, 9801\n",
      "data.405.wcnf\n",
      "Time limit reached: 300 seconds\n",
      "data.405.wcnf, 27270\n",
      "data.729.wcnf\n",
      "Time limit reached: 300 seconds\n",
      "data.729.wcnf, 88452\n",
      "hamming10-4.clq.wcnf\n",
      "Time limit reached: 300 seconds\n",
      "hamming10-4.clq.wcnf, 88575\n",
      "maxcut-140-630-0.7-12.wcnf\n",
      "Step 100, Best: 1074, current: 1049, formula size: 1260, Elapsed Time: 19.77s\n",
      "Step 200, Best: 1074, current: 1051, formula size: 1260, Elapsed Time: 35.73s\n",
      "Step 300, Best: 1074, current: 1047, formula size: 1260, Elapsed Time: 51.82s\n",
      "Step 400, Best: 1074, current: 1044, formula size: 1260, Elapsed Time: 68.05s\n",
      "Step 500, Best: 1074, current: 1049, formula size: 1260, Elapsed Time: 84.37s\n",
      "Step 600, Best: 1074, current: 1042, formula size: 1260, Elapsed Time: 100.39s\n",
      "Step 700, Best: 1074, current: 1045, formula size: 1260, Elapsed Time: 116.11s\n",
      "Step 800, Best: 1074, current: 1050, formula size: 1260, Elapsed Time: 132.74s\n",
      "Step 900, Best: 1074, current: 1047, formula size: 1260, Elapsed Time: 154.22s\n",
      "Step 1000, Best: 1074, current: 1044, formula size: 1260, Elapsed Time: 174.82s\n",
      "Step 1100, Best: 1074, current: 1045, formula size: 1260, Elapsed Time: 195.13s\n",
      "Step 1200, Best: 1075, current: 1043, formula size: 1260, Elapsed Time: 214.91s\n",
      "Step 1300, Best: 1075, current: 1045, formula size: 1260, Elapsed Time: 235.70s\n",
      "Step 1400, Best: 1075, current: 1041, formula size: 1260, Elapsed Time: 254.55s\n",
      "Step 1500, Best: 1075, current: 1050, formula size: 1260, Elapsed Time: 270.32s\n",
      "Step 1600, Best: 1075, current: 1050, formula size: 1260, Elapsed Time: 285.83s\n",
      "Step 1700, Best: 1075, current: 1043, formula size: 1260, Elapsed Time: 301.28s\n",
      "Time limit reached: 300 seconds\n",
      "maxcut-140-630-0.7-12.wcnf, 1075\n",
      "maxcut-140-630-0.7-20.wcnf\n",
      "Step 100, Best: 1072, current: 1044, formula size: 1260, Elapsed Time: 18.98s\n",
      "Step 200, Best: 1072, current: 1040, formula size: 1260, Elapsed Time: 34.23s\n",
      "Step 300, Best: 1073, current: 1039, formula size: 1260, Elapsed Time: 49.56s\n",
      "Step 400, Best: 1073, current: 1042, formula size: 1260, Elapsed Time: 64.79s\n",
      "Step 500, Best: 1073, current: 1043, formula size: 1260, Elapsed Time: 79.99s\n",
      "Step 600, Best: 1073, current: 1040, formula size: 1260, Elapsed Time: 95.21s\n",
      "Step 700, Best: 1073, current: 1039, formula size: 1260, Elapsed Time: 110.46s\n",
      "Step 800, Best: 1073, current: 1041, formula size: 1260, Elapsed Time: 125.68s\n",
      "Step 900, Best: 1073, current: 1046, formula size: 1260, Elapsed Time: 140.94s\n",
      "Step 1000, Best: 1073, current: 1042, formula size: 1260, Elapsed Time: 156.13s\n",
      "Step 1100, Best: 1073, current: 1040, formula size: 1260, Elapsed Time: 171.56s\n",
      "Step 1200, Best: 1074, current: 1041, formula size: 1260, Elapsed Time: 187.17s\n",
      "Step 1300, Best: 1074, current: 1037, formula size: 1260, Elapsed Time: 202.43s\n",
      "Step 1400, Best: 1074, current: 1040, formula size: 1260, Elapsed Time: 217.64s\n",
      "Step 1500, Best: 1074, current: 1043, formula size: 1260, Elapsed Time: 232.85s\n",
      "Step 1600, Best: 1074, current: 1040, formula size: 1260, Elapsed Time: 248.06s\n",
      "Step 1700, Best: 1074, current: 1039, formula size: 1260, Elapsed Time: 263.25s\n",
      "Step 1800, Best: 1074, current: 1043, formula size: 1260, Elapsed Time: 278.49s\n",
      "Step 1900, Best: 1074, current: 1041, formula size: 1260, Elapsed Time: 293.77s\n",
      "Time limit reached: 300 seconds\n",
      "maxcut-140-630-0.7-20.wcnf, 1074\n",
      "maxcut-140-630-0.7-28.wcnf\n",
      "Step 100, Best: 1061, current: 1028, formula size: 1260, Elapsed Time: 19.29s\n",
      "Step 200, Best: 1065, current: 1040, formula size: 1260, Elapsed Time: 34.89s\n",
      "Step 300, Best: 1066, current: 1042, formula size: 1260, Elapsed Time: 50.54s\n",
      "Step 400, Best: 1066, current: 1046, formula size: 1260, Elapsed Time: 66.16s\n",
      "Step 500, Best: 1066, current: 1043, formula size: 1260, Elapsed Time: 81.67s\n",
      "Step 600, Best: 1066, current: 1041, formula size: 1260, Elapsed Time: 97.33s\n",
      "Step 700, Best: 1066, current: 1034, formula size: 1260, Elapsed Time: 113.23s\n",
      "Step 800, Best: 1066, current: 1042, formula size: 1260, Elapsed Time: 129.33s\n",
      "Step 900, Best: 1066, current: 1036, formula size: 1260, Elapsed Time: 144.93s\n",
      "Step 1000, Best: 1066, current: 1036, formula size: 1260, Elapsed Time: 160.48s\n",
      "Step 1100, Best: 1066, current: 1035, formula size: 1260, Elapsed Time: 176.29s\n",
      "Step 1200, Best: 1066, current: 1049, formula size: 1260, Elapsed Time: 191.91s\n",
      "Step 1300, Best: 1066, current: 1040, formula size: 1260, Elapsed Time: 207.44s\n",
      "Step 1400, Best: 1067, current: 1036, formula size: 1260, Elapsed Time: 223.18s\n",
      "Step 1500, Best: 1067, current: 1037, formula size: 1260, Elapsed Time: 238.79s\n",
      "Step 1600, Best: 1067, current: 1039, formula size: 1260, Elapsed Time: 254.39s\n",
      "Step 1700, Best: 1067, current: 1035, formula size: 1260, Elapsed Time: 269.97s\n",
      "Step 1800, Best: 1067, current: 1045, formula size: 1260, Elapsed Time: 285.54s\n",
      "Step 1900, Best: 1067, current: 1040, formula size: 1260, Elapsed Time: 301.38s\n",
      "Time limit reached: 300 seconds\n",
      "maxcut-140-630-0.7-28.wcnf, 1067\n",
      "maxcut-140-630-0.7-4.wcnf\n",
      "Step 100, Best: 1062, current: 1035, formula size: 1260, Elapsed Time: 19.13s\n",
      "Step 200, Best: 1062, current: 1037, formula size: 1260, Elapsed Time: 34.59s\n",
      "Step 300, Best: 1062, current: 1039, formula size: 1260, Elapsed Time: 50.31s\n",
      "Step 400, Best: 1064, current: 1034, formula size: 1260, Elapsed Time: 66.42s\n",
      "Step 500, Best: 1064, current: 1038, formula size: 1260, Elapsed Time: 82.13s\n",
      "Step 600, Best: 1064, current: 1039, formula size: 1260, Elapsed Time: 97.58s\n",
      "Step 700, Best: 1064, current: 1038, formula size: 1260, Elapsed Time: 113.12s\n",
      "Step 800, Best: 1064, current: 1037, formula size: 1260, Elapsed Time: 128.62s\n",
      "Step 900, Best: 1064, current: 1031, formula size: 1260, Elapsed Time: 144.08s\n",
      "Step 1000, Best: 1064, current: 1041, formula size: 1260, Elapsed Time: 159.57s\n",
      "Step 1100, Best: 1064, current: 1041, formula size: 1260, Elapsed Time: 175.02s\n",
      "Step 1200, Best: 1064, current: 1040, formula size: 1260, Elapsed Time: 190.54s\n",
      "Step 1300, Best: 1064, current: 1037, formula size: 1260, Elapsed Time: 205.99s\n",
      "Step 1400, Best: 1064, current: 1039, formula size: 1260, Elapsed Time: 221.49s\n",
      "Step 1500, Best: 1066, current: 1037, formula size: 1260, Elapsed Time: 237.01s\n",
      "Step 1600, Best: 1066, current: 1037, formula size: 1260, Elapsed Time: 252.51s\n",
      "Step 1700, Best: 1066, current: 1037, formula size: 1260, Elapsed Time: 268.02s\n",
      "Step 1800, Best: 1066, current: 1038, formula size: 1260, Elapsed Time: 283.57s\n",
      "Step 1900, Best: 1066, current: 1035, formula size: 1260, Elapsed Time: 299.89s\n",
      "Time limit reached: 300 seconds\n",
      "maxcut-140-630-0.7-4.wcnf, 1066\n",
      "maxcut-140-630-0.7-45.wcnf\n",
      "Step 100, Best: 1058, current: 1033, formula size: 1260, Elapsed Time: 20.52s\n",
      "Step 200, Best: 1058, current: 1041, formula size: 1260, Elapsed Time: 36.74s\n",
      "Step 300, Best: 1058, current: 1037, formula size: 1260, Elapsed Time: 52.77s\n",
      "Step 400, Best: 1058, current: 1034, formula size: 1260, Elapsed Time: 68.81s\n",
      "Step 500, Best: 1058, current: 1037, formula size: 1260, Elapsed Time: 85.03s\n",
      "Step 600, Best: 1058, current: 1039, formula size: 1260, Elapsed Time: 101.06s\n",
      "Step 700, Best: 1058, current: 1038, formula size: 1260, Elapsed Time: 117.05s\n",
      "Step 800, Best: 1058, current: 1037, formula size: 1260, Elapsed Time: 133.03s\n",
      "Step 900, Best: 1058, current: 1040, formula size: 1260, Elapsed Time: 148.97s\n",
      "Step 1000, Best: 1058, current: 1034, formula size: 1260, Elapsed Time: 165.13s\n",
      "Step 1100, Best: 1058, current: 1039, formula size: 1260, Elapsed Time: 181.12s\n",
      "Step 1200, Best: 1058, current: 1035, formula size: 1260, Elapsed Time: 197.12s\n",
      "Step 1300, Best: 1058, current: 1036, formula size: 1260, Elapsed Time: 213.00s\n",
      "Step 1400, Best: 1058, current: 1033, formula size: 1260, Elapsed Time: 229.00s\n",
      "Step 1500, Best: 1058, current: 1043, formula size: 1260, Elapsed Time: 245.70s\n",
      "Step 1600, Best: 1058, current: 1037, formula size: 1260, Elapsed Time: 262.08s\n",
      "Step 1700, Best: 1058, current: 1039, formula size: 1260, Elapsed Time: 278.11s\n",
      "Step 1800, Best: 1058, current: 1036, formula size: 1260, Elapsed Time: 296.53s\n",
      "Time limit reached: 300 seconds\n",
      "maxcut-140-630-0.7-45.wcnf, 1058\n",
      "maxcut-140-630-0.7-6.wcnf\n",
      "Step 100, Best: 1074, current: 1056, formula size: 1260, Elapsed Time: 34.24s\n",
      "Step 200, Best: 1082, current: 1054, formula size: 1260, Elapsed Time: 50.73s\n",
      "Step 300, Best: 1082, current: 1053, formula size: 1260, Elapsed Time: 66.48s\n",
      "Step 400, Best: 1082, current: 1054, formula size: 1260, Elapsed Time: 82.34s\n",
      "Step 500, Best: 1082, current: 1056, formula size: 1260, Elapsed Time: 98.12s\n",
      "Step 600, Best: 1082, current: 1056, formula size: 1260, Elapsed Time: 114.04s\n",
      "Step 700, Best: 1082, current: 1059, formula size: 1260, Elapsed Time: 129.94s\n",
      "Step 800, Best: 1082, current: 1052, formula size: 1260, Elapsed Time: 145.59s\n",
      "Step 900, Best: 1082, current: 1056, formula size: 1260, Elapsed Time: 161.08s\n",
      "Step 1000, Best: 1082, current: 1053, formula size: 1260, Elapsed Time: 177.67s\n",
      "Step 1100, Best: 1082, current: 1055, formula size: 1260, Elapsed Time: 194.36s\n",
      "Step 1200, Best: 1082, current: 1053, formula size: 1260, Elapsed Time: 209.97s\n",
      "Step 1300, Best: 1082, current: 1055, formula size: 1260, Elapsed Time: 225.69s\n",
      "Step 1400, Best: 1082, current: 1052, formula size: 1260, Elapsed Time: 241.54s\n",
      "Step 1500, Best: 1082, current: 1053, formula size: 1260, Elapsed Time: 257.40s\n",
      "Step 1600, Best: 1082, current: 1053, formula size: 1260, Elapsed Time: 273.21s\n",
      "Step 1700, Best: 1082, current: 1053, formula size: 1260, Elapsed Time: 289.23s\n",
      "Step 1800, Best: 1082, current: 1052, formula size: 1260, Elapsed Time: 305.62s\n",
      "Time limit reached: 300 seconds\n",
      "maxcut-140-630-0.7-6.wcnf, 1082\n",
      "maxcut-140-630-0.8-1.wcnf\n",
      "Step 100, Best: 1074, current: 1044, formula size: 1258, Elapsed Time: 19.77s\n",
      "Step 200, Best: 1074, current: 1035, formula size: 1258, Elapsed Time: 35.26s\n",
      "Step 300, Best: 1074, current: 1039, formula size: 1258, Elapsed Time: 50.95s\n",
      "Step 400, Best: 1074, current: 1035, formula size: 1258, Elapsed Time: 66.81s\n",
      "Step 500, Best: 1074, current: 1034, formula size: 1258, Elapsed Time: 82.39s\n",
      "Step 600, Best: 1075, current: 1040, formula size: 1258, Elapsed Time: 99.86s\n",
      "Step 700, Best: 1075, current: 1039, formula size: 1258, Elapsed Time: 119.26s\n",
      "Step 800, Best: 1075, current: 1045, formula size: 1258, Elapsed Time: 135.37s\n",
      "Step 900, Best: 1075, current: 1044, formula size: 1258, Elapsed Time: 151.30s\n",
      "Step 1000, Best: 1075, current: 1040, formula size: 1258, Elapsed Time: 167.39s\n",
      "Step 1100, Best: 1075, current: 1035, formula size: 1258, Elapsed Time: 183.63s\n",
      "Step 1200, Best: 1075, current: 1034, formula size: 1258, Elapsed Time: 200.41s\n",
      "Step 1300, Best: 1075, current: 1036, formula size: 1258, Elapsed Time: 217.94s\n",
      "Step 1400, Best: 1075, current: 1037, formula size: 1258, Elapsed Time: 236.18s\n",
      "Step 1500, Best: 1075, current: 1037, formula size: 1258, Elapsed Time: 253.62s\n",
      "Step 1600, Best: 1075, current: 1046, formula size: 1258, Elapsed Time: 272.28s\n",
      "Step 1700, Best: 1075, current: 1037, formula size: 1258, Elapsed Time: 289.80s\n",
      "Time limit reached: 300 seconds\n",
      "maxcut-140-630-0.8-1.wcnf, 1075\n",
      "maxcut-140-630-0.8-18.wcnf\n",
      "Step 100, Best: 1071, current: 1039, formula size: 1258, Elapsed Time: 24.41s\n",
      "Step 200, Best: 1071, current: 1037, formula size: 1258, Elapsed Time: 42.72s\n",
      "Step 300, Best: 1074, current: 1036, formula size: 1258, Elapsed Time: 60.92s\n",
      "Step 400, Best: 1074, current: 1035, formula size: 1258, Elapsed Time: 78.66s\n",
      "Step 500, Best: 1074, current: 1030, formula size: 1258, Elapsed Time: 97.75s\n",
      "Step 600, Best: 1074, current: 1035, formula size: 1258, Elapsed Time: 116.52s\n",
      "Step 700, Best: 1074, current: 1035, formula size: 1258, Elapsed Time: 134.15s\n",
      "Step 800, Best: 1074, current: 1042, formula size: 1258, Elapsed Time: 152.00s\n",
      "Step 900, Best: 1075, current: 1030, formula size: 1258, Elapsed Time: 169.41s\n",
      "Step 1000, Best: 1075, current: 1042, formula size: 1258, Elapsed Time: 187.79s\n",
      "Step 1100, Best: 1075, current: 1031, formula size: 1258, Elapsed Time: 205.38s\n",
      "Step 1200, Best: 1075, current: 1033, formula size: 1258, Elapsed Time: 223.03s\n",
      "Step 1300, Best: 1075, current: 1036, formula size: 1258, Elapsed Time: 239.16s\n",
      "Step 1400, Best: 1075, current: 1035, formula size: 1258, Elapsed Time: 255.30s\n",
      "Step 1500, Best: 1075, current: 1038, formula size: 1258, Elapsed Time: 271.51s\n",
      "Step 1600, Best: 1075, current: 1033, formula size: 1258, Elapsed Time: 287.26s\n",
      "Step 1700, Best: 1075, current: 1032, formula size: 1258, Elapsed Time: 303.01s\n",
      "Time limit reached: 300 seconds\n",
      "maxcut-140-630-0.8-18.wcnf, 1075\n",
      "maxcut-140-630-0.8-26.wcnf\n",
      "Step 100, Best: 1059, current: 1026, formula size: 1258, Elapsed Time: 19.47s\n",
      "Step 200, Best: 1059, current: 1025, formula size: 1258, Elapsed Time: 36.01s\n",
      "Step 300, Best: 1059, current: 1027, formula size: 1258, Elapsed Time: 52.20s\n",
      "Step 400, Best: 1059, current: 1035, formula size: 1258, Elapsed Time: 68.21s\n",
      "Step 500, Best: 1059, current: 1030, formula size: 1258, Elapsed Time: 84.32s\n",
      "Step 600, Best: 1059, current: 1025, formula size: 1258, Elapsed Time: 100.93s\n",
      "Step 700, Best: 1059, current: 1027, formula size: 1258, Elapsed Time: 117.16s\n",
      "Step 800, Best: 1059, current: 1025, formula size: 1258, Elapsed Time: 133.50s\n",
      "Step 900, Best: 1059, current: 1027, formula size: 1258, Elapsed Time: 149.87s\n",
      "Step 1000, Best: 1059, current: 1025, formula size: 1258, Elapsed Time: 165.66s\n",
      "Step 1100, Best: 1059, current: 1026, formula size: 1258, Elapsed Time: 181.75s\n",
      "Step 1200, Best: 1059, current: 1034, formula size: 1258, Elapsed Time: 197.99s\n",
      "Step 1300, Best: 1059, current: 1029, formula size: 1258, Elapsed Time: 214.43s\n",
      "Step 1400, Best: 1059, current: 1030, formula size: 1258, Elapsed Time: 230.72s\n",
      "Step 1500, Best: 1059, current: 1036, formula size: 1258, Elapsed Time: 246.74s\n",
      "Step 1600, Best: 1059, current: 1034, formula size: 1258, Elapsed Time: 262.76s\n",
      "Step 1700, Best: 1059, current: 1026, formula size: 1258, Elapsed Time: 279.20s\n",
      "Step 1800, Best: 1059, current: 1029, formula size: 1258, Elapsed Time: 295.41s\n",
      "Time limit reached: 300 seconds\n",
      "maxcut-140-630-0.8-26.wcnf, 1059\n",
      "maxcut-140-630-0.8-39.wcnf\n",
      "Step 100, Best: 1064, current: 1037, formula size: 1258, Elapsed Time: 19.70s\n",
      "Step 200, Best: 1064, current: 1033, formula size: 1258, Elapsed Time: 36.03s\n",
      "Step 300, Best: 1064, current: 1030, formula size: 1258, Elapsed Time: 51.98s\n",
      "Step 400, Best: 1064, current: 1032, formula size: 1258, Elapsed Time: 68.36s\n",
      "Step 500, Best: 1064, current: 1036, formula size: 1258, Elapsed Time: 84.74s\n",
      "Step 600, Best: 1065, current: 1032, formula size: 1258, Elapsed Time: 100.83s\n",
      "Step 700, Best: 1065, current: 1039, formula size: 1258, Elapsed Time: 116.46s\n",
      "Step 800, Best: 1065, current: 1032, formula size: 1258, Elapsed Time: 132.38s\n",
      "Step 900, Best: 1065, current: 1038, formula size: 1258, Elapsed Time: 148.84s\n",
      "Step 1000, Best: 1065, current: 1034, formula size: 1258, Elapsed Time: 165.29s\n",
      "Step 1100, Best: 1065, current: 1041, formula size: 1258, Elapsed Time: 181.60s\n",
      "Step 1200, Best: 1065, current: 1034, formula size: 1258, Elapsed Time: 197.52s\n",
      "Step 1300, Best: 1065, current: 1036, formula size: 1258, Elapsed Time: 213.31s\n",
      "Step 1400, Best: 1065, current: 1029, formula size: 1258, Elapsed Time: 229.57s\n",
      "Step 1500, Best: 1065, current: 1033, formula size: 1258, Elapsed Time: 245.67s\n",
      "Step 1600, Best: 1065, current: 1036, formula size: 1258, Elapsed Time: 261.26s\n",
      "Step 1700, Best: 1065, current: 1033, formula size: 1258, Elapsed Time: 277.22s\n",
      "Step 1800, Best: 1065, current: 1035, formula size: 1258, Elapsed Time: 293.39s\n",
      "Time limit reached: 300 seconds\n",
      "maxcut-140-630-0.8-39.wcnf, 1065\n",
      "MinFill_R0_queen5_5.wcnf\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "einsum(): subscript h has size 7 for operand 1 which does not broadcast with previously seen size 8",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     formula_str \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     13\u001b[0m formula \u001b[38;5;241m=\u001b[39m CNFFormula\u001b[38;5;241m.\u001b[39mfrom_dimacs(formula_str)\n\u001b[1;32m---> 14\u001b[0m best_v, best_num_satisfied \u001b[38;5;241m=\u001b[39m \u001b[43msolve_maxsat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mformula\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheuristic_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m best_v_list\u001b[38;5;241m.\u001b[39mappend(best_v)\n\u001b[0;32m     21\u001b[0m best_num_satisfied_list\u001b[38;5;241m.\u001b[39mappend(best_num_satisfied)\n",
      "File \u001b[1;32md:\\academic\\proj2024\\rbmsat\\main.py:70\u001b[0m, in \u001b[0;36msolve_maxsat\u001b[1;34m(formula, max_time, heuristic_interval, batch_size, device)\u001b[0m\n\u001b[0;32m     68\u001b[0m h_sample, _ \u001b[38;5;241m=\u001b[39m rbm\u001b[38;5;241m.\u001b[39msample_h_given_v(v)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Sample v given h\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m v_sample, p_v_given_h \u001b[38;5;241m=\u001b[39m \u001b[43mrbm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_v_given_h\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m v \u001b[38;5;241m=\u001b[39m v_sample\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Update moving averages of ν_i\u001b[39;00m\n",
      "File \u001b[1;32md:\\academic\\proj2024\\rbmsat\\models\\rbm_parallel.py:97\u001b[0m, in \u001b[0;36mformulaRBM_parallel.sample_v_given_h\u001b[1;34m(self, h)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample_v_given_h\u001b[39m(\u001b[38;5;28mself\u001b[39m, h):\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;66;03m# h: B x C x H_c\u001b[39;00m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;66;03m# Compute contributions from hidden units\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m     c_logits \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbch,ckh->bck\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW_full\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# B x C x K\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;66;03m# Adjust for polarities\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     c_logits \u001b[38;5;241m=\u001b[39m c_logits \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolarities\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda\\Lib\\site-packages\\torch\\functional.py:386\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[1;32m--> 386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    388\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[1;31mRuntimeError\u001b[0m: einsum(): subscript h has size 7 for operand 1 which does not broadcast with previously seen size 8"
     ]
    }
   ],
   "source": [
    "from utils import CNFFormula\n",
    "from main import solve_maxsat\n",
    "import os\n",
    "\n",
    "best_v_list = []\n",
    "best_num_satisfied_list = []\n",
    "# test all files in ../wcnfdata\n",
    "for file in os.listdir(\"./wcnfdata\"):\n",
    "    print(file)\n",
    "    with open(os.path.join(\"./wcnfdata\", file), 'r') as f:\n",
    "        formula_str = f.read()\n",
    "    \n",
    "    formula = CNFFormula(formula_str)\n",
    "    best_v, best_num_satisfied = solve_maxsat(\n",
    "        formula, \n",
    "        max_time=300, \n",
    "        heuristic_interval=100, \n",
    "        batch_size=1024)\n",
    "\n",
    "    best_v_list.append(best_v)\n",
    "    best_num_satisfied_list.append(best_num_satisfied)\n",
    "\n",
    "    print(f\"{file}, {best_num_satisfied}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "822\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from utils import CNFFormula\n",
    "from main import count_satisfied_clauses\n",
    "v = torch.tensor([[0,1,1,0,0,0,0,0,1,1,1,0,1,1,1,0,0,1,0,0,1,1,0,1,1,0,0,1,0,0,1,0,1,1,0,1,1,0,0,1]])\n",
    "\n",
    "with open('./wcnfdata/brock200_3.clq.wcnf', 'r') as f:\n",
    "    formula_str = f.read()\n",
    "formula = CNFFormula(formula_str)\n",
    "best_v, best_num_satisfied = count_satisfied_clauses(\n",
    "    formula, \n",
    "    v\n",
    ")\n",
    "print(best_num_satisfied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pre-trained RBM to rbm_models\\rbm_length_5_F_-1.0_num_epochs_0_lr_0.01_device_cpu.pth\n",
      "Epoch 200, Loss: 0.40392351150512695\n",
      "Epoch 400, Loss: 0.15035410225391388\n",
      "Epoch 600, Loss: 0.07847076654434204\n",
      "Epoch 800, Loss: 0.054054055362939835\n",
      "Epoch 1000, Loss: 0.0438944511115551\n",
      "Epoch 1200, Loss: 0.03873978182673454\n",
      "Epoch 1400, Loss: 0.03555965796113014\n",
      "Epoch 1600, Loss: 0.033251821994781494\n",
      "Epoch 1800, Loss: 0.031401023268699646\n",
      "Epoch 2000, Loss: 0.0298561193048954\n",
      "Epoch 2200, Loss: 0.028566580265760422\n",
      "Epoch 2400, Loss: 0.027515148743987083\n",
      "Epoch 2600, Loss: 0.026690170168876648\n",
      "Epoch 2800, Loss: 0.026074139401316643\n",
      "Epoch 3000, Loss: 0.02563847415149212\n",
      "Epoch 3200, Loss: 0.025342069566249847\n",
      "Epoch 3400, Loss: 0.025134537369012833\n",
      "Epoch 3600, Loss: 0.02496362291276455\n",
      "Epoch 3800, Loss: 0.024781839922070503\n",
      "Epoch 4000, Loss: 0.024546733126044273\n",
      "Epoch 4200, Loss: 0.024215035140514374\n",
      "Epoch 4400, Loss: 0.02373451739549637\n",
      "Epoch 4600, Loss: 0.02303897961974144\n",
      "Epoch 4800, Loss: 0.022056490182876587\n",
      "Epoch 5000, Loss: 0.02074250765144825\n",
      "Epoch 5200, Loss: 0.01912299171090126\n",
      "Epoch 5400, Loss: 0.01727711223065853\n",
      "Epoch 5600, Loss: 0.015307983383536339\n",
      "Epoch 5800, Loss: 0.013373110443353653\n",
      "Epoch 6000, Loss: 0.011549542658030987\n",
      "Epoch 6200, Loss: 0.009875369258224964\n",
      "Epoch 6400, Loss: 0.008388581685721874\n",
      "Epoch 6600, Loss: 0.007084610406309366\n",
      "Epoch 6800, Loss: 0.005943823605775833\n",
      "Epoch 7000, Loss: 0.0049563259817659855\n",
      "Epoch 7200, Loss: 0.004117242991924286\n",
      "Epoch 7400, Loss: 0.0034164630342274904\n",
      "Epoch 7600, Loss: 0.0028380085714161396\n",
      "Epoch 7800, Loss: 0.002363809384405613\n",
      "Epoch 8000, Loss: 0.0019763780292123556\n",
      "Epoch 8200, Loss: 0.0016599998343735933\n",
      "Epoch 8400, Loss: 0.0014011700404807925\n",
      "Epoch 8600, Loss: 0.001188668073154986\n",
      "Epoch 8800, Loss: 0.0010133805917575955\n",
      "Epoch 9000, Loss: 0.0008680210448801517\n",
      "Epoch 9200, Loss: 0.0007468013791367412\n",
      "Epoch 9400, Loss: 0.0006451421650126576\n",
      "Epoch 9600, Loss: 0.0005594170652329922\n",
      "Epoch 9800, Loss: 0.000486749573610723\n",
      "Epoch 10000, Loss: 0.0004248454642947763\n",
      "Epoch 10200, Loss: 0.0003718665393535048\n",
      "Epoch 10400, Loss: 0.00032633013324812055\n",
      "Epoch 10600, Loss: 0.000287036964436993\n",
      "Epoch 10800, Loss: 0.00025300716515630484\n",
      "Epoch 11000, Loss: 0.0002234371640952304\n",
      "Epoch 11200, Loss: 0.00019766455807257444\n",
      "Epoch 11400, Loss: 0.000175138731719926\n",
      "Epoch 11600, Loss: 0.00015540105232503265\n",
      "Epoch 11800, Loss: 0.0001381541951559484\n",
      "Epoch 12000, Loss: 0.00012305565178394318\n",
      "Epoch 12200, Loss: 0.00010980173101415858\n",
      "Epoch 12400, Loss: 9.81421981123276e-05\n",
      "Epoch 12600, Loss: 8.78721839399077e-05\n",
      "Epoch 12800, Loss: 8.203398465411738e-05\n",
      "Epoch 13000, Loss: 7.085879042278975e-05\n",
      "Epoch 13200, Loss: 6.366102752508596e-05\n",
      "Epoch 13400, Loss: 5.7363293308299035e-05\n",
      "Epoch 13600, Loss: 5.1768038247246295e-05\n",
      "Epoch 13800, Loss: 4.716054172604345e-05\n",
      "Epoch 14000, Loss: 4.428903048392385e-05\n",
      "Epoch 14200, Loss: 5.9192359913140535e-05\n",
      "Epoch 14400, Loss: 3.509763701003976e-05\n",
      "Epoch 14600, Loss: 3.170587297063321e-05\n",
      "Epoch 14800, Loss: 2.8883041522931308e-05\n",
      "Epoch 15000, Loss: 2.635696728248149e-05\n",
      "Epoch 15200, Loss: 2.408993532299064e-05\n",
      "Epoch 15400, Loss: 2.2055515728425235e-05\n",
      "Epoch 15600, Loss: 2.0227511413395405e-05\n",
      "Epoch 15800, Loss: 1.8578704839455895e-05\n",
      "Epoch 16000, Loss: 1.9340182916494086e-05\n",
      "Epoch 16200, Loss: 1.5759673260618e-05\n",
      "Epoch 16400, Loss: 1.455203255318338e-05\n",
      "Epoch 16600, Loss: 1.3461986782203894e-05\n",
      "Epoch 16800, Loss: 1.468576374463737e-05\n",
      "Epoch 17000, Loss: 1.1578315024962649e-05\n",
      "Epoch 17200, Loss: 1.076745229511289e-05\n",
      "Epoch 17400, Loss: 1.0032485079136677e-05\n",
      "Epoch 17600, Loss: 9.36283413466299e-06\n",
      "Epoch 17800, Loss: 8.84791006683372e-06\n",
      "Epoch 18000, Loss: 9.364960533275735e-06\n",
      "Epoch 18200, Loss: 7.694693522353191e-06\n",
      "Epoch 18400, Loss: 7.237149475258775e-06\n",
      "Epoch 18600, Loss: 6.8171698330843356e-06\n",
      "Epoch 18800, Loss: 6.437068350351183e-06\n",
      "Epoch 19000, Loss: 1.031122974382015e-05\n",
      "Epoch 19200, Loss: 5.763316949014552e-06\n",
      "Epoch 19400, Loss: 5.470068572321907e-06\n",
      "Epoch 19600, Loss: 5.1998263188579585e-06\n",
      "Epoch 19800, Loss: 4.951129085384309e-06\n",
      "Epoch 20000, Loss: 4.740450549434172e-06\n",
      "Saved pre-trained RBM to rbm_models\\rbm_length_5_F_-1.0_num_epochs_20000_lr_0.01_device_cpu.pth\n",
      "-4.158883094787598 -4.268941879272461 -4.259560585021973 -4.032647609710693 -4.160459041595459 -4.262772083282471\n",
      "-0.011785866692662239 -0.9983624219894409 -1.000793218612671 -0.9994568228721619 -0.9998423457145691 -1.0006365776062012\n"
     ]
    }
   ],
   "source": [
    "from models.rbm import clauseRBM\n",
    "import torch\n",
    "\n",
    "rbm_random = clauseRBM(5, F_s=-1.0, num_epochs=0, lr=0.01, device='cpu', verbose=True)\n",
    "rbm_trained = clauseRBM(5, F_s=-1.0, num_epochs=20000, lr=0.01, device='cpu', verbose=True)\n",
    "\n",
    "v_neg = torch.tensor([[0.0, 0.0, 0.0, 0.0, 0.0]])\n",
    "v_pos1 = torch.tensor([[1.0, 0.0, 0.0, 0.0, 0.0]])\n",
    "v_pos2 = torch.tensor([[1.0, 1.0, 0.0, 0.0, 0.0]])\n",
    "v_pos3 = torch.tensor([[1.0, 1.0, 1.0, 0.0, 0.0]])\n",
    "v_pos4 = torch.tensor([[1.0, 1.0, 1.0, 1.0, 0.0]])\n",
    "v_pos5 = torch.tensor([[1.0, 1.0, 1.0, 1.0, 1.0]])\n",
    "\n",
    "print(rbm_random.free_energy(v_neg).item(), rbm_random.free_energy(v_pos1).item(), rbm_random.free_energy(v_pos2).item(), rbm_random.free_energy(v_pos3).item(), rbm_random.free_energy(v_pos4).item(), rbm_random.free_energy(v_pos5).item())\n",
    "print(rbm_trained.free_energy(v_neg).item(), rbm_trained.free_energy(v_pos1).item(), rbm_trained.free_energy(v_pos2).item(), rbm_trained.free_energy(v_pos3).item(), rbm_trained.free_energy(v_pos4).item(), rbm_trained.free_energy(v_pos5).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 1., 1., 0.]]), 783)\n",
      "trained (tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 1., 1., 0.]]), 755)\n",
      "random (tensor([[0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1.,\n",
      "         0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1.]]), 789)\n"
     ]
    }
   ],
   "source": [
    "from utils import CNFFormula, count_satisfied_clauses\n",
    "from models.rbm import formulaRBM\n",
    "import torch\n",
    "\n",
    "with open('./wcnfdata/brock200_3.clq.wcnf', 'r') as f:\n",
    "    formula_str = f.read()\n",
    "formula = CNFFormula(formula_str)\n",
    "n_visible = formula.num_vars\n",
    "v_init = torch.bernoulli(torch.full((1, n_visible), 0.5, device=\"cuda\"))\n",
    "\n",
    "print(count_satisfied_clauses(formula, v_init.cpu()))\n",
    "\n",
    "rbm_trained = formulaRBM(formula,\n",
    "                         num_epochs=10000,\n",
    "                         device=\"cuda\")\n",
    "\n",
    "v = v_init.clone()\n",
    "for _ in range(100):\n",
    "    h_sample, _ = rbm_trained.sample_h_given_v(v)\n",
    "    v_sample, p_v_given_h = rbm_trained.sample_v_given_h(h_sample)\n",
    "    v = v_sample\n",
    "\n",
    "    # plot the F_s each step\n",
    "\n",
    "print(f\"trained {count_satisfied_clauses(formula, v.cpu())}\")\n",
    "v1 = v.clone()\n",
    "\n",
    "\n",
    "rbm_random = formulaRBM(formula,\n",
    "                        num_epochs=0,\n",
    "                        device=\"cuda\")\n",
    "\n",
    "v = v_init.clone()\n",
    "for _ in range(100):\n",
    "    h_sample, _ = rbm_random.sample_h_given_v(v)\n",
    "    v_sample, p_v_given_h = rbm_random.sample_v_given_h(h_sample)\n",
    "    v = v_sample\n",
    "print(f\"random {count_satisfied_clauses(formula, v.cpu())}\")\n",
    "\n",
    "v2 = v.clone()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([819.0408], device='cuda:0')\n",
      "tensor([1106.0283], device='cuda:0')\n",
      "tensor([772.8834], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(rbm_trained.free_energy(v_init))\n",
    "print(rbm_trained.free_energy(v1))\n",
    "print(rbm_trained.free_energy(v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training consistently leads to worse results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.9978, -3.1180,  0.0000,  0.0000, -2.3297,  3.9816, -2.4516,  2.3680,\n",
      "          1.0588,  2.0557,  1.2671, -4.1289,  2.4115],\n",
      "        [-1.1680,  6.0914, -1.9978,  3.1180,  5.4193, -1.5878, -1.0259,  4.1523,\n",
      "         -0.5033, -1.9225, -0.8347, -0.4400, -2.4606],\n",
      "        [ 0.0000,  0.0000, -1.1680,  6.0914, -1.1188, -1.4281,  5.4664, -0.6311,\n",
      "         -0.3739,  4.0862, -0.5167, -0.5561, -1.6324],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.6017,\n",
      "          3.5612, -2.3678,  3.1999, -0.8163, -2.3878],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.3785,\n",
      "         -0.1678, -0.7122, -0.6422, -0.3692,  4.1957]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from utils import CNFFormula\n",
    "from models.rbm import formulaRBM\n",
    "import torch\n",
    "\n",
    "with open('./test02.cnf', 'r') as f:\n",
    "    formula_str = f.read()\n",
    "formula = CNFFormula(formula_str)\n",
    "\n",
    "rbm_trained = formulaRBM(formula,\n",
    "                         num_epochs=10000,\n",
    "                         device=\"cuda\")\n",
    "\n",
    "print(rbm_trained.W_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.9978, -3.1180],\n",
      "        [-1.1680,  6.0914]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-5.6222, -5.4554], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.9735, -0.0620], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "from models.rbm import clauseRBM\n",
    "\n",
    "rbm_len2 = clauseRBM(2, F_s=-1.0, num_epochs=10000, lr=0.01, device='cuda')\n",
    "\n",
    "print(rbm_len2.W)\n",
    "print(rbm_len2.b)\n",
    "print(rbm_len2.d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2500, -0.7500, -0.7500, -1.2500])\n",
      "tensor([[-3.3325e-08,  1.1686e-08],\n",
      "        [-3.3325e-08,  1.1686e-08]])\n",
      "tensor([0.5000, 0.5000])\n",
      "tensor([-2.0163, -2.0163])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import time\n",
    "from utils import CNFFormula, count_satisfied_clauses, unit_propagation\n",
    "class clauseRBM_symmetric(nn.Module):\n",
    "    def __init__(self, clause_length=2, F_s=-1.0, num_epochs=20000, lr=0.1, verbose=False):\n",
    "        super(clauseRBM_symmetric, self).__init__()\n",
    "        self.n_visible = clause_length\n",
    "        self.n_hidden = clause_length if clause_length <= 3 else clause_length + 1\n",
    "\n",
    "        # self.train_clause_rbm(F_s=F_s, num_epochs=num_epochs, lr=lr)\n",
    "        self.W = torch.Tensor([[-3.3325e-08,  1.1686e-08],[-3.3325e-08,  1.1686e-08]])\n",
    "        self.d = torch.Tensor([0.5 , 0.5])\n",
    "        self.b = torch.Tensor([-2.0163, -2.0163])\n",
    "    \n",
    "    def train_clause_rbm(self, F_s=-1.0, num_epochs=30000, lr=0.1):\n",
    "        \n",
    "        self.W = nn.Parameter(torch.randn(1, self.n_hidden) * 0.1)\n",
    "        self.d = nn.Parameter(torch.zeros(1))\n",
    "        self.b = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.01)\n",
    "\n",
    "        # Generate all possible binary vectors\n",
    "        n_samples = 2 ** self.n_visible\n",
    "        v_vectors = torch.tensor([ [int(x) for x in bin(i)[2:].zfill(self.n_visible)] for i in range(n_samples)], dtype=torch.float32)\n",
    "        # print(v_vectors)\n",
    "        # Target free energies\n",
    "        F_targets = torch.zeros(n_samples)\n",
    "        for i in range(n_samples):\n",
    "            if v_vectors[i].sum() == 0:\n",
    "                F_targets[i] = 0.0\n",
    "            else:\n",
    "                F_targets[i] = F_s\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            optimizer.zero_grad()\n",
    "            F_v = self.free_energy(v_vectors)\n",
    "            loss = torch.mean((F_v - F_targets) ** 2)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (epoch+1) % 1000 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "        \n",
    "    \n",
    "    def free_energy(self, v):\n",
    "        # v: batch_size x n_visible\n",
    "        v_term = torch.sum(self.d * v, dim=1)\n",
    "        # print(v_term)\n",
    "        # print(self.W)\n",
    "        W_expanded = self.W.expand(v.shape[1], self.n_hidden)\n",
    "        # print(W_expanded)\n",
    "        pre_activation = self.b + torch.matmul(v, W_expanded)\n",
    "        # print(pre_activation)\n",
    "        h_term = torch.sum(torch.log1p(torch.exp(pre_activation)), dim=1)\n",
    "        F_v = -v_term - h_term\n",
    "        # print(F_v)\n",
    "        return F_v  # batch_size\n",
    "    \n",
    "rbm = clauseRBM_symmetric()\n",
    "print(rbm.free_energy(torch.tensor([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])))\n",
    "print(rbm.W)\n",
    "print(rbm.d)\n",
    "print(rbm.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class formulaRBM_symmetric:\n",
    "    def __init__(self, \n",
    "                 formula: CNFFormula, \n",
    "                 device='cpu'):\n",
    "\n",
    "        literal_clauses = formula.to_literal_form()\n",
    "        n_visible_total = formula.num_vars\n",
    "        n_hidden_total = 0\n",
    "        for clause in literal_clauses:\n",
    "            clause_length = len(clause)\n",
    "            n_hidden = clause_length if clause_length <= 3 else clause_length + 1\n",
    "            n_hidden_total += n_hidden\n",
    "\n",
    "        self.W_full = torch.zeros(n_visible_total, n_hidden_total, device=device)\n",
    "        self.b_full = torch.zeros(n_hidden_total, device=device)\n",
    "        self.d_full = torch.zeros(n_visible_total, device=device)\n",
    "        \n",
    "        curr_hidden_idx = 0\n",
    "\n",
    "        for clause in literal_clauses:\n",
    "            # clause: list of (var_idx, is_negated)\n",
    "            variable_indices, signs = zip(*clause)\n",
    "            # variable_indices = [var_idx for (var_idx, is_negated) in clause]\n",
    "            # signs = [is_negated for (var_idx, is_negated) in clause]\n",
    "            \n",
    "            clause_length = len(clause)\n",
    "            n_visible = clause_length\n",
    "            n_hidden = n_visible if n_visible <= 3 else n_visible + 1\n",
    "            \n",
    "            # Get pre-trained RBM for this clause length\n",
    "            rbm = clauseRBM_symmetric(clause_length)\n",
    "            \n",
    "            # Adjust W and b according to signs\n",
    "            Lambda = torch.tensor([-1.0 if is_negated else 1.0 for is_negated in signs], device=device)\n",
    "            W = rbm.W.detach()  # size n_visible x n_hidden\n",
    "            b = rbm.b.detach()  # size n_hidden\n",
    "            d = rbm.d.detach()  # size n_visible\n",
    "            \n",
    "            W_prime = Lambda.unsqueeze(1) * W  # size n_visible x n_hidden\n",
    "            b_prime = b + 0.5 * torch.matmul((1 - Lambda), W)\n",
    "            \n",
    "            # Map W_prime into W_full\n",
    "            for local_var_idx, var_idx in enumerate(variable_indices):\n",
    "                self.W_full[var_idx, curr_hidden_idx : curr_hidden_idx + n_hidden] += W_prime[local_var_idx, :]\n",
    "                self.d_full[var_idx] += d[local_var_idx]\n",
    "            # Map b_prime into b_full\n",
    "            self.b_full[curr_hidden_idx : curr_hidden_idx + n_hidden] = b_prime\n",
    "            \n",
    "            curr_hidden_idx += n_hidden\n",
    "\n",
    "        self.n_visible = n_visible_total\n",
    "        self.n_hidden = n_hidden_total\n",
    "        self.device = device\n",
    "        \n",
    "    def sample_h_given_v(self, v):\n",
    "        # v: batch_size x n_visible\n",
    "        p_h_given_v = torch.sigmoid(self.b_full + torch.matmul(v, self.W_full))\n",
    "        h_sample = torch.bernoulli(p_h_given_v)\n",
    "        return h_sample, p_h_given_v\n",
    "    \n",
    "    def sample_v_given_h(self, h):\n",
    "        # h: batch_size x n_hidden\n",
    "        p_v_given_h = torch.sigmoid(self.d_full + torch.matmul(h, self.W_full.t()))\n",
    "        v_sample = torch.bernoulli(p_v_given_h)\n",
    "        return v_sample, p_v_given_h\n",
    "    \n",
    "    def gibbs_sampling(self, v_init, k=1):\n",
    "        v = v_init\n",
    "        for _ in range(k):\n",
    "            h_sample, _ = self.sample_h_given_v(v)\n",
    "            v_sample, _ = self.sample_v_given_h(h_sample)\n",
    "            v = v_sample\n",
    "        return v\n",
    "\n",
    "    def free_energy(self, v):\n",
    "        return torch.sum(v * self.d_full, dim=1) + torch.sum(torch.log(1 + torch.exp(self.b_full + torch.matmul(v, self.W_full))), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100, Best: 787, current: 787, formula size: 1038, Elapsed Time: 3.95s\n",
      "Step 200, Best: 787, current: 787, formula size: 1038, Elapsed Time: 7.69s\n",
      "Step 300, Best: 787, current: 787, formula size: 1038, Elapsed Time: 11.50s\n",
      "Step 400, Best: 787, current: 787, formula size: 1038, Elapsed Time: 15.21s\n",
      "Step 500, Best: 787, current: 787, formula size: 1038, Elapsed Time: 18.94s\n",
      "Step 600, Best: 787, current: 787, formula size: 1038, Elapsed Time: 22.76s\n",
      "Step 700, Best: 787, current: 787, formula size: 1038, Elapsed Time: 26.37s\n",
      "Step 800, Best: 787, current: 787, formula size: 1038, Elapsed Time: 29.79s\n",
      "Step 900, Best: 787, current: 787, formula size: 1038, Elapsed Time: 33.35s\n",
      "Step 1000, Best: 787, current: 787, formula size: 1038, Elapsed Time: 37.06s\n",
      "Step 1100, Best: 787, current: 787, formula size: 1038, Elapsed Time: 40.70s\n",
      "Step 1200, Best: 787, current: 787, formula size: 1038, Elapsed Time: 44.51s\n",
      "Step 1300, Best: 787, current: 787, formula size: 1038, Elapsed Time: 48.07s\n",
      "Step 1400, Best: 787, current: 787, formula size: 1038, Elapsed Time: 51.34s\n",
      "Step 1500, Best: 787, current: 787, formula size: 1038, Elapsed Time: 55.12s\n",
      "Step 1600, Best: 787, current: 787, formula size: 1038, Elapsed Time: 58.86s\n",
      "Time limit reached: 60 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('./wcnfdata/brock200_3.clq.wcnf', 'r') as f:\n",
    "    formula_str = f.read()\n",
    "formula = CNFFormula(formula_str)\n",
    "\n",
    "rbm_trained = formulaRBM_symmetric(formula)\n",
    "batch_size = 1\n",
    "heuristic_interval = 100\n",
    "max_time = 60\n",
    "\n",
    "n_visible = rbm_trained.n_visible\n",
    "\n",
    "# Initialize v to random assignment\n",
    "v = torch.bernoulli(torch.full((batch_size, n_visible), 0.5))\n",
    "\n",
    "best_v, best_num_satisfied = count_satisfied_clauses(formula, v.cpu())\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Unit propagation heuristic: Initialize moving averages of ν_i\n",
    "nu_i = torch.zeros(n_visible)\n",
    "alpha = 0.9  # Decay factor for moving average\n",
    "step = 0\n",
    "while True:\n",
    "    step += 1\n",
    "    \n",
    "    # Sample h given v\n",
    "    h_sample, _ = rbm_trained.sample_h_given_v(v)\n",
    "    # Sample v given h\n",
    "    v_sample, p_v_given_h = rbm_trained.sample_v_given_h(h_sample)\n",
    "    v = v_sample\n",
    "    \n",
    "    # Update moving averages of ν_i\n",
    "    rho_i = p_v_given_h.mean(dim=0)  # size n_visible\n",
    "    nu_i = alpha * nu_i + (1 - alpha) * (rho_i * (1 - rho_i))\n",
    "    \n",
    "    # Every heuristic_interval steps, apply heuristic\n",
    "    if step % heuristic_interval == 0:\n",
    "        # Rank variables by ν_i\n",
    "        _, indices = torch.sort(nu_i)\n",
    "        num_vars_to_unassign = n_visible // 2\n",
    "        vars_to_unassign = indices[:num_vars_to_unassign]\n",
    "        # Set these variables to unassigned (-1)\n",
    "\n",
    "        assignments = []\n",
    "        for i in range(batch_size):\n",
    "            assignment = v[i].clone()\n",
    "            assignment[vars_to_unassign] = -1  # Unassign variables\n",
    "            # Apply unit propagation\n",
    "            assignment = unit_propagation(formula, assignment.cpu())\n",
    "            assignments.append(assignment)\n",
    "        v = torch.stack(assignments, dim=0)  # Shape: (B, N)\n",
    "        # Reset moving averages\n",
    "        nu_i.zero_()\n",
    "    \n",
    "    # Evaluate current assignment\n",
    "    current_best_v, current_best_num_satisfied = count_satisfied_clauses(formula, v)\n",
    "    if current_best_num_satisfied > best_num_satisfied:\n",
    "        best_num_satisfied = current_best_num_satisfied\n",
    "        best_v = current_best_v.clone()\n",
    "    \n",
    "    if best_num_satisfied == formula.num_clauses:\n",
    "        print(f\"Solved in {step} steps\")\n",
    "        break\n",
    "    \n",
    "    # Optional: print progress\n",
    "    if step % 100 == 0:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Step {step}, Best: {best_num_satisfied}, current: {current_best_num_satisfied}, formula size: {formula.num_clauses}, Elapsed Time: {elapsed_time:.2f}s\")\n",
    "    \n",
    "    # Check time limit\n",
    "    if time.time() - start_time >= max_time:\n",
    "        print(f\"Time limit reached: {max_time} seconds\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import CNFFormula\n",
    "from models.rbm import clauseRBM\n",
    "import torch\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "with open(\"wcnfdata/brock200_3.clq.wcnf\", 'r') as f:\n",
    "    cnf_str = f.read()\n",
    "\n",
    "formula = CNFFormula(cnf_str)\n",
    "formula.padding()\n",
    "rbm = clauseRBM(formula.max_clause_length)\n",
    "\n",
    "def gather_and_count(v, Q, polarity):\n",
    "    c = torch.einsum('bv,ckv->bck', v, Q) # (batch_size, num_clause, max_len_clause)\n",
    "    return c, (((polarity + c) == 2) + ((polarity + c) == -1)).any(dim=-1).sum(dim=-1)\n",
    "\n",
    "def time_remaining(start_time, time_limit):\n",
    "    if time.time() - start_time > time_limit:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def construct_Q(T, num_var):\n",
    "    Q = torch.zeros(T.shape[0], T.shape[1], num_var) #(num_clause, len_clause, num_var)\n",
    "    for clause_idx, clause in enumerate(T):\n",
    "        for lit_idx, lit in enumerate(clause):\n",
    "            if lit != 0:\n",
    "                Q[clause_idx, lit_idx, torch.abs(lit)-1] = 1.0\n",
    "    return Q\n",
    "\n",
    "def rbmsat(W_c, b_c, T, B, N, seed, time_limit, upp=100, upw=1, alpha=0.1): # B is batch size, N is the total number of variables\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Initialize variables\n",
    "    s_max = 0 # initially 0 clauses are satisfied\n",
    "    v = torch.bernoulli(torch.ones(B, N, device=W_c.device) * 0.5)  # sample random inits \n",
    "\n",
    "    # Setup tensors\n",
    "    polarity = torch.sign(T)  # strictly {1, -1}, not 0 (-1 2 -3) -> (-1 1 -1)\n",
    "    Q = construct_Q(T, formula.num_vars) # C * K * N    (num_clause, max_len_clause, num_var)\n",
    "    W = torch.einsum('ck,kh->ckh', polarity, W_c) # polarity.unsqueeze(1) * W_c.unsqueeze(2) # element-wise multiplication\n",
    "    b = b_c.repeat([T.shape[0], 1]) + torch.mm((1 - polarity) / 2, W_c)\n",
    "    \n",
    "    # t, d = 1, -1\n",
    "    step = 0\n",
    "    s_max_list = []\n",
    "    best_v = torch.ones(N, device=W_c.device)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    while time_remaining(start_time, time_limit):\n",
    "        step += 1\n",
    "            \n",
    "        c, s = gather_and_count(v, Q, polarity)\n",
    "        s_max_step, idx_max = s.max(dim=0)\n",
    "        if s_max_step.item() > s_max:\n",
    "            s_max = s_max_step.item()\n",
    "            best_v = v[idx_max].clone()\n",
    "            s_max_list.append((step, s_max))\n",
    "            print(f\"new s_max found {s_max} at step {step}\")\n",
    "\n",
    "        h_logits = b + torch.einsum('bck,ckh->bch', c, W) # 'bck,chk->bhk'  bck,ckh->bch\n",
    "        h = torch.bernoulli(torch.sigmoid(h_logits))\n",
    "        ro = torch.sigmoid(torch.einsum('bch,ckh,ckv->bv', h, W, Q)) # 'bhk,chk,ckv->bv'\n",
    "        v = torch.bernoulli(ro)\n",
    "        \n",
    "    return s_max_list, best_v\n",
    "\n",
    "T = torch.tensor(formula.clauses)\n",
    "\n",
    "alpha = 0.1\n",
    "upp = 100\n",
    "upw = 1\n",
    "\n",
    "s_max, best_v = rbmsat(rbm.W, rbm.b, T, 128, formula.num_vars, 42, 300, upp, upw, alpha)\n",
    "print(best_v)\n",
    "x_ax, y_ax = zip(*s_max)\n",
    "plt.step(x_ax,y_ax)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
