{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brock200_3.clq.wcnf\n",
      "Step 100, Best: 813, current: 810, formula size: 1038, Elapsed Time: 16.84s\n",
      "Step 200, Best: 818, current: 803, formula size: 1038, Elapsed Time: 29.70s\n",
      "Step 300, Best: 818, current: 801, formula size: 1038, Elapsed Time: 44.76s\n",
      "Step 400, Best: 818, current: 801, formula size: 1038, Elapsed Time: 60.85s\n",
      "Step 500, Best: 818, current: 806, formula size: 1038, Elapsed Time: 77.98s\n",
      "Step 600, Best: 818, current: 802, formula size: 1038, Elapsed Time: 97.43s\n",
      "Step 700, Best: 818, current: 805, formula size: 1038, Elapsed Time: 110.33s\n",
      "Step 800, Best: 818, current: 802, formula size: 1038, Elapsed Time: 123.68s\n",
      "Step 900, Best: 818, current: 806, formula size: 1038, Elapsed Time: 136.62s\n",
      "Step 1000, Best: 818, current: 803, formula size: 1038, Elapsed Time: 149.38s\n",
      "Step 1100, Best: 818, current: 808, formula size: 1038, Elapsed Time: 162.05s\n",
      "Step 1200, Best: 818, current: 796, formula size: 1038, Elapsed Time: 174.84s\n",
      "Step 1300, Best: 818, current: 804, formula size: 1038, Elapsed Time: 187.38s\n",
      "Step 1400, Best: 818, current: 804, formula size: 1038, Elapsed Time: 199.90s\n",
      "Step 1500, Best: 818, current: 804, formula size: 1038, Elapsed Time: 212.75s\n",
      "Step 1600, Best: 818, current: 797, formula size: 1038, Elapsed Time: 226.16s\n",
      "Step 1700, Best: 818, current: 805, formula size: 1038, Elapsed Time: 239.29s\n",
      "Step 1800, Best: 818, current: 799, formula size: 1038, Elapsed Time: 252.24s\n",
      "Step 1900, Best: 818, current: 800, formula size: 1038, Elapsed Time: 276.34s\n",
      "Step 2000, Best: 818, current: 798, formula size: 1038, Elapsed Time: 302.01s\n",
      "Time limit reached: 300 seconds\n",
      "brock200_3.clq.wcnf, 818\n",
      "data.135.wcnf\n",
      "Step 100, Best: 3016, current: 2608, formula size: 3150, Elapsed Time: 85.12s\n",
      "Step 200, Best: 3016, current: 2605, formula size: 3150, Elapsed Time: 128.71s\n",
      "Step 300, Best: 3016, current: 2602, formula size: 3150, Elapsed Time: 177.22s\n",
      "Step 400, Best: 3016, current: 2605, formula size: 3150, Elapsed Time: 222.67s\n",
      "Step 500, Best: 3016, current: 2608, formula size: 3150, Elapsed Time: 266.49s\n",
      "Step 600, Best: 3016, current: 2607, formula size: 3150, Elapsed Time: 311.06s\n",
      "Time limit reached: 300 seconds\n",
      "data.135.wcnf, 3016\n",
      "data.243.wcnf\n",
      "Step 100, Best: 9801, current: 8712, formula size: 10044, Elapsed Time: 205.07s\n",
      "Time limit reached: 300 seconds\n",
      "data.243.wcnf, 9801\n",
      "data.405.wcnf\n",
      "Time limit reached: 300 seconds\n",
      "data.405.wcnf, 27270\n",
      "data.729.wcnf\n",
      "Time limit reached: 300 seconds\n",
      "data.729.wcnf, 88452\n",
      "hamming10-4.clq.wcnf\n",
      "Time limit reached: 300 seconds\n",
      "hamming10-4.clq.wcnf, 88575\n",
      "maxcut-140-630-0.7-12.wcnf\n",
      "Step 100, Best: 1074, current: 1049, formula size: 1260, Elapsed Time: 19.77s\n",
      "Step 200, Best: 1074, current: 1051, formula size: 1260, Elapsed Time: 35.73s\n",
      "Step 300, Best: 1074, current: 1047, formula size: 1260, Elapsed Time: 51.82s\n",
      "Step 400, Best: 1074, current: 1044, formula size: 1260, Elapsed Time: 68.05s\n",
      "Step 500, Best: 1074, current: 1049, formula size: 1260, Elapsed Time: 84.37s\n",
      "Step 600, Best: 1074, current: 1042, formula size: 1260, Elapsed Time: 100.39s\n",
      "Step 700, Best: 1074, current: 1045, formula size: 1260, Elapsed Time: 116.11s\n",
      "Step 800, Best: 1074, current: 1050, formula size: 1260, Elapsed Time: 132.74s\n",
      "Step 900, Best: 1074, current: 1047, formula size: 1260, Elapsed Time: 154.22s\n",
      "Step 1000, Best: 1074, current: 1044, formula size: 1260, Elapsed Time: 174.82s\n",
      "Step 1100, Best: 1074, current: 1045, formula size: 1260, Elapsed Time: 195.13s\n",
      "Step 1200, Best: 1075, current: 1043, formula size: 1260, Elapsed Time: 214.91s\n",
      "Step 1300, Best: 1075, current: 1045, formula size: 1260, Elapsed Time: 235.70s\n",
      "Step 1400, Best: 1075, current: 1041, formula size: 1260, Elapsed Time: 254.55s\n",
      "Step 1500, Best: 1075, current: 1050, formula size: 1260, Elapsed Time: 270.32s\n",
      "Step 1600, Best: 1075, current: 1050, formula size: 1260, Elapsed Time: 285.83s\n",
      "Step 1700, Best: 1075, current: 1043, formula size: 1260, Elapsed Time: 301.28s\n",
      "Time limit reached: 300 seconds\n",
      "maxcut-140-630-0.7-12.wcnf, 1075\n",
      "maxcut-140-630-0.7-20.wcnf\n",
      "Step 100, Best: 1072, current: 1044, formula size: 1260, Elapsed Time: 18.98s\n",
      "Step 200, Best: 1072, current: 1040, formula size: 1260, Elapsed Time: 34.23s\n",
      "Step 300, Best: 1073, current: 1039, formula size: 1260, Elapsed Time: 49.56s\n",
      "Step 400, Best: 1073, current: 1042, formula size: 1260, Elapsed Time: 64.79s\n",
      "Step 500, Best: 1073, current: 1043, formula size: 1260, Elapsed Time: 79.99s\n",
      "Step 600, Best: 1073, current: 1040, formula size: 1260, Elapsed Time: 95.21s\n",
      "Step 700, Best: 1073, current: 1039, formula size: 1260, Elapsed Time: 110.46s\n",
      "Step 800, Best: 1073, current: 1041, formula size: 1260, Elapsed Time: 125.68s\n",
      "Step 900, Best: 1073, current: 1046, formula size: 1260, Elapsed Time: 140.94s\n",
      "Step 1000, Best: 1073, current: 1042, formula size: 1260, Elapsed Time: 156.13s\n",
      "Step 1100, Best: 1073, current: 1040, formula size: 1260, Elapsed Time: 171.56s\n",
      "Step 1200, Best: 1074, current: 1041, formula size: 1260, Elapsed Time: 187.17s\n",
      "Step 1300, Best: 1074, current: 1037, formula size: 1260, Elapsed Time: 202.43s\n",
      "Step 1400, Best: 1074, current: 1040, formula size: 1260, Elapsed Time: 217.64s\n",
      "Step 1500, Best: 1074, current: 1043, formula size: 1260, Elapsed Time: 232.85s\n",
      "Step 1600, Best: 1074, current: 1040, formula size: 1260, Elapsed Time: 248.06s\n",
      "Step 1700, Best: 1074, current: 1039, formula size: 1260, Elapsed Time: 263.25s\n",
      "Step 1800, Best: 1074, current: 1043, formula size: 1260, Elapsed Time: 278.49s\n",
      "Step 1900, Best: 1074, current: 1041, formula size: 1260, Elapsed Time: 293.77s\n",
      "Time limit reached: 300 seconds\n",
      "maxcut-140-630-0.7-20.wcnf, 1074\n",
      "maxcut-140-630-0.7-28.wcnf\n",
      "Step 100, Best: 1061, current: 1028, formula size: 1260, Elapsed Time: 19.29s\n",
      "Step 200, Best: 1065, current: 1040, formula size: 1260, Elapsed Time: 34.89s\n",
      "Step 300, Best: 1066, current: 1042, formula size: 1260, Elapsed Time: 50.54s\n",
      "Step 400, Best: 1066, current: 1046, formula size: 1260, Elapsed Time: 66.16s\n",
      "Step 500, Best: 1066, current: 1043, formula size: 1260, Elapsed Time: 81.67s\n",
      "Step 600, Best: 1066, current: 1041, formula size: 1260, Elapsed Time: 97.33s\n",
      "Step 700, Best: 1066, current: 1034, formula size: 1260, Elapsed Time: 113.23s\n",
      "Step 800, Best: 1066, current: 1042, formula size: 1260, Elapsed Time: 129.33s\n",
      "Step 900, Best: 1066, current: 1036, formula size: 1260, Elapsed Time: 144.93s\n",
      "Step 1000, Best: 1066, current: 1036, formula size: 1260, Elapsed Time: 160.48s\n",
      "Step 1100, Best: 1066, current: 1035, formula size: 1260, Elapsed Time: 176.29s\n",
      "Step 1200, Best: 1066, current: 1049, formula size: 1260, Elapsed Time: 191.91s\n",
      "Step 1300, Best: 1066, current: 1040, formula size: 1260, Elapsed Time: 207.44s\n",
      "Step 1400, Best: 1067, current: 1036, formula size: 1260, Elapsed Time: 223.18s\n",
      "Step 1500, Best: 1067, current: 1037, formula size: 1260, Elapsed Time: 238.79s\n",
      "Step 1600, Best: 1067, current: 1039, formula size: 1260, Elapsed Time: 254.39s\n",
      "Step 1700, Best: 1067, current: 1035, formula size: 1260, Elapsed Time: 269.97s\n",
      "Step 1800, Best: 1067, current: 1045, formula size: 1260, Elapsed Time: 285.54s\n",
      "Step 1900, Best: 1067, current: 1040, formula size: 1260, Elapsed Time: 301.38s\n",
      "Time limit reached: 300 seconds\n",
      "maxcut-140-630-0.7-28.wcnf, 1067\n",
      "maxcut-140-630-0.7-4.wcnf\n",
      "Step 100, Best: 1062, current: 1035, formula size: 1260, Elapsed Time: 19.13s\n",
      "Step 200, Best: 1062, current: 1037, formula size: 1260, Elapsed Time: 34.59s\n",
      "Step 300, Best: 1062, current: 1039, formula size: 1260, Elapsed Time: 50.31s\n",
      "Step 400, Best: 1064, current: 1034, formula size: 1260, Elapsed Time: 66.42s\n",
      "Step 500, Best: 1064, current: 1038, formula size: 1260, Elapsed Time: 82.13s\n",
      "Step 600, Best: 1064, current: 1039, formula size: 1260, Elapsed Time: 97.58s\n",
      "Step 700, Best: 1064, current: 1038, formula size: 1260, Elapsed Time: 113.12s\n",
      "Step 800, Best: 1064, current: 1037, formula size: 1260, Elapsed Time: 128.62s\n",
      "Step 900, Best: 1064, current: 1031, formula size: 1260, Elapsed Time: 144.08s\n",
      "Step 1000, Best: 1064, current: 1041, formula size: 1260, Elapsed Time: 159.57s\n",
      "Step 1100, Best: 1064, current: 1041, formula size: 1260, Elapsed Time: 175.02s\n",
      "Step 1200, Best: 1064, current: 1040, formula size: 1260, Elapsed Time: 190.54s\n",
      "Step 1300, Best: 1064, current: 1037, formula size: 1260, Elapsed Time: 205.99s\n",
      "Step 1400, Best: 1064, current: 1039, formula size: 1260, Elapsed Time: 221.49s\n",
      "Step 1500, Best: 1066, current: 1037, formula size: 1260, Elapsed Time: 237.01s\n",
      "Step 1600, Best: 1066, current: 1037, formula size: 1260, Elapsed Time: 252.51s\n",
      "Step 1700, Best: 1066, current: 1037, formula size: 1260, Elapsed Time: 268.02s\n",
      "Step 1800, Best: 1066, current: 1038, formula size: 1260, Elapsed Time: 283.57s\n",
      "Step 1900, Best: 1066, current: 1035, formula size: 1260, Elapsed Time: 299.89s\n",
      "Time limit reached: 300 seconds\n",
      "maxcut-140-630-0.7-4.wcnf, 1066\n",
      "maxcut-140-630-0.7-45.wcnf\n",
      "Step 100, Best: 1058, current: 1033, formula size: 1260, Elapsed Time: 20.52s\n",
      "Step 200, Best: 1058, current: 1041, formula size: 1260, Elapsed Time: 36.74s\n",
      "Step 300, Best: 1058, current: 1037, formula size: 1260, Elapsed Time: 52.77s\n",
      "Step 400, Best: 1058, current: 1034, formula size: 1260, Elapsed Time: 68.81s\n",
      "Step 500, Best: 1058, current: 1037, formula size: 1260, Elapsed Time: 85.03s\n",
      "Step 600, Best: 1058, current: 1039, formula size: 1260, Elapsed Time: 101.06s\n",
      "Step 700, Best: 1058, current: 1038, formula size: 1260, Elapsed Time: 117.05s\n",
      "Step 800, Best: 1058, current: 1037, formula size: 1260, Elapsed Time: 133.03s\n",
      "Step 900, Best: 1058, current: 1040, formula size: 1260, Elapsed Time: 148.97s\n",
      "Step 1000, Best: 1058, current: 1034, formula size: 1260, Elapsed Time: 165.13s\n",
      "Step 1100, Best: 1058, current: 1039, formula size: 1260, Elapsed Time: 181.12s\n",
      "Step 1200, Best: 1058, current: 1035, formula size: 1260, Elapsed Time: 197.12s\n",
      "Step 1300, Best: 1058, current: 1036, formula size: 1260, Elapsed Time: 213.00s\n",
      "Step 1400, Best: 1058, current: 1033, formula size: 1260, Elapsed Time: 229.00s\n",
      "Step 1500, Best: 1058, current: 1043, formula size: 1260, Elapsed Time: 245.70s\n",
      "Step 1600, Best: 1058, current: 1037, formula size: 1260, Elapsed Time: 262.08s\n",
      "Step 1700, Best: 1058, current: 1039, formula size: 1260, Elapsed Time: 278.11s\n",
      "Step 1800, Best: 1058, current: 1036, formula size: 1260, Elapsed Time: 296.53s\n",
      "Time limit reached: 300 seconds\n",
      "maxcut-140-630-0.7-45.wcnf, 1058\n",
      "maxcut-140-630-0.7-6.wcnf\n",
      "Step 100, Best: 1074, current: 1056, formula size: 1260, Elapsed Time: 34.24s\n",
      "Step 200, Best: 1082, current: 1054, formula size: 1260, Elapsed Time: 50.73s\n",
      "Step 300, Best: 1082, current: 1053, formula size: 1260, Elapsed Time: 66.48s\n",
      "Step 400, Best: 1082, current: 1054, formula size: 1260, Elapsed Time: 82.34s\n",
      "Step 500, Best: 1082, current: 1056, formula size: 1260, Elapsed Time: 98.12s\n",
      "Step 600, Best: 1082, current: 1056, formula size: 1260, Elapsed Time: 114.04s\n",
      "Step 700, Best: 1082, current: 1059, formula size: 1260, Elapsed Time: 129.94s\n",
      "Step 800, Best: 1082, current: 1052, formula size: 1260, Elapsed Time: 145.59s\n",
      "Step 900, Best: 1082, current: 1056, formula size: 1260, Elapsed Time: 161.08s\n",
      "Step 1000, Best: 1082, current: 1053, formula size: 1260, Elapsed Time: 177.67s\n",
      "Step 1100, Best: 1082, current: 1055, formula size: 1260, Elapsed Time: 194.36s\n",
      "Step 1200, Best: 1082, current: 1053, formula size: 1260, Elapsed Time: 209.97s\n",
      "Step 1300, Best: 1082, current: 1055, formula size: 1260, Elapsed Time: 225.69s\n",
      "Step 1400, Best: 1082, current: 1052, formula size: 1260, Elapsed Time: 241.54s\n",
      "Step 1500, Best: 1082, current: 1053, formula size: 1260, Elapsed Time: 257.40s\n",
      "Step 1600, Best: 1082, current: 1053, formula size: 1260, Elapsed Time: 273.21s\n",
      "Step 1700, Best: 1082, current: 1053, formula size: 1260, Elapsed Time: 289.23s\n",
      "Step 1800, Best: 1082, current: 1052, formula size: 1260, Elapsed Time: 305.62s\n",
      "Time limit reached: 300 seconds\n",
      "maxcut-140-630-0.7-6.wcnf, 1082\n",
      "maxcut-140-630-0.8-1.wcnf\n",
      "Step 100, Best: 1074, current: 1044, formula size: 1258, Elapsed Time: 19.77s\n",
      "Step 200, Best: 1074, current: 1035, formula size: 1258, Elapsed Time: 35.26s\n",
      "Step 300, Best: 1074, current: 1039, formula size: 1258, Elapsed Time: 50.95s\n",
      "Step 400, Best: 1074, current: 1035, formula size: 1258, Elapsed Time: 66.81s\n",
      "Step 500, Best: 1074, current: 1034, formula size: 1258, Elapsed Time: 82.39s\n",
      "Step 600, Best: 1075, current: 1040, formula size: 1258, Elapsed Time: 99.86s\n",
      "Step 700, Best: 1075, current: 1039, formula size: 1258, Elapsed Time: 119.26s\n",
      "Step 800, Best: 1075, current: 1045, formula size: 1258, Elapsed Time: 135.37s\n",
      "Step 900, Best: 1075, current: 1044, formula size: 1258, Elapsed Time: 151.30s\n",
      "Step 1000, Best: 1075, current: 1040, formula size: 1258, Elapsed Time: 167.39s\n",
      "Step 1100, Best: 1075, current: 1035, formula size: 1258, Elapsed Time: 183.63s\n",
      "Step 1200, Best: 1075, current: 1034, formula size: 1258, Elapsed Time: 200.41s\n",
      "Step 1300, Best: 1075, current: 1036, formula size: 1258, Elapsed Time: 217.94s\n",
      "Step 1400, Best: 1075, current: 1037, formula size: 1258, Elapsed Time: 236.18s\n",
      "Step 1500, Best: 1075, current: 1037, formula size: 1258, Elapsed Time: 253.62s\n",
      "Step 1600, Best: 1075, current: 1046, formula size: 1258, Elapsed Time: 272.28s\n",
      "Step 1700, Best: 1075, current: 1037, formula size: 1258, Elapsed Time: 289.80s\n",
      "Time limit reached: 300 seconds\n",
      "maxcut-140-630-0.8-1.wcnf, 1075\n",
      "maxcut-140-630-0.8-18.wcnf\n",
      "Step 100, Best: 1071, current: 1039, formula size: 1258, Elapsed Time: 24.41s\n",
      "Step 200, Best: 1071, current: 1037, formula size: 1258, Elapsed Time: 42.72s\n",
      "Step 300, Best: 1074, current: 1036, formula size: 1258, Elapsed Time: 60.92s\n",
      "Step 400, Best: 1074, current: 1035, formula size: 1258, Elapsed Time: 78.66s\n",
      "Step 500, Best: 1074, current: 1030, formula size: 1258, Elapsed Time: 97.75s\n",
      "Step 600, Best: 1074, current: 1035, formula size: 1258, Elapsed Time: 116.52s\n",
      "Step 700, Best: 1074, current: 1035, formula size: 1258, Elapsed Time: 134.15s\n",
      "Step 800, Best: 1074, current: 1042, formula size: 1258, Elapsed Time: 152.00s\n",
      "Step 900, Best: 1075, current: 1030, formula size: 1258, Elapsed Time: 169.41s\n",
      "Step 1000, Best: 1075, current: 1042, formula size: 1258, Elapsed Time: 187.79s\n",
      "Step 1100, Best: 1075, current: 1031, formula size: 1258, Elapsed Time: 205.38s\n",
      "Step 1200, Best: 1075, current: 1033, formula size: 1258, Elapsed Time: 223.03s\n",
      "Step 1300, Best: 1075, current: 1036, formula size: 1258, Elapsed Time: 239.16s\n",
      "Step 1400, Best: 1075, current: 1035, formula size: 1258, Elapsed Time: 255.30s\n",
      "Step 1500, Best: 1075, current: 1038, formula size: 1258, Elapsed Time: 271.51s\n",
      "Step 1600, Best: 1075, current: 1033, formula size: 1258, Elapsed Time: 287.26s\n",
      "Step 1700, Best: 1075, current: 1032, formula size: 1258, Elapsed Time: 303.01s\n",
      "Time limit reached: 300 seconds\n",
      "maxcut-140-630-0.8-18.wcnf, 1075\n",
      "maxcut-140-630-0.8-26.wcnf\n",
      "Step 100, Best: 1059, current: 1026, formula size: 1258, Elapsed Time: 19.47s\n",
      "Step 200, Best: 1059, current: 1025, formula size: 1258, Elapsed Time: 36.01s\n",
      "Step 300, Best: 1059, current: 1027, formula size: 1258, Elapsed Time: 52.20s\n",
      "Step 400, Best: 1059, current: 1035, formula size: 1258, Elapsed Time: 68.21s\n",
      "Step 500, Best: 1059, current: 1030, formula size: 1258, Elapsed Time: 84.32s\n",
      "Step 600, Best: 1059, current: 1025, formula size: 1258, Elapsed Time: 100.93s\n",
      "Step 700, Best: 1059, current: 1027, formula size: 1258, Elapsed Time: 117.16s\n",
      "Step 800, Best: 1059, current: 1025, formula size: 1258, Elapsed Time: 133.50s\n",
      "Step 900, Best: 1059, current: 1027, formula size: 1258, Elapsed Time: 149.87s\n",
      "Step 1000, Best: 1059, current: 1025, formula size: 1258, Elapsed Time: 165.66s\n",
      "Step 1100, Best: 1059, current: 1026, formula size: 1258, Elapsed Time: 181.75s\n",
      "Step 1200, Best: 1059, current: 1034, formula size: 1258, Elapsed Time: 197.99s\n",
      "Step 1300, Best: 1059, current: 1029, formula size: 1258, Elapsed Time: 214.43s\n",
      "Step 1400, Best: 1059, current: 1030, formula size: 1258, Elapsed Time: 230.72s\n",
      "Step 1500, Best: 1059, current: 1036, formula size: 1258, Elapsed Time: 246.74s\n",
      "Step 1600, Best: 1059, current: 1034, formula size: 1258, Elapsed Time: 262.76s\n",
      "Step 1700, Best: 1059, current: 1026, formula size: 1258, Elapsed Time: 279.20s\n",
      "Step 1800, Best: 1059, current: 1029, formula size: 1258, Elapsed Time: 295.41s\n",
      "Time limit reached: 300 seconds\n",
      "maxcut-140-630-0.8-26.wcnf, 1059\n",
      "maxcut-140-630-0.8-39.wcnf\n",
      "Step 100, Best: 1064, current: 1037, formula size: 1258, Elapsed Time: 19.70s\n",
      "Step 200, Best: 1064, current: 1033, formula size: 1258, Elapsed Time: 36.03s\n",
      "Step 300, Best: 1064, current: 1030, formula size: 1258, Elapsed Time: 51.98s\n",
      "Step 400, Best: 1064, current: 1032, formula size: 1258, Elapsed Time: 68.36s\n",
      "Step 500, Best: 1064, current: 1036, formula size: 1258, Elapsed Time: 84.74s\n",
      "Step 600, Best: 1065, current: 1032, formula size: 1258, Elapsed Time: 100.83s\n",
      "Step 700, Best: 1065, current: 1039, formula size: 1258, Elapsed Time: 116.46s\n",
      "Step 800, Best: 1065, current: 1032, formula size: 1258, Elapsed Time: 132.38s\n",
      "Step 900, Best: 1065, current: 1038, formula size: 1258, Elapsed Time: 148.84s\n",
      "Step 1000, Best: 1065, current: 1034, formula size: 1258, Elapsed Time: 165.29s\n",
      "Step 1100, Best: 1065, current: 1041, formula size: 1258, Elapsed Time: 181.60s\n",
      "Step 1200, Best: 1065, current: 1034, formula size: 1258, Elapsed Time: 197.52s\n",
      "Step 1300, Best: 1065, current: 1036, formula size: 1258, Elapsed Time: 213.31s\n",
      "Step 1400, Best: 1065, current: 1029, formula size: 1258, Elapsed Time: 229.57s\n",
      "Step 1500, Best: 1065, current: 1033, formula size: 1258, Elapsed Time: 245.67s\n",
      "Step 1600, Best: 1065, current: 1036, formula size: 1258, Elapsed Time: 261.26s\n",
      "Step 1700, Best: 1065, current: 1033, formula size: 1258, Elapsed Time: 277.22s\n",
      "Step 1800, Best: 1065, current: 1035, formula size: 1258, Elapsed Time: 293.39s\n",
      "Time limit reached: 300 seconds\n",
      "maxcut-140-630-0.8-39.wcnf, 1065\n",
      "MinFill_R0_queen5_5.wcnf\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "einsum(): subscript h has size 7 for operand 1 which does not broadcast with previously seen size 8",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     formula_str \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     13\u001b[0m formula \u001b[38;5;241m=\u001b[39m CNFFormula\u001b[38;5;241m.\u001b[39mfrom_dimacs(formula_str)\n\u001b[1;32m---> 14\u001b[0m best_v, best_num_satisfied \u001b[38;5;241m=\u001b[39m \u001b[43msolve_maxsat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mformula\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheuristic_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m best_v_list\u001b[38;5;241m.\u001b[39mappend(best_v)\n\u001b[0;32m     21\u001b[0m best_num_satisfied_list\u001b[38;5;241m.\u001b[39mappend(best_num_satisfied)\n",
      "File \u001b[1;32md:\\academic\\proj2024\\rbmsat\\main.py:70\u001b[0m, in \u001b[0;36msolve_maxsat\u001b[1;34m(formula, max_time, heuristic_interval, batch_size, device)\u001b[0m\n\u001b[0;32m     68\u001b[0m h_sample, _ \u001b[38;5;241m=\u001b[39m rbm\u001b[38;5;241m.\u001b[39msample_h_given_v(v)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Sample v given h\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m v_sample, p_v_given_h \u001b[38;5;241m=\u001b[39m \u001b[43mrbm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_v_given_h\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m v \u001b[38;5;241m=\u001b[39m v_sample\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Update moving averages of ν_i\u001b[39;00m\n",
      "File \u001b[1;32md:\\academic\\proj2024\\rbmsat\\models\\rbm_parallel.py:97\u001b[0m, in \u001b[0;36mformulaRBM_parallel.sample_v_given_h\u001b[1;34m(self, h)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample_v_given_h\u001b[39m(\u001b[38;5;28mself\u001b[39m, h):\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;66;03m# h: B x C x H_c\u001b[39;00m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;66;03m# Compute contributions from hidden units\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m     c_logits \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbch,ckh->bck\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW_full\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# B x C x K\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;66;03m# Adjust for polarities\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     c_logits \u001b[38;5;241m=\u001b[39m c_logits \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolarities\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda\\Lib\\site-packages\\torch\\functional.py:386\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[1;32m--> 386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    388\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[1;31mRuntimeError\u001b[0m: einsum(): subscript h has size 7 for operand 1 which does not broadcast with previously seen size 8"
     ]
    }
   ],
   "source": [
    "from utils import CNFFormula\n",
    "from main import solve_maxsat\n",
    "import os\n",
    "\n",
    "best_v_list = []\n",
    "best_num_satisfied_list = []\n",
    "# test all files in ../wcnfdata\n",
    "for file in os.listdir(\"./wcnfdata\"):\n",
    "    print(file)\n",
    "    with open(os.path.join(\"./wcnfdata\", file), 'r') as f:\n",
    "        formula_str = f.read()\n",
    "    \n",
    "    formula = CNFFormula.from_dimacs(formula_str)\n",
    "    best_v, best_num_satisfied = solve_maxsat(\n",
    "        formula, \n",
    "        max_time=300, \n",
    "        heuristic_interval=100, \n",
    "        batch_size=1024)\n",
    "\n",
    "    best_v_list.append(best_v)\n",
    "    best_num_satisfied_list.append(best_num_satisfied)\n",
    "\n",
    "    print(f\"{file}, {best_num_satisfied}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "822\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from utils import CNFFormula\n",
    "from main import count_satisfied_clauses\n",
    "v = torch.tensor([[0,1,1,0,0,0,0,0,1,1,1,0,1,1,1,0,0,1,0,0,1,1,0,1,1,0,0,1,0,0,1,0,1,1,0,1,1,0,0,1]])\n",
    "\n",
    "with open('./wcnfdata/brock200_3.clq.wcnf', 'r') as f:\n",
    "    formula_str = f.read()\n",
    "formula = CNFFormula.from_dimacs(formula_str)\n",
    "best_v, best_num_satisfied = count_satisfied_clauses(\n",
    "    formula, \n",
    "    v\n",
    ")\n",
    "print(best_num_satisfied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3046\n"
     ]
    }
   ],
   "source": [
    "v_str = \"111111101111111001111111111111010111111111100011011100111011101101110001111111101111011011111111011110011111111111101110110110011110010\"\n",
    "\n",
    "v = torch.tensor([[int(i) for i in v_str]])\n",
    "\n",
    "with open('./wcnfdata/data.135.wcnf', 'r') as f:\n",
    "    formula_str = f.read()\n",
    "formula = CNFFormula.from_dimacs(formula_str)\n",
    "best_v, best_num_satisfied = count_satisfied_clauses(\n",
    "    formula, \n",
    "    v\n",
    ")\n",
    "print(best_num_satisfied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pre-trained RBM to rbm_models\\rbm_length_7_F_-1.0_num_epochs_0_lr_0.01_device_cpu.pth\n",
      "Epoch 200, Loss: 0.5294787287712097\n",
      "Epoch 400, Loss: 0.18371400237083435\n",
      "Epoch 600, Loss: 0.08564762771129608\n",
      "Epoch 800, Loss: 0.05263989791274071\n",
      "Epoch 1000, Loss: 0.03903345391154289\n",
      "Epoch 1200, Loss: 0.0323796309530735\n",
      "Epoch 1400, Loss: 0.028537021949887276\n",
      "Epoch 1600, Loss: 0.025904139503836632\n",
      "Epoch 1800, Loss: 0.023803526535630226\n",
      "Epoch 2000, Loss: 0.021942131221294403\n",
      "Epoch 2200, Loss: 0.02020268887281418\n",
      "Epoch 2400, Loss: 0.01855131797492504\n",
      "Epoch 2600, Loss: 0.01699143461883068\n",
      "Epoch 2800, Loss: 0.015539235435426235\n",
      "Epoch 3000, Loss: 0.014211130328476429\n",
      "Epoch 3200, Loss: 0.013018311001360416\n",
      "Epoch 3400, Loss: 0.01196541078388691\n",
      "Epoch 3600, Loss: 0.011051185429096222\n",
      "Epoch 3800, Loss: 0.010269776917994022\n",
      "Epoch 4000, Loss: 0.00961202010512352\n",
      "Epoch 4200, Loss: 0.0090664466843009\n",
      "Epoch 4400, Loss: 0.008620152249932289\n",
      "Epoch 4600, Loss: 0.00825975276529789\n",
      "Epoch 4800, Loss: 0.007972470484673977\n",
      "Epoch 5000, Loss: 0.0077470834366977215\n",
      "Epoch 5200, Loss: 0.007574262097477913\n",
      "Epoch 5400, Loss: 0.007446164730936289\n",
      "Epoch 5600, Loss: 0.007355446461588144\n",
      "Epoch 5800, Loss: 0.007294321898370981\n",
      "Epoch 6000, Loss: 0.007254349067807198\n",
      "Epoch 6200, Loss: 0.007227140013128519\n",
      "Epoch 6400, Loss: 0.007205468136817217\n",
      "Epoch 6600, Loss: 0.007183882873505354\n",
      "Epoch 6800, Loss: 0.0071584065444767475\n",
      "Epoch 7000, Loss: 0.007125698961317539\n",
      "Epoch 7200, Loss: 0.007082290481775999\n",
      "Epoch 7400, Loss: 0.0070243533700704575\n",
      "Epoch 7600, Loss: 0.006948472931981087\n",
      "Epoch 7800, Loss: 0.006853132508695126\n",
      "Epoch 8000, Loss: 0.0067384084686636925\n",
      "Epoch 8200, Loss: 0.006604158319532871\n",
      "Epoch 8400, Loss: 0.006453399546444416\n",
      "Epoch 8600, Loss: 0.006299347151070833\n",
      "Epoch 8800, Loss: 0.006153916474431753\n",
      "Epoch 9000, Loss: 0.006006129551678896\n",
      "Epoch 9200, Loss: 0.005827309098094702\n",
      "Epoch 9400, Loss: 0.005577153526246548\n",
      "Epoch 9600, Loss: 0.005194780882447958\n",
      "Epoch 9800, Loss: 0.004645222797989845\n",
      "Epoch 10000, Loss: 0.004009023774415255\n",
      "Saved pre-trained RBM to rbm_models\\rbm_length_7_F_-1.0_num_epochs_10000_lr_0.01_device_cpu.pth\n",
      "-5.545177459716797 -5.436546802520752 -5.465852737426758 -5.553926944732666 -5.394552707672119\n",
      "-0.6320355534553528 -0.9887940287590027 -1.0042240619659424 -1.0044212341308594 -0.9907715320587158\n"
     ]
    }
   ],
   "source": [
    "from models.rbm import clauseRBM\n",
    "import torch\n",
    "\n",
    "rbm_random = clauseRBM(7, F_s=-1.0, num_epochs=0, lr=0.01, device='cpu', verbose=True)\n",
    "rbm_trained = clauseRBM(7, F_s=-1.0, num_epochs=10000, lr=0.01, device='cpu', verbose=True)\n",
    "\n",
    "v_neg = torch.tensor([[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]])\n",
    "v_pos1 = torch.tensor([[1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]])\n",
    "v_pos2 = torch.tensor([[1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]])\n",
    "v_pos3 = torch.tensor([[1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]])\n",
    "v_pos4 = torch.tensor([[1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]])\n",
    "v_pos5 = torch.tensor([[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]])\n",
    "\n",
    "print(rbm_random.free_energy(v_neg).item(), rbm_random.free_energy(v_pos1).item(), rbm_random.free_energy(v_pos2).item(), rbm_random.free_energy(v_pos3).item(), rbm_random.free_energy(v_pos4).item())\n",
    "print(rbm_trained.free_energy(v_neg).item(), rbm_trained.free_energy(v_pos1).item(), rbm_trained.free_energy(v_pos2).item(), rbm_trained.free_energy(v_pos3).item(), rbm_trained.free_energy(v_pos4).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 1., 1., 0.]]), 783)\n",
      "trained (tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 1., 1., 0.]]), 755)\n",
      "random (tensor([[0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1.,\n",
      "         0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1.]]), 789)\n"
     ]
    }
   ],
   "source": [
    "from utils import CNFFormula, count_satisfied_clauses\n",
    "from models.rbm import formulaRBM\n",
    "import torch\n",
    "\n",
    "with open('./wcnfdata/brock200_3.clq.wcnf', 'r') as f:\n",
    "    formula_str = f.read()\n",
    "formula = CNFFormula.from_dimacs(formula_str)\n",
    "n_visible = formula.num_vars\n",
    "v_init = torch.bernoulli(torch.full((1, n_visible), 0.5, device=\"cuda\"))\n",
    "\n",
    "print(count_satisfied_clauses(formula, v_init.cpu()))\n",
    "\n",
    "rbm_trained = formulaRBM(formula,\n",
    "                         num_epochs=10000,\n",
    "                         device=\"cuda\")\n",
    "\n",
    "v = v_init.clone()\n",
    "for _ in range(100):\n",
    "    h_sample, _ = rbm_trained.sample_h_given_v(v)\n",
    "    v_sample, p_v_given_h = rbm_trained.sample_v_given_h(h_sample)\n",
    "    v = v_sample\n",
    "\n",
    "    # plot the F_s each step\n",
    "\n",
    "print(f\"trained {count_satisfied_clauses(formula, v.cpu())}\")\n",
    "v1 = v.clone()\n",
    "\n",
    "\n",
    "rbm_random = formulaRBM(formula,\n",
    "                        num_epochs=0,\n",
    "                        device=\"cuda\")\n",
    "\n",
    "v = v_init.clone()\n",
    "for _ in range(100):\n",
    "    h_sample, _ = rbm_random.sample_h_given_v(v)\n",
    "    v_sample, p_v_given_h = rbm_random.sample_v_given_h(h_sample)\n",
    "    v = v_sample\n",
    "print(f\"random {count_satisfied_clauses(formula, v.cpu())}\")\n",
    "\n",
    "v2 = v.clone()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([819.0408], device='cuda:0')\n",
      "tensor([1106.0283], device='cuda:0')\n",
      "tensor([772.8834], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(rbm_trained.free_energy(v_init))\n",
    "print(rbm_trained.free_energy(v1))\n",
    "print(rbm_trained.free_energy(v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training consistently leads to worse results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.9978, -3.1180,  0.0000,  0.0000, -2.3297,  3.9816, -2.4516,  2.3680,\n",
      "          1.0588,  2.0557,  1.2671, -4.1289,  2.4115],\n",
      "        [-1.1680,  6.0914, -1.9978,  3.1180,  5.4193, -1.5878, -1.0259,  4.1523,\n",
      "         -0.5033, -1.9225, -0.8347, -0.4400, -2.4606],\n",
      "        [ 0.0000,  0.0000, -1.1680,  6.0914, -1.1188, -1.4281,  5.4664, -0.6311,\n",
      "         -0.3739,  4.0862, -0.5167, -0.5561, -1.6324],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.6017,\n",
      "          3.5612, -2.3678,  3.1999, -0.8163, -2.3878],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.3785,\n",
      "         -0.1678, -0.7122, -0.6422, -0.3692,  4.1957]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from utils import CNFFormula\n",
    "from models.rbm import formulaRBM\n",
    "import torch\n",
    "\n",
    "with open('./test02.cnf', 'r') as f:\n",
    "    formula_str = f.read()\n",
    "formula = CNFFormula.from_dimacs(formula_str)\n",
    "\n",
    "rbm_trained = formulaRBM(formula,\n",
    "                         num_epochs=10000,\n",
    "                         device=\"cuda\")\n",
    "\n",
    "print(rbm_trained.W_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.9978, -3.1180],\n",
      "        [-1.1680,  6.0914]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-5.6222, -5.4554], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.9735, -0.0620], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "from models.rbm import clauseRBM\n",
    "\n",
    "rbm_len2 = clauseRBM(2, F_s=-1.0, num_epochs=10000, lr=0.01, device='cuda')\n",
    "\n",
    "print(rbm_len2.W)\n",
    "print(rbm_len2.b)\n",
    "print(rbm_len2.d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000, Loss: 0.06264595687389374\n",
      "Epoch 2000, Loss: 0.06250002980232239\n",
      "Epoch 3000, Loss: 0.0625000149011612\n",
      "Epoch 4000, Loss: 0.0624999925494194\n",
      "Epoch 5000, Loss: 0.0624999962747097\n",
      "Epoch 6000, Loss: 0.062499985098838806\n",
      "Epoch 7000, Loss: 0.0625000149011612\n",
      "Epoch 8000, Loss: 0.0624999962747097\n",
      "Epoch 9000, Loss: 0.0625000074505806\n",
      "Epoch 10000, Loss: 0.06250017136335373\n",
      "Epoch 11000, Loss: 0.0625000149011612\n",
      "Epoch 12000, Loss: 0.0625\n",
      "Epoch 13000, Loss: 0.062499985098838806\n",
      "Epoch 14000, Loss: 0.0625000074505806\n",
      "Epoch 15000, Loss: 0.0624999962747097\n",
      "Epoch 16000, Loss: 0.0625000074505806\n",
      "Epoch 17000, Loss: 0.0625026598572731\n",
      "Epoch 18000, Loss: 0.0625\n",
      "Epoch 19000, Loss: 0.0625\n",
      "Epoch 20000, Loss: 0.0624999962747097\n",
      "Epoch 21000, Loss: 0.0625\n",
      "Epoch 22000, Loss: 0.0625\n",
      "Epoch 23000, Loss: 0.0625\n",
      "Epoch 24000, Loss: 0.0625\n",
      "Epoch 25000, Loss: 0.0625\n",
      "Epoch 26000, Loss: 0.0625\n",
      "Epoch 27000, Loss: 0.0625\n",
      "Epoch 28000, Loss: 0.0625\n",
      "Epoch 29000, Loss: 0.0625\n",
      "Epoch 30000, Loss: 0.0625\n",
      "Epoch 31000, Loss: 0.0625\n",
      "Epoch 32000, Loss: 0.0625\n",
      "Epoch 33000, Loss: 0.0625\n",
      "Epoch 34000, Loss: 0.0625\n",
      "Epoch 35000, Loss: 0.0625\n",
      "Epoch 36000, Loss: 0.0625\n",
      "Epoch 37000, Loss: 0.0625\n",
      "Epoch 38000, Loss: 0.0625\n",
      "Epoch 39000, Loss: 0.0625\n",
      "Epoch 40000, Loss: 0.0625\n",
      "Epoch 41000, Loss: 0.0625\n",
      "Epoch 42000, Loss: 0.0625\n",
      "Epoch 43000, Loss: 0.0625\n",
      "Epoch 44000, Loss: 0.0625\n",
      "Epoch 45000, Loss: 0.0625\n",
      "Epoch 46000, Loss: 0.0625\n",
      "Epoch 47000, Loss: 0.0625\n",
      "Epoch 48000, Loss: 0.0625\n",
      "Epoch 49000, Loss: 0.0625\n",
      "Epoch 50000, Loss: 0.0625\n",
      "Epoch 51000, Loss: 0.0625\n",
      "Epoch 52000, Loss: 0.0625\n",
      "Epoch 53000, Loss: 0.0625\n",
      "Epoch 54000, Loss: 0.0625\n",
      "Epoch 55000, Loss: 0.0625\n",
      "Epoch 56000, Loss: 0.0625\n",
      "Epoch 57000, Loss: 0.0625\n",
      "Epoch 58000, Loss: 0.0625\n",
      "Epoch 59000, Loss: 0.0625\n",
      "Epoch 60000, Loss: 0.0625\n",
      "Epoch 61000, Loss: 0.0625\n",
      "Epoch 62000, Loss: 0.0625\n",
      "Epoch 63000, Loss: 0.0625\n",
      "Epoch 64000, Loss: 0.0625\n",
      "Epoch 65000, Loss: 0.0625\n",
      "Epoch 66000, Loss: 0.0625\n",
      "Epoch 67000, Loss: 0.0625\n",
      "Epoch 68000, Loss: 0.0625\n",
      "Epoch 69000, Loss: 0.0625\n",
      "Epoch 70000, Loss: 0.0625\n",
      "Epoch 71000, Loss: 0.0625\n",
      "Epoch 72000, Loss: 0.0625\n",
      "Epoch 73000, Loss: 0.0625\n",
      "Epoch 74000, Loss: 0.0625\n",
      "Epoch 75000, Loss: 0.0625\n",
      "Epoch 76000, Loss: 0.0625\n",
      "Epoch 77000, Loss: 0.0625\n",
      "Epoch 78000, Loss: 0.0625\n",
      "Epoch 79000, Loss: 0.0625\n",
      "Epoch 80000, Loss: 0.0625\n",
      "Epoch 81000, Loss: 0.0625\n",
      "Epoch 82000, Loss: 0.0625\n",
      "Epoch 83000, Loss: 0.0625\n",
      "Epoch 84000, Loss: 0.0625\n",
      "Epoch 85000, Loss: 0.0625\n",
      "Epoch 86000, Loss: 0.0625\n",
      "Epoch 87000, Loss: 0.0625\n",
      "Epoch 88000, Loss: 0.0625\n",
      "Epoch 89000, Loss: 0.0625\n",
      "Epoch 90000, Loss: 0.0625\n",
      "Epoch 91000, Loss: 0.0625\n",
      "Epoch 92000, Loss: 0.0625\n",
      "Epoch 93000, Loss: 0.0625\n",
      "Epoch 94000, Loss: 0.0625\n",
      "Epoch 95000, Loss: 0.0625\n",
      "Epoch 96000, Loss: 0.0625\n",
      "Epoch 97000, Loss: 0.0625\n",
      "Epoch 98000, Loss: 0.0625\n",
      "Epoch 99000, Loss: 0.0625\n",
      "Epoch 100000, Loss: 0.0625\n",
      "tensor([-0.2500, -0.7500, -0.7500, -1.2500], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class clauseRBM_symmetric(nn.Module):\n",
    "    def __init__(self, n_visible = 2, n_hidden = 2, F_s=-1.0, num_epochs=100000, lr=0.1, verbose=False):\n",
    "        super(clauseRBM_symmetric, self).__init__()\n",
    "        self.n_visible = n_visible\n",
    "        self.n_hidden = n_hidden\n",
    "        self.W = nn.Parameter(torch.randn(1, n_hidden) * 0.1)\n",
    "        self.d = nn.Parameter(torch.zeros(1))\n",
    "        self.b = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.01)\n",
    "\n",
    "        # Generate all possible binary vectors\n",
    "        n_samples = 2 ** n_visible\n",
    "        v_vectors = torch.tensor([ [int(x) for x in bin(i)[2:].zfill(n_visible)] for i in range(n_samples)], dtype=torch.float32)\n",
    "        # print(v_vectors)\n",
    "        # Target free energies\n",
    "        F_targets = torch.zeros(n_samples)\n",
    "        for i in range(n_samples):\n",
    "            if v_vectors[i].sum() == 0:\n",
    "                F_targets[i] = 0.0\n",
    "            else:\n",
    "                F_targets[i] = F_s\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            optimizer.zero_grad()\n",
    "            F_v = self.free_energy(v_vectors)\n",
    "            loss = torch.mean((F_v - F_targets) ** 2)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (epoch+1) % 1000 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "    \n",
    "    def free_energy(self, v):\n",
    "        # v: batch_size x n_visible\n",
    "        v_term = torch.sum(self.d * v, dim=1)\n",
    "        # print(v_term)\n",
    "        # print(self.W)\n",
    "        W_expanded = self.W.expand(v.shape[1], self.n_hidden)\n",
    "        # print(W_expanded)\n",
    "        pre_activation = self.b + torch.matmul(v, W_expanded)\n",
    "        # print(pre_activation)\n",
    "        h_term = torch.sum(torch.log1p(torch.exp(pre_activation)), dim=1)\n",
    "        F_v = -v_term - h_term\n",
    "        # print(F_v)\n",
    "        return F_v  # batch_size\n",
    "    \n",
    "rbm = clauseRBM_symmetric()\n",
    "print(rbm.free_energy(torch.tensor([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])))\n",
    "# print(rbm.W)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
