{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import CNFFormula\n",
    "from models.rbm import clauseRBM\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# with open(\"wcnfdata/ram_k3_n18.ra0.wcnf\", 'r') as f:\n",
    "#     cnf_str = f.read()\n",
    "\n",
    "# with open(\"wcnfdata/brock200_3.clq.wcnf\", 'r') as f:\n",
    "#     cnf_str = f.read()\n",
    "\n",
    "with open(\"wcnfdata/ram_k3_n11.ra0.wcnf\", 'r') as f:\n",
    "    cnf_str = f.read()\n",
    "\n",
    "formula = CNFFormula(cnf_str)\n",
    "formula.padding()\n",
    "rbm = clauseRBM(formula.max_clause_length, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new s_max found 483 at step 1\n",
      "new s_max found 485 at step 2\n",
      "new s_max found 486 at step 4\n",
      "new s_max found 487 at step 34082\n",
      "tensor([1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
      "        0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
      "        0.], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2439783c7d0>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "'''\n",
    "# Global variable to store the result of unit propagation\n",
    "# unit_prop_result = None\n",
    "# last_v = None  # To store the last variable assignments\n",
    "# T_global = None  # To store the clauses\n",
    "\n",
    "\n",
    "def send_for_unit_propagation(v, mu):\n",
    "    global last_v, T_global, T\n",
    "    # Store the last variable assignments and clauses for unit propagation\n",
    "    last_v = v.clone()\n",
    "    T_global = T.clone()\n",
    "\n",
    "def fetch_unit_prop_result():\n",
    "    global last_v, T_global\n",
    "    # Perform unit propagation on the last variable assignments\n",
    "    # and return the new variable assignments\n",
    "    unit_prop_result = []\n",
    "    for v_i in last_v:\n",
    "        v_i_new = v_i.clone()\n",
    "        changed = True\n",
    "        while changed:\n",
    "            changed = False\n",
    "            # For each clause\n",
    "            for clause in T_global:\n",
    "                unassigned_literals = []\n",
    "                clause_satisfied = False\n",
    "                for lit in clause:\n",
    "                    if lit == 0:\n",
    "                        continue  # Skip padding zeros\n",
    "                    var_idx = abs(lit) - 1\n",
    "                    var_value = v_i_new[var_idx].item()\n",
    "                    if var_value == -1:\n",
    "                        unassigned_literals.append(lit)\n",
    "                    elif (lit > 0 and var_value == 1) or (lit < 0 and var_value == 0):\n",
    "                        clause_satisfied = True\n",
    "                        break  # Clause is satisfied\n",
    "                if not clause_satisfied and len(unassigned_literals) == 1:\n",
    "                    # Unit clause found; assign the unit literal\n",
    "                    lit = unassigned_literals[0]\n",
    "                    var_idx = abs(lit) - 1\n",
    "                    v_i_new[var_idx] = 1 if lit > 0 else 0\n",
    "                    changed = True  # Changes occurred; need to recheck\n",
    "        unit_prop_result.append(v_i_new)\n",
    "    unit_prop_result = torch.stack(unit_prop_result)\n",
    "    return unit_prop_result\n",
    "'''\n",
    "\n",
    "def gather_and_count(v, Q, polarity):\n",
    "    c = torch.einsum('bv,ckv->bck', v, Q) # (batch_size, num_clause, max_len_clause)\n",
    "    return c, (((polarity + c) == 2) + ((polarity + c) == -1)).any(dim=-1).sum(dim=-1)\n",
    "\n",
    "def time_remaining(start_time, time_limit):\n",
    "    if time.time() - start_time > time_limit:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def construct_Q(T, num_var, device):\n",
    "    Q = torch.zeros(T.shape[0], T.shape[1], num_var, device=device) #(num_clause, len_clause, num_var)\n",
    "    for clause_idx, clause in enumerate(T):\n",
    "        for lit_idx, lit in enumerate(clause):\n",
    "            if lit != 0:\n",
    "                Q[clause_idx, lit_idx, torch.abs(lit)-1] = 1.0\n",
    "    return Q\n",
    "\n",
    "def rbmsat(W_c, b_c, T, B, N, seed, time_limit, upp=100, upw=1, alpha=0.1): # B is batch size, N is the total number of variables\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Initialize variables\n",
    "    s_max = 0 # initially 0 clauses are satisfied\n",
    "    v = torch.bernoulli(torch.ones(B, N, device=W_c.device) * 0.5)  # sample random inits \n",
    "    # mu = 0.25 * torch.ones_like(v) \n",
    "    # Setup tensors\n",
    "    polarity = torch.sign(T)  # strictly {1, -1}, not 0 (-1 2 -3) -> (-1 1 -1)\n",
    "    Q = construct_Q(T, formula.num_vars, device) # C * K * N    (num_clause, max_len_clause, num_var)\n",
    "    W = torch.einsum('ck,kh->ckh', polarity, W_c) # polarity.unsqueeze(1) * W_c.unsqueeze(2) # element-wise multiplication\n",
    "    # print(W)\n",
    "    b = b_c.repeat([T.shape[0], 1]) + torch.mm((1 - polarity) / 2, W_c)\n",
    "    \n",
    "    # t, d = 1, -1\n",
    "    step = 0\n",
    "    s_max_list = []\n",
    "    best_v = torch.ones(N, device=W_c.device)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        while True:\n",
    "            if time_remaining(start_time, time_limit):\n",
    "                step += 1\n",
    "                # if step == 3:\n",
    "                #     break\n",
    "                #     # print(f\"step {step}, {s_max_list[-1]}\")\n",
    "                \n",
    "                # if upp > 0 and d == 0:\n",
    "                #    v_u = torch.vstack([v, fetch_unit_prop_result()])\n",
    "                #    _, s_u = gather_and_count(v_u, Q, polarity)\n",
    "                #    ranks = torch.argsort(s_u)\n",
    "                #    v = v_u[ranks[-B:]]\n",
    "                #    mu = torch.vstack([mu, mu])[ranks[-B:]]\n",
    "                # if upp > 0 and t % upp == 0:\n",
    "                #     d = upw\n",
    "                #     send_for_unit_propagation(v, mu)\n",
    "                    \n",
    "                c, s = gather_and_count(v, Q, polarity)\n",
    "                s_max_step, idx_max = s.max(dim=0)\n",
    "                # print(s_max_step.item())\n",
    "                # print(f\"v:{v[idx_max]}\")\n",
    "                if s_max_step.item() > s_max:\n",
    "                    s_max = s_max_step.item()\n",
    "                    best_v = v[idx_max].clone()\n",
    "                    s_max_list.append((step, s_max))\n",
    "                    print(f\"new s_max found {s_max} at step {step}\")\n",
    "\n",
    "                h_logits = b + torch.einsum('bck,ckh->bch', c, W) # 'bck,chk->bhk'  bck,ckh->bch\n",
    "                \n",
    "                h = torch.bernoulli(torch.sigmoid(h_logits))\n",
    "                # print(h[0,0:10])\n",
    "                ro = torch.sigmoid(torch.einsum('bch,ckh,ckv->bv', h, W, Q)) # 'bhk,chk,ckv->bv'\n",
    "                # print(f\"ro:{ro}\")\n",
    "                # mu = (1 - alpha) * mu + alpha * ro * (1 - ro)\n",
    "                v = torch.bernoulli(ro)\n",
    "                \n",
    "                # t, d = t + 1, max(d - 1, -1)\n",
    "            else:\n",
    "                s_max_list.append((step, s_max_list[-1][-1]))\n",
    "                break\n",
    "        \n",
    "    return s_max_list, best_v\n",
    "\n",
    "T = torch.tensor(formula.clauses, device=device)\n",
    "\n",
    "bath_size = 128\n",
    "seed = 42\n",
    "timeout = 300\n",
    "\n",
    "alpha = 0.1\n",
    "upp = 100\n",
    "upw = 1\n",
    "\n",
    "s_max, best_v = rbmsat(rbm.W, rbm.b, T, bath_size, formula.num_vars, seed, timeout, upp, upw, alpha)\n",
    "print(best_v)\n",
    "x_ax, y_ax = zip(*s_max)\n",
    "plt.step(x_ax,y_ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 1., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.]]], device='cuda:0')\n",
      "tensor([[-1,  1,  0,  0,  0,  0],\n",
      "        [-1,  1,  0,  0,  0,  0],\n",
      "        [-1,  1,  0,  0,  0,  0],\n",
      "        ...,\n",
      "        [-1,  0,  0,  0,  0,  0],\n",
      "        [-1,  0,  0,  0,  0,  0],\n",
      "        [-1,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([2682], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
    "        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1.,\n",
    "        0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
    "        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
    "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
    "        1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device=device)\n",
    "\n",
    "def construct_Q(T, num_var, device):\n",
    "    Q = torch.zeros(T.shape[0], T.shape[1], num_var, device=device) #(num_clause, len_clause, num_var)\n",
    "    for clause_idx, clause in enumerate(T):\n",
    "        for lit_idx, lit in enumerate(clause):\n",
    "            if lit != 0:\n",
    "                Q[clause_idx, lit_idx, torch.abs(lit)-1] = 1.0\n",
    "    return Q\n",
    "\n",
    "def gather_and_count(v, Q, polarity):\n",
    "    c = torch.einsum('bv,ckv->bck', v, Q) # (batch_size, num_clause, max_len_clause)\n",
    "    print(c)\n",
    "    print(polarity)\n",
    "    return c, (((polarity + c) == 2) + ((polarity + c) == -1)).any(dim=-1).sum(dim=-1)\n",
    "\n",
    "T = torch.tensor(formula.clauses, device=device)\n",
    "polarity = torch.sign(T)  # strictly {1, -1}, not 0 (-1 2 -3) -> (-1 1 -1)\n",
    "Q = construct_Q(T, formula.num_vars, device) # C * K * N    (num_clause, max_len_clause, num_var)\n",
    "c, s = gather_and_count(v, Q, polarity)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0.,\n",
      "        0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 1., 1., 0.], grad_fn=<IndexPutBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from utils import unit_propagation, count_satisfied_clauses\n",
    "\n",
    "v = unit_propagation(formula, s_max[1].cpu())\n",
    "# current_best_v, current_best_num_satisfied = count_satisfied_clauses(formula, v)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assignment test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[0,79]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2682\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1.,\n",
      "         0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from utils import CNFFormula\n",
    "from main import count_satisfied_clauses\n",
    "v = torch.tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
    "        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1.,\n",
    "        0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
    "        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
    "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
    "        1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
    "\n",
    "# v_str = \"111111101111111001111111111111010111111111100011011100111011101101110001111111101111011011111111011110011111111111101110110110011110010\"\n",
    "\n",
    "# v = torch.tensor([[int(i) for i in v_str]])\n",
    "\n",
    "with open(\"wcnfdata/uaq-min-nr-nr50-np400-rpp5-nc0-rs0-t0-plb100-n9.wcnf\", 'r') as f:\n",
    "    cnf_str = f.read()\n",
    "formula = CNFFormula(cnf_str)\n",
    "\n",
    "best_v, best_num_satisfied = count_satisfied_clauses(\n",
    "    formula, \n",
    "    v\n",
    ")\n",
    "print(best_num_satisfied)\n",
    "print(best_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
      "         0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
      "         1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
      "         0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "v_str = \"1 -2 -3 -4 -5 -6 7 -8 -9 -10 -11 12 -13 -14 -15 -16 17 -18 -19 20 -21 -22 23 -24 -25 -26 27 -28 -29 30 -31 -32 -33 -34 -35 -36 -37 -38 39 -40 -41 42 -43 -44 45 46 -47 48 49 -50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 -155 156 157 -158 159 160 161 162 163 164 165 166 167 -168 169 170 171 172 173 174 175 -176 177 178 179 180 181 182 183 184 185 -186 187 188 189 190 191 192 -193 194 195 196 197 198 199 200 -201 202 -203 204 205 206 -207 208 209 210 211 212 213 -214 215 -216 -217 218 219 220 221 222 223 -224 225 -226 227 228 229 230 231 232 233 234 235 -236 237 238 -239 -240 241 242 243 244 245 246 -247 248 249 250 251 252 253 254 255 256 257 258 259 260 -261 -262 -263 264 265 266 267 268 269 270 271 272 -273 274 275 276 277 278 279 280 281 -282 283 284 285 286 287 288 289 290 -291 292 -293 294 295 296 -297 -298 299 -300 301 302 303 304 305 306 307 -308 -309 310 311 312 -313 -314 -315 316 317 318 319 320 321 322 323 324 325 326 -327 328 329 330 -331 332 333 334 335 336 337 338 339 340 341 342 343 -344 345 346 347 348 349 350 351 352 -353 354 355 356 357 -358 359 -360 361 362 363 364 -365 -366 367 368 369 -370 -371 372 373 374 375 376 -377 -378 -379 380 381 382 383 384 385 386 387 -388 389 390 391 392 393 -394 -395 -396 -397 398 -399 400 401 402 403 404 405 406 407 408 409 410 411 -412 413 -414 415 416 417 418 419 420 -421 422 423 424 -425 426 427 428 429 430 431 -432 -433 434 -435 -436 437 438 439 -440 441 442 443 444 -445 -446 447 -448 -449 -450\"\n",
    "v_list = v_str.split()\n",
    "v_temp = []\n",
    "for var in v_list:\n",
    "    if int(var) > 0:\n",
    "        v_temp.append(1.0)\n",
    "    else:\n",
    "        v_temp.append(0.0)\n",
    "\n",
    "v_torch = torch.tensor([v_temp])\n",
    "print(v_torch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "53\n",
      "58\n",
      "61\n",
      "66\n",
      "71\n",
      "72\n",
      "75\n",
      "78\n",
      "80\n",
      "82\n",
      "87\n",
      "91\n",
      "100\n",
      "102\n",
      "103\n",
      "108\n",
      "120\n",
      "124\n",
      "127\n",
      "138\n",
      "139\n",
      "141\n",
      "142\n",
      "143\n",
      "145\n",
      "149\n",
      "150\n",
      "163\n",
      "189\n",
      "195\n",
      "214\n",
      "251\n",
      "308\n",
      "327\n",
      "342\n",
      "439\n"
     ]
    }
   ],
   "source": [
    "v_item = v[0].tolist()\n",
    "for idx, var in enumerate(v_item):\n",
    "    if var == 1.0:\n",
    "        print(idx+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brock200_3.clq.wcnf\n",
      "Step 100, Best: 813, current: 810, formula size: 1038, Elapsed Time: 16.84s\n",
      "Step 200, Best: 818, current: 803, formula size: 1038, Elapsed Time: 29.70s\n",
      "Step 300, Best: 818, current: 801, formula size: 1038, Elapsed Time: 44.76s\n",
      "Step 400, Best: 818, current: 801, formula size: 1038, Elapsed Time: 60.85s\n",
      "Step 500, Best: 818, current: 806, formula size: 1038, Elapsed Time: 77.98s\n",
      "Step 600, Best: 818, current: 802, formula size: 1038, Elapsed Time: 97.43s\n",
      "Step 700, Best: 818, current: 805, formula size: 1038, Elapsed Time: 110.33s\n",
      "Step 800, Best: 818, current: 802, formula size: 1038, Elapsed Time: 123.68s\n",
      "Step 900, Best: 818, current: 806, formula size: 1038, Elapsed Time: 136.62s\n",
      "Step 1000, Best: 818, current: 803, formula size: 1038, Elapsed Time: 149.38s\n",
      "Step 1100, Best: 818, current: 808, formula size: 1038, Elapsed Time: 162.05s\n",
      "Step 1200, Best: 818, current: 796, formula size: 1038, Elapsed Time: 174.84s\n",
      "Step 1300, Best: 818, current: 804, formula size: 1038, Elapsed Time: 187.38s\n",
      "Step 1400, Best: 818, current: 804, formula size: 1038, Elapsed Time: 199.90s\n",
      "Step 1500, Best: 818, current: 804, formula size: 1038, Elapsed Time: 212.75s\n",
      "Step 1600, Best: 818, current: 797, formula size: 1038, Elapsed Time: 226.16s\n",
      "Step 1700, Best: 818, current: 805, formula size: 1038, Elapsed Time: 239.29s\n",
      "Step 1800, Best: 818, current: 799, formula size: 1038, Elapsed Time: 252.24s\n",
      "Step 1900, Best: 818, current: 800, formula size: 1038, Elapsed Time: 276.34s\n",
      "Step 2000, Best: 818, current: 798, formula size: 1038, Elapsed Time: 302.01s\n",
      "Time limit reached: 300 seconds\n",
      "brock200_3.clq.wcnf, 818\n",
      "data.135.wcnf\n",
      "Step 100, Best: 3016, current: 2608, formula size: 3150, Elapsed Time: 85.12s\n",
      "Step 200, Best: 3016, current: 2605, formula size: 3150, Elapsed Time: 128.71s\n",
      "Step 300, Best: 3016, current: 2602, formula size: 3150, Elapsed Time: 177.22s\n",
      "Step 400, Best: 3016, current: 2605, formula size: 3150, Elapsed Time: 222.67s\n",
      "Step 500, Best: 3016, current: 2608, formula size: 3150, Elapsed Time: 266.49s\n",
      "Step 600, Best: 3016, current: 2607, formula size: 3150, Elapsed Time: 311.06s\n",
      "Time limit reached: 300 seconds\n",
      "data.135.wcnf, 3016\n",
      "data.243.wcnf\n",
      "Step 100, Best: 9801, current: 8712, formula size: 10044, Elapsed Time: 205.07s\n",
      "Time limit reached: 300 seconds\n",
      "data.243.wcnf, 9801\n",
      "data.405.wcnf\n",
      "Time limit reached: 300 seconds\n",
      "data.405.wcnf, 27270\n",
      "data.729.wcnf\n",
      "Time limit reached: 300 seconds\n",
      "data.729.wcnf, 88452\n",
      "hamming10-4.clq.wcnf\n",
      "Time limit reached: 300 seconds\n",
      "hamming10-4.clq.wcnf, 88575\n",
      "maxcut-140-630-0.7-12.wcnf\n",
      "Step 100, Best: 1074, current: 1049, formula size: 1260, Elapsed Time: 19.77s\n",
      "Step 200, Best: 1074, current: 1051, formula size: 1260, Elapsed Time: 35.73s\n",
      "Step 300, Best: 1074, current: 1047, formula size: 1260, Elapsed Time: 51.82s\n",
      "Step 400, Best: 1074, current: 1044, formula size: 1260, Elapsed Time: 68.05s\n",
      "Step 500, Best: 1074, current: 1049, formula size: 1260, Elapsed Time: 84.37s\n",
      "Step 600, Best: 1074, current: 1042, formula size: 1260, Elapsed Time: 100.39s\n",
      "Step 700, Best: 1074, current: 1045, formula size: 1260, Elapsed Time: 116.11s\n",
      "Step 800, Best: 1074, current: 1050, formula size: 1260, Elapsed Time: 132.74s\n",
      "Step 900, Best: 1074, current: 1047, formula size: 1260, Elapsed Time: 154.22s\n",
      "Step 1000, Best: 1074, current: 1044, formula size: 1260, Elapsed Time: 174.82s\n",
      "Step 1100, Best: 1074, current: 1045, formula size: 1260, Elapsed Time: 195.13s\n",
      "Step 1200, Best: 1075, current: 1043, formula size: 1260, Elapsed Time: 214.91s\n",
      "Step 1300, Best: 1075, current: 1045, formula size: 1260, Elapsed Time: 235.70s\n",
      "Step 1400, Best: 1075, current: 1041, formula size: 1260, Elapsed Time: 254.55s\n",
      "Step 1500, Best: 1075, current: 1050, formula size: 1260, Elapsed Time: 270.32s\n",
      "Step 1600, Best: 1075, current: 1050, formula size: 1260, Elapsed Time: 285.83s\n",
      "Step 1700, Best: 1075, current: 1043, formula size: 1260, Elapsed Time: 301.28s\n",
      "Time limit reached: 300 seconds\n",
      "maxcut-140-630-0.7-12.wcnf, 1075\n",
      "maxcut-140-630-0.7-20.wcnf\n",
      "Step 100, Best: 1072, current: 1044, formula size: 1260, Elapsed Time: 18.98s\n",
      "Step 200, Best: 1072, current: 1040, formula size: 1260, Elapsed Time: 34.23s\n",
      "Step 300, Best: 1073, current: 1039, formula size: 1260, Elapsed Time: 49.56s\n",
      "Step 400, Best: 1073, current: 1042, formula size: 1260, Elapsed Time: 64.79s\n",
      "Step 500, Best: 1073, current: 1043, formula size: 1260, Elapsed Time: 79.99s\n",
      "Step 600, Best: 1073, current: 1040, formula size: 1260, Elapsed Time: 95.21s\n",
      "Step 700, Best: 1073, current: 1039, formula size: 1260, Elapsed Time: 110.46s\n",
      "Step 800, Best: 1073, current: 1041, formula size: 1260, Elapsed Time: 125.68s\n",
      "Step 900, Best: 1073, current: 1046, formula size: 1260, Elapsed Time: 140.94s\n",
      "Step 1000, Best: 1073, current: 1042, formula size: 1260, Elapsed Time: 156.13s\n",
      "Step 1100, Best: 1073, current: 1040, formula size: 1260, Elapsed Time: 171.56s\n",
      "Step 1200, Best: 1074, current: 1041, formula size: 1260, Elapsed Time: 187.17s\n",
      "Step 1300, Best: 1074, current: 1037, formula size: 1260, Elapsed Time: 202.43s\n",
      "Step 1400, Best: 1074, current: 1040, formula size: 1260, Elapsed Time: 217.64s\n",
      "Step 1500, Best: 1074, current: 1043, formula size: 1260, Elapsed Time: 232.85s\n",
      "Step 1600, Best: 1074, current: 1040, formula size: 1260, Elapsed Time: 248.06s\n",
      "Step 1700, Best: 1074, current: 1039, formula size: 1260, Elapsed Time: 263.25s\n",
      "Step 1800, Best: 1074, current: 1043, formula size: 1260, Elapsed Time: 278.49s\n",
      "Step 1900, Best: 1074, current: 1041, formula size: 1260, Elapsed Time: 293.77s\n",
      "Time limit reached: 300 seconds\n",
      "maxcut-140-630-0.7-20.wcnf, 1074\n",
      "maxcut-140-630-0.7-28.wcnf\n",
      "Step 100, Best: 1061, current: 1028, formula size: 1260, Elapsed Time: 19.29s\n",
      "Step 200, Best: 1065, current: 1040, formula size: 1260, Elapsed Time: 34.89s\n",
      "Step 300, Best: 1066, current: 1042, formula size: 1260, Elapsed Time: 50.54s\n",
      "Step 400, Best: 1066, current: 1046, formula size: 1260, Elapsed Time: 66.16s\n",
      "Step 500, Best: 1066, current: 1043, formula size: 1260, Elapsed Time: 81.67s\n",
      "Step 600, Best: 1066, current: 1041, formula size: 1260, Elapsed Time: 97.33s\n",
      "Step 700, Best: 1066, current: 1034, formula size: 1260, Elapsed Time: 113.23s\n",
      "Step 800, Best: 1066, current: 1042, formula size: 1260, Elapsed Time: 129.33s\n",
      "Step 900, Best: 1066, current: 1036, formula size: 1260, Elapsed Time: 144.93s\n",
      "Step 1000, Best: 1066, current: 1036, formula size: 1260, Elapsed Time: 160.48s\n",
      "Step 1100, Best: 1066, current: 1035, formula size: 1260, Elapsed Time: 176.29s\n",
      "Step 1200, Best: 1066, current: 1049, formula size: 1260, Elapsed Time: 191.91s\n",
      "Step 1300, Best: 1066, current: 1040, formula size: 1260, Elapsed Time: 207.44s\n",
      "Step 1400, Best: 1067, current: 1036, formula size: 1260, Elapsed Time: 223.18s\n",
      "Step 1500, Best: 1067, current: 1037, formula size: 1260, Elapsed Time: 238.79s\n",
      "Step 1600, Best: 1067, current: 1039, formula size: 1260, Elapsed Time: 254.39s\n",
      "Step 1700, Best: 1067, current: 1035, formula size: 1260, Elapsed Time: 269.97s\n",
      "Step 1800, Best: 1067, current: 1045, formula size: 1260, Elapsed Time: 285.54s\n",
      "Step 1900, Best: 1067, current: 1040, formula size: 1260, Elapsed Time: 301.38s\n",
      "Time limit reached: 300 seconds\n",
      "maxcut-140-630-0.7-28.wcnf, 1067\n",
      "maxcut-140-630-0.7-4.wcnf\n",
      "Step 100, Best: 1062, current: 1035, formula size: 1260, Elapsed Time: 19.13s\n",
      "Step 200, Best: 1062, current: 1037, formula size: 1260, Elapsed Time: 34.59s\n",
      "Step 300, Best: 1062, current: 1039, formula size: 1260, Elapsed Time: 50.31s\n",
      "Step 400, Best: 1064, current: 1034, formula size: 1260, Elapsed Time: 66.42s\n",
      "Step 500, Best: 1064, current: 1038, formula size: 1260, Elapsed Time: 82.13s\n",
      "Step 600, Best: 1064, current: 1039, formula size: 1260, Elapsed Time: 97.58s\n",
      "Step 700, Best: 1064, current: 1038, formula size: 1260, Elapsed Time: 113.12s\n",
      "Step 800, Best: 1064, current: 1037, formula size: 1260, Elapsed Time: 128.62s\n",
      "Step 900, Best: 1064, current: 1031, formula size: 1260, Elapsed Time: 144.08s\n",
      "Step 1000, Best: 1064, current: 1041, formula size: 1260, Elapsed Time: 159.57s\n",
      "Step 1100, Best: 1064, current: 1041, formula size: 1260, Elapsed Time: 175.02s\n",
      "Step 1200, Best: 1064, current: 1040, formula size: 1260, Elapsed Time: 190.54s\n",
      "Step 1300, Best: 1064, current: 1037, formula size: 1260, Elapsed Time: 205.99s\n",
      "Step 1400, Best: 1064, current: 1039, formula size: 1260, Elapsed Time: 221.49s\n",
      "Step 1500, Best: 1066, current: 1037, formula size: 1260, Elapsed Time: 237.01s\n",
      "Step 1600, Best: 1066, current: 1037, formula size: 1260, Elapsed Time: 252.51s\n",
      "Step 1700, Best: 1066, current: 1037, formula size: 1260, Elapsed Time: 268.02s\n",
      "Step 1800, Best: 1066, current: 1038, formula size: 1260, Elapsed Time: 283.57s\n",
      "Step 1900, Best: 1066, current: 1035, formula size: 1260, Elapsed Time: 299.89s\n",
      "Time limit reached: 300 seconds\n",
      "maxcut-140-630-0.7-4.wcnf, 1066\n",
      "maxcut-140-630-0.7-45.wcnf\n",
      "Step 100, Best: 1058, current: 1033, formula size: 1260, Elapsed Time: 20.52s\n",
      "Step 200, Best: 1058, current: 1041, formula size: 1260, Elapsed Time: 36.74s\n",
      "Step 300, Best: 1058, current: 1037, formula size: 1260, Elapsed Time: 52.77s\n",
      "Step 400, Best: 1058, current: 1034, formula size: 1260, Elapsed Time: 68.81s\n",
      "Step 500, Best: 1058, current: 1037, formula size: 1260, Elapsed Time: 85.03s\n",
      "Step 600, Best: 1058, current: 1039, formula size: 1260, Elapsed Time: 101.06s\n",
      "Step 700, Best: 1058, current: 1038, formula size: 1260, Elapsed Time: 117.05s\n",
      "Step 800, Best: 1058, current: 1037, formula size: 1260, Elapsed Time: 133.03s\n",
      "Step 900, Best: 1058, current: 1040, formula size: 1260, Elapsed Time: 148.97s\n",
      "Step 1000, Best: 1058, current: 1034, formula size: 1260, Elapsed Time: 165.13s\n",
      "Step 1100, Best: 1058, current: 1039, formula size: 1260, Elapsed Time: 181.12s\n",
      "Step 1200, Best: 1058, current: 1035, formula size: 1260, Elapsed Time: 197.12s\n",
      "Step 1300, Best: 1058, current: 1036, formula size: 1260, Elapsed Time: 213.00s\n",
      "Step 1400, Best: 1058, current: 1033, formula size: 1260, Elapsed Time: 229.00s\n",
      "Step 1500, Best: 1058, current: 1043, formula size: 1260, Elapsed Time: 245.70s\n",
      "Step 1600, Best: 1058, current: 1037, formula size: 1260, Elapsed Time: 262.08s\n",
      "Step 1700, Best: 1058, current: 1039, formula size: 1260, Elapsed Time: 278.11s\n",
      "Step 1800, Best: 1058, current: 1036, formula size: 1260, Elapsed Time: 296.53s\n",
      "Time limit reached: 300 seconds\n",
      "maxcut-140-630-0.7-45.wcnf, 1058\n",
      "maxcut-140-630-0.7-6.wcnf\n",
      "Step 100, Best: 1074, current: 1056, formula size: 1260, Elapsed Time: 34.24s\n",
      "Step 200, Best: 1082, current: 1054, formula size: 1260, Elapsed Time: 50.73s\n",
      "Step 300, Best: 1082, current: 1053, formula size: 1260, Elapsed Time: 66.48s\n",
      "Step 400, Best: 1082, current: 1054, formula size: 1260, Elapsed Time: 82.34s\n",
      "Step 500, Best: 1082, current: 1056, formula size: 1260, Elapsed Time: 98.12s\n",
      "Step 600, Best: 1082, current: 1056, formula size: 1260, Elapsed Time: 114.04s\n",
      "Step 700, Best: 1082, current: 1059, formula size: 1260, Elapsed Time: 129.94s\n",
      "Step 800, Best: 1082, current: 1052, formula size: 1260, Elapsed Time: 145.59s\n",
      "Step 900, Best: 1082, current: 1056, formula size: 1260, Elapsed Time: 161.08s\n",
      "Step 1000, Best: 1082, current: 1053, formula size: 1260, Elapsed Time: 177.67s\n",
      "Step 1100, Best: 1082, current: 1055, formula size: 1260, Elapsed Time: 194.36s\n",
      "Step 1200, Best: 1082, current: 1053, formula size: 1260, Elapsed Time: 209.97s\n",
      "Step 1300, Best: 1082, current: 1055, formula size: 1260, Elapsed Time: 225.69s\n",
      "Step 1400, Best: 1082, current: 1052, formula size: 1260, Elapsed Time: 241.54s\n",
      "Step 1500, Best: 1082, current: 1053, formula size: 1260, Elapsed Time: 257.40s\n",
      "Step 1600, Best: 1082, current: 1053, formula size: 1260, Elapsed Time: 273.21s\n",
      "Step 1700, Best: 1082, current: 1053, formula size: 1260, Elapsed Time: 289.23s\n",
      "Step 1800, Best: 1082, current: 1052, formula size: 1260, Elapsed Time: 305.62s\n",
      "Time limit reached: 300 seconds\n",
      "maxcut-140-630-0.7-6.wcnf, 1082\n",
      "maxcut-140-630-0.8-1.wcnf\n",
      "Step 100, Best: 1074, current: 1044, formula size: 1258, Elapsed Time: 19.77s\n",
      "Step 200, Best: 1074, current: 1035, formula size: 1258, Elapsed Time: 35.26s\n",
      "Step 300, Best: 1074, current: 1039, formula size: 1258, Elapsed Time: 50.95s\n",
      "Step 400, Best: 1074, current: 1035, formula size: 1258, Elapsed Time: 66.81s\n",
      "Step 500, Best: 1074, current: 1034, formula size: 1258, Elapsed Time: 82.39s\n",
      "Step 600, Best: 1075, current: 1040, formula size: 1258, Elapsed Time: 99.86s\n",
      "Step 700, Best: 1075, current: 1039, formula size: 1258, Elapsed Time: 119.26s\n",
      "Step 800, Best: 1075, current: 1045, formula size: 1258, Elapsed Time: 135.37s\n",
      "Step 900, Best: 1075, current: 1044, formula size: 1258, Elapsed Time: 151.30s\n",
      "Step 1000, Best: 1075, current: 1040, formula size: 1258, Elapsed Time: 167.39s\n",
      "Step 1100, Best: 1075, current: 1035, formula size: 1258, Elapsed Time: 183.63s\n",
      "Step 1200, Best: 1075, current: 1034, formula size: 1258, Elapsed Time: 200.41s\n",
      "Step 1300, Best: 1075, current: 1036, formula size: 1258, Elapsed Time: 217.94s\n",
      "Step 1400, Best: 1075, current: 1037, formula size: 1258, Elapsed Time: 236.18s\n",
      "Step 1500, Best: 1075, current: 1037, formula size: 1258, Elapsed Time: 253.62s\n",
      "Step 1600, Best: 1075, current: 1046, formula size: 1258, Elapsed Time: 272.28s\n",
      "Step 1700, Best: 1075, current: 1037, formula size: 1258, Elapsed Time: 289.80s\n",
      "Time limit reached: 300 seconds\n",
      "maxcut-140-630-0.8-1.wcnf, 1075\n",
      "maxcut-140-630-0.8-18.wcnf\n",
      "Step 100, Best: 1071, current: 1039, formula size: 1258, Elapsed Time: 24.41s\n",
      "Step 200, Best: 1071, current: 1037, formula size: 1258, Elapsed Time: 42.72s\n",
      "Step 300, Best: 1074, current: 1036, formula size: 1258, Elapsed Time: 60.92s\n",
      "Step 400, Best: 1074, current: 1035, formula size: 1258, Elapsed Time: 78.66s\n",
      "Step 500, Best: 1074, current: 1030, formula size: 1258, Elapsed Time: 97.75s\n",
      "Step 600, Best: 1074, current: 1035, formula size: 1258, Elapsed Time: 116.52s\n",
      "Step 700, Best: 1074, current: 1035, formula size: 1258, Elapsed Time: 134.15s\n",
      "Step 800, Best: 1074, current: 1042, formula size: 1258, Elapsed Time: 152.00s\n",
      "Step 900, Best: 1075, current: 1030, formula size: 1258, Elapsed Time: 169.41s\n",
      "Step 1000, Best: 1075, current: 1042, formula size: 1258, Elapsed Time: 187.79s\n",
      "Step 1100, Best: 1075, current: 1031, formula size: 1258, Elapsed Time: 205.38s\n",
      "Step 1200, Best: 1075, current: 1033, formula size: 1258, Elapsed Time: 223.03s\n",
      "Step 1300, Best: 1075, current: 1036, formula size: 1258, Elapsed Time: 239.16s\n",
      "Step 1400, Best: 1075, current: 1035, formula size: 1258, Elapsed Time: 255.30s\n",
      "Step 1500, Best: 1075, current: 1038, formula size: 1258, Elapsed Time: 271.51s\n",
      "Step 1600, Best: 1075, current: 1033, formula size: 1258, Elapsed Time: 287.26s\n",
      "Step 1700, Best: 1075, current: 1032, formula size: 1258, Elapsed Time: 303.01s\n",
      "Time limit reached: 300 seconds\n",
      "maxcut-140-630-0.8-18.wcnf, 1075\n",
      "maxcut-140-630-0.8-26.wcnf\n",
      "Step 100, Best: 1059, current: 1026, formula size: 1258, Elapsed Time: 19.47s\n",
      "Step 200, Best: 1059, current: 1025, formula size: 1258, Elapsed Time: 36.01s\n",
      "Step 300, Best: 1059, current: 1027, formula size: 1258, Elapsed Time: 52.20s\n",
      "Step 400, Best: 1059, current: 1035, formula size: 1258, Elapsed Time: 68.21s\n",
      "Step 500, Best: 1059, current: 1030, formula size: 1258, Elapsed Time: 84.32s\n",
      "Step 600, Best: 1059, current: 1025, formula size: 1258, Elapsed Time: 100.93s\n",
      "Step 700, Best: 1059, current: 1027, formula size: 1258, Elapsed Time: 117.16s\n",
      "Step 800, Best: 1059, current: 1025, formula size: 1258, Elapsed Time: 133.50s\n",
      "Step 900, Best: 1059, current: 1027, formula size: 1258, Elapsed Time: 149.87s\n",
      "Step 1000, Best: 1059, current: 1025, formula size: 1258, Elapsed Time: 165.66s\n",
      "Step 1100, Best: 1059, current: 1026, formula size: 1258, Elapsed Time: 181.75s\n",
      "Step 1200, Best: 1059, current: 1034, formula size: 1258, Elapsed Time: 197.99s\n",
      "Step 1300, Best: 1059, current: 1029, formula size: 1258, Elapsed Time: 214.43s\n",
      "Step 1400, Best: 1059, current: 1030, formula size: 1258, Elapsed Time: 230.72s\n",
      "Step 1500, Best: 1059, current: 1036, formula size: 1258, Elapsed Time: 246.74s\n",
      "Step 1600, Best: 1059, current: 1034, formula size: 1258, Elapsed Time: 262.76s\n",
      "Step 1700, Best: 1059, current: 1026, formula size: 1258, Elapsed Time: 279.20s\n",
      "Step 1800, Best: 1059, current: 1029, formula size: 1258, Elapsed Time: 295.41s\n",
      "Time limit reached: 300 seconds\n",
      "maxcut-140-630-0.8-26.wcnf, 1059\n",
      "maxcut-140-630-0.8-39.wcnf\n",
      "Step 100, Best: 1064, current: 1037, formula size: 1258, Elapsed Time: 19.70s\n",
      "Step 200, Best: 1064, current: 1033, formula size: 1258, Elapsed Time: 36.03s\n",
      "Step 300, Best: 1064, current: 1030, formula size: 1258, Elapsed Time: 51.98s\n",
      "Step 400, Best: 1064, current: 1032, formula size: 1258, Elapsed Time: 68.36s\n",
      "Step 500, Best: 1064, current: 1036, formula size: 1258, Elapsed Time: 84.74s\n",
      "Step 600, Best: 1065, current: 1032, formula size: 1258, Elapsed Time: 100.83s\n",
      "Step 700, Best: 1065, current: 1039, formula size: 1258, Elapsed Time: 116.46s\n",
      "Step 800, Best: 1065, current: 1032, formula size: 1258, Elapsed Time: 132.38s\n",
      "Step 900, Best: 1065, current: 1038, formula size: 1258, Elapsed Time: 148.84s\n",
      "Step 1000, Best: 1065, current: 1034, formula size: 1258, Elapsed Time: 165.29s\n",
      "Step 1100, Best: 1065, current: 1041, formula size: 1258, Elapsed Time: 181.60s\n",
      "Step 1200, Best: 1065, current: 1034, formula size: 1258, Elapsed Time: 197.52s\n",
      "Step 1300, Best: 1065, current: 1036, formula size: 1258, Elapsed Time: 213.31s\n",
      "Step 1400, Best: 1065, current: 1029, formula size: 1258, Elapsed Time: 229.57s\n",
      "Step 1500, Best: 1065, current: 1033, formula size: 1258, Elapsed Time: 245.67s\n",
      "Step 1600, Best: 1065, current: 1036, formula size: 1258, Elapsed Time: 261.26s\n",
      "Step 1700, Best: 1065, current: 1033, formula size: 1258, Elapsed Time: 277.22s\n",
      "Step 1800, Best: 1065, current: 1035, formula size: 1258, Elapsed Time: 293.39s\n",
      "Time limit reached: 300 seconds\n",
      "maxcut-140-630-0.8-39.wcnf, 1065\n",
      "MinFill_R0_queen5_5.wcnf\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "einsum(): subscript h has size 7 for operand 1 which does not broadcast with previously seen size 8",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     formula_str \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     13\u001b[0m formula \u001b[38;5;241m=\u001b[39m CNFFormula\u001b[38;5;241m.\u001b[39mfrom_dimacs(formula_str)\n\u001b[1;32m---> 14\u001b[0m best_v, best_num_satisfied \u001b[38;5;241m=\u001b[39m \u001b[43msolve_maxsat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mformula\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheuristic_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m best_v_list\u001b[38;5;241m.\u001b[39mappend(best_v)\n\u001b[0;32m     21\u001b[0m best_num_satisfied_list\u001b[38;5;241m.\u001b[39mappend(best_num_satisfied)\n",
      "File \u001b[1;32md:\\academic\\proj2024\\rbmsat\\main.py:70\u001b[0m, in \u001b[0;36msolve_maxsat\u001b[1;34m(formula, max_time, heuristic_interval, batch_size, device)\u001b[0m\n\u001b[0;32m     68\u001b[0m h_sample, _ \u001b[38;5;241m=\u001b[39m rbm\u001b[38;5;241m.\u001b[39msample_h_given_v(v)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Sample v given h\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m v_sample, p_v_given_h \u001b[38;5;241m=\u001b[39m \u001b[43mrbm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_v_given_h\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m v \u001b[38;5;241m=\u001b[39m v_sample\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Update moving averages of _i\u001b[39;00m\n",
      "File \u001b[1;32md:\\academic\\proj2024\\rbmsat\\models\\rbm_parallel.py:97\u001b[0m, in \u001b[0;36mformulaRBM_parallel.sample_v_given_h\u001b[1;34m(self, h)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample_v_given_h\u001b[39m(\u001b[38;5;28mself\u001b[39m, h):\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;66;03m# h: B x C x H_c\u001b[39;00m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;66;03m# Compute contributions from hidden units\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m     c_logits \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbch,ckh->bck\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW_full\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# B x C x K\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;66;03m# Adjust for polarities\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     c_logits \u001b[38;5;241m=\u001b[39m c_logits \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolarities\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda\\Lib\\site-packages\\torch\\functional.py:386\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[1;32m--> 386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    388\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[1;31mRuntimeError\u001b[0m: einsum(): subscript h has size 7 for operand 1 which does not broadcast with previously seen size 8"
     ]
    }
   ],
   "source": [
    "from utils import CNFFormula\n",
    "from main import solve_maxsat\n",
    "import os\n",
    "\n",
    "best_v_list = []\n",
    "best_num_satisfied_list = []\n",
    "# test all files in ../wcnfdata\n",
    "for file in os.listdir(\"./wcnfdata\"):\n",
    "    print(file)\n",
    "    with open(os.path.join(\"./wcnfdata\", file), 'r') as f:\n",
    "        formula_str = f.read()\n",
    "    \n",
    "    formula = CNFFormula(formula_str)\n",
    "    best_v, best_num_satisfied = solve_maxsat(\n",
    "        formula, \n",
    "        max_time=300, \n",
    "        heuristic_interval=100, \n",
    "        batch_size=1024)\n",
    "\n",
    "    best_v_list.append(best_v)\n",
    "    best_num_satisfied_list.append(best_num_satisfied)\n",
    "\n",
    "    print(f\"{file}, {best_num_satisfied}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "822\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from utils import CNFFormula\n",
    "from main import count_satisfied_clauses\n",
    "v = torch.tensor([[0,1,1,0,0,0,0,0,1,1,1,0,1,1,1,0,0,1,0,0,1,1,0,1,1,0,0,1,0,0,1,0,1,1,0,1,1,0,0,1]])\n",
    "\n",
    "with open('./wcnfdata/brock200_3.clq.wcnf', 'r') as f:\n",
    "    formula_str = f.read()\n",
    "formula = CNFFormula(formula_str)\n",
    "best_v, best_num_satisfied = count_satisfied_clauses(\n",
    "    formula, \n",
    "    v\n",
    ")\n",
    "print(best_num_satisfied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pre-trained RBM to rbm_models\\rbm_length_5_F_-1.0_num_epochs_0_lr_0.01_device_cpu.pth\n",
      "Epoch 200, Loss: 0.40392351150512695\n",
      "Epoch 400, Loss: 0.15035410225391388\n",
      "Epoch 600, Loss: 0.07847076654434204\n",
      "Epoch 800, Loss: 0.054054055362939835\n",
      "Epoch 1000, Loss: 0.0438944511115551\n",
      "Epoch 1200, Loss: 0.03873978182673454\n",
      "Epoch 1400, Loss: 0.03555965796113014\n",
      "Epoch 1600, Loss: 0.033251821994781494\n",
      "Epoch 1800, Loss: 0.031401023268699646\n",
      "Epoch 2000, Loss: 0.0298561193048954\n",
      "Epoch 2200, Loss: 0.028566580265760422\n",
      "Epoch 2400, Loss: 0.027515148743987083\n",
      "Epoch 2600, Loss: 0.026690170168876648\n",
      "Epoch 2800, Loss: 0.026074139401316643\n",
      "Epoch 3000, Loss: 0.02563847415149212\n",
      "Epoch 3200, Loss: 0.025342069566249847\n",
      "Epoch 3400, Loss: 0.025134537369012833\n",
      "Epoch 3600, Loss: 0.02496362291276455\n",
      "Epoch 3800, Loss: 0.024781839922070503\n",
      "Epoch 4000, Loss: 0.024546733126044273\n",
      "Epoch 4200, Loss: 0.024215035140514374\n",
      "Epoch 4400, Loss: 0.02373451739549637\n",
      "Epoch 4600, Loss: 0.02303897961974144\n",
      "Epoch 4800, Loss: 0.022056490182876587\n",
      "Epoch 5000, Loss: 0.02074250765144825\n",
      "Epoch 5200, Loss: 0.01912299171090126\n",
      "Epoch 5400, Loss: 0.01727711223065853\n",
      "Epoch 5600, Loss: 0.015307983383536339\n",
      "Epoch 5800, Loss: 0.013373110443353653\n",
      "Epoch 6000, Loss: 0.011549542658030987\n",
      "Epoch 6200, Loss: 0.009875369258224964\n",
      "Epoch 6400, Loss: 0.008388581685721874\n",
      "Epoch 6600, Loss: 0.007084610406309366\n",
      "Epoch 6800, Loss: 0.005943823605775833\n",
      "Epoch 7000, Loss: 0.0049563259817659855\n",
      "Epoch 7200, Loss: 0.004117242991924286\n",
      "Epoch 7400, Loss: 0.0034164630342274904\n",
      "Epoch 7600, Loss: 0.0028380085714161396\n",
      "Epoch 7800, Loss: 0.002363809384405613\n",
      "Epoch 8000, Loss: 0.0019763780292123556\n",
      "Epoch 8200, Loss: 0.0016599998343735933\n",
      "Epoch 8400, Loss: 0.0014011700404807925\n",
      "Epoch 8600, Loss: 0.001188668073154986\n",
      "Epoch 8800, Loss: 0.0010133805917575955\n",
      "Epoch 9000, Loss: 0.0008680210448801517\n",
      "Epoch 9200, Loss: 0.0007468013791367412\n",
      "Epoch 9400, Loss: 0.0006451421650126576\n",
      "Epoch 9600, Loss: 0.0005594170652329922\n",
      "Epoch 9800, Loss: 0.000486749573610723\n",
      "Epoch 10000, Loss: 0.0004248454642947763\n",
      "Epoch 10200, Loss: 0.0003718665393535048\n",
      "Epoch 10400, Loss: 0.00032633013324812055\n",
      "Epoch 10600, Loss: 0.000287036964436993\n",
      "Epoch 10800, Loss: 0.00025300716515630484\n",
      "Epoch 11000, Loss: 0.0002234371640952304\n",
      "Epoch 11200, Loss: 0.00019766455807257444\n",
      "Epoch 11400, Loss: 0.000175138731719926\n",
      "Epoch 11600, Loss: 0.00015540105232503265\n",
      "Epoch 11800, Loss: 0.0001381541951559484\n",
      "Epoch 12000, Loss: 0.00012305565178394318\n",
      "Epoch 12200, Loss: 0.00010980173101415858\n",
      "Epoch 12400, Loss: 9.81421981123276e-05\n",
      "Epoch 12600, Loss: 8.78721839399077e-05\n",
      "Epoch 12800, Loss: 8.203398465411738e-05\n",
      "Epoch 13000, Loss: 7.085879042278975e-05\n",
      "Epoch 13200, Loss: 6.366102752508596e-05\n",
      "Epoch 13400, Loss: 5.7363293308299035e-05\n",
      "Epoch 13600, Loss: 5.1768038247246295e-05\n",
      "Epoch 13800, Loss: 4.716054172604345e-05\n",
      "Epoch 14000, Loss: 4.428903048392385e-05\n",
      "Epoch 14200, Loss: 5.9192359913140535e-05\n",
      "Epoch 14400, Loss: 3.509763701003976e-05\n",
      "Epoch 14600, Loss: 3.170587297063321e-05\n",
      "Epoch 14800, Loss: 2.8883041522931308e-05\n",
      "Epoch 15000, Loss: 2.635696728248149e-05\n",
      "Epoch 15200, Loss: 2.408993532299064e-05\n",
      "Epoch 15400, Loss: 2.2055515728425235e-05\n",
      "Epoch 15600, Loss: 2.0227511413395405e-05\n",
      "Epoch 15800, Loss: 1.8578704839455895e-05\n",
      "Epoch 16000, Loss: 1.9340182916494086e-05\n",
      "Epoch 16200, Loss: 1.5759673260618e-05\n",
      "Epoch 16400, Loss: 1.455203255318338e-05\n",
      "Epoch 16600, Loss: 1.3461986782203894e-05\n",
      "Epoch 16800, Loss: 1.468576374463737e-05\n",
      "Epoch 17000, Loss: 1.1578315024962649e-05\n",
      "Epoch 17200, Loss: 1.076745229511289e-05\n",
      "Epoch 17400, Loss: 1.0032485079136677e-05\n",
      "Epoch 17600, Loss: 9.36283413466299e-06\n",
      "Epoch 17800, Loss: 8.84791006683372e-06\n",
      "Epoch 18000, Loss: 9.364960533275735e-06\n",
      "Epoch 18200, Loss: 7.694693522353191e-06\n",
      "Epoch 18400, Loss: 7.237149475258775e-06\n",
      "Epoch 18600, Loss: 6.8171698330843356e-06\n",
      "Epoch 18800, Loss: 6.437068350351183e-06\n",
      "Epoch 19000, Loss: 1.031122974382015e-05\n",
      "Epoch 19200, Loss: 5.763316949014552e-06\n",
      "Epoch 19400, Loss: 5.470068572321907e-06\n",
      "Epoch 19600, Loss: 5.1998263188579585e-06\n",
      "Epoch 19800, Loss: 4.951129085384309e-06\n",
      "Epoch 20000, Loss: 4.740450549434172e-06\n",
      "Saved pre-trained RBM to rbm_models\\rbm_length_5_F_-1.0_num_epochs_20000_lr_0.01_device_cpu.pth\n",
      "-4.158883094787598 -4.268941879272461 -4.259560585021973 -4.032647609710693 -4.160459041595459 -4.262772083282471\n",
      "-0.011785866692662239 -0.9983624219894409 -1.000793218612671 -0.9994568228721619 -0.9998423457145691 -1.0006365776062012\n"
     ]
    }
   ],
   "source": [
    "from models.rbm import clauseRBM\n",
    "import torch\n",
    "\n",
    "rbm_random = clauseRBM(5, F_s=-1.0, num_epochs=0, lr=0.01, device='cpu', verbose=True)\n",
    "rbm_trained = clauseRBM(5, F_s=-1.0, num_epochs=20000, lr=0.01, device='cpu', verbose=True)\n",
    "\n",
    "v_neg = torch.tensor([[0.0, 0.0, 0.0, 0.0, 0.0]])\n",
    "v_pos1 = torch.tensor([[1.0, 0.0, 0.0, 0.0, 0.0]])\n",
    "v_pos2 = torch.tensor([[1.0, 1.0, 0.0, 0.0, 0.0]])\n",
    "v_pos3 = torch.tensor([[1.0, 1.0, 1.0, 0.0, 0.0]])\n",
    "v_pos4 = torch.tensor([[1.0, 1.0, 1.0, 1.0, 0.0]])\n",
    "v_pos5 = torch.tensor([[1.0, 1.0, 1.0, 1.0, 1.0]])\n",
    "\n",
    "print(rbm_random.free_energy(v_neg).item(), rbm_random.free_energy(v_pos1).item(), rbm_random.free_energy(v_pos2).item(), rbm_random.free_energy(v_pos3).item(), rbm_random.free_energy(v_pos4).item(), rbm_random.free_energy(v_pos5).item())\n",
    "print(rbm_trained.free_energy(v_neg).item(), rbm_trained.free_energy(v_pos1).item(), rbm_trained.free_energy(v_pos2).item(), rbm_trained.free_energy(v_pos3).item(), rbm_trained.free_energy(v_pos4).item(), rbm_trained.free_energy(v_pos5).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 1., 1., 0.]]), 783)\n",
      "trained (tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 1., 1., 0.]]), 755)\n",
      "random (tensor([[0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1.,\n",
      "         0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1.]]), 789)\n"
     ]
    }
   ],
   "source": [
    "from utils import CNFFormula, count_satisfied_clauses\n",
    "from models.rbm import formulaRBM\n",
    "import torch\n",
    "\n",
    "with open('./wcnfdata/brock200_3.clq.wcnf', 'r') as f:\n",
    "    formula_str = f.read()\n",
    "formula = CNFFormula(formula_str)\n",
    "n_visible = formula.num_vars\n",
    "v_init = torch.bernoulli(torch.full((1, n_visible), 0.5, device=\"cuda\"))\n",
    "\n",
    "print(count_satisfied_clauses(formula, v_init.cpu()))\n",
    "\n",
    "rbm_trained = formulaRBM(formula,\n",
    "                         num_epochs=10000,\n",
    "                         device=\"cuda\")\n",
    "\n",
    "v = v_init.clone()\n",
    "for _ in range(100):\n",
    "    h_sample, _ = rbm_trained.sample_h_given_v(v)\n",
    "    v_sample, p_v_given_h = rbm_trained.sample_v_given_h(h_sample)\n",
    "    v = v_sample\n",
    "\n",
    "    # plot the F_s each step\n",
    "\n",
    "print(f\"trained {count_satisfied_clauses(formula, v.cpu())}\")\n",
    "v1 = v.clone()\n",
    "\n",
    "\n",
    "rbm_random = formulaRBM(formula,\n",
    "                        num_epochs=0,\n",
    "                        device=\"cuda\")\n",
    "\n",
    "v = v_init.clone()\n",
    "for _ in range(100):\n",
    "    h_sample, _ = rbm_random.sample_h_given_v(v)\n",
    "    v_sample, p_v_given_h = rbm_random.sample_v_given_h(h_sample)\n",
    "    v = v_sample\n",
    "print(f\"random {count_satisfied_clauses(formula, v.cpu())}\")\n",
    "\n",
    "v2 = v.clone()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([819.0408], device='cuda:0')\n",
      "tensor([1106.0283], device='cuda:0')\n",
      "tensor([772.8834], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(rbm_trained.free_energy(v_init))\n",
    "print(rbm_trained.free_energy(v1))\n",
    "print(rbm_trained.free_energy(v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training consistently leads to worse results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.9978, -3.1180,  0.0000,  0.0000, -2.3297,  3.9816, -2.4516,  2.3680,\n",
      "          1.0588,  2.0557,  1.2671, -4.1289,  2.4115],\n",
      "        [-1.1680,  6.0914, -1.9978,  3.1180,  5.4193, -1.5878, -1.0259,  4.1523,\n",
      "         -0.5033, -1.9225, -0.8347, -0.4400, -2.4606],\n",
      "        [ 0.0000,  0.0000, -1.1680,  6.0914, -1.1188, -1.4281,  5.4664, -0.6311,\n",
      "         -0.3739,  4.0862, -0.5167, -0.5561, -1.6324],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.6017,\n",
      "          3.5612, -2.3678,  3.1999, -0.8163, -2.3878],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.3785,\n",
      "         -0.1678, -0.7122, -0.6422, -0.3692,  4.1957]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from utils import CNFFormula\n",
    "from models.rbm import formulaRBM\n",
    "import torch\n",
    "\n",
    "with open('./test02.cnf', 'r') as f:\n",
    "    formula_str = f.read()\n",
    "formula = CNFFormula(formula_str)\n",
    "\n",
    "rbm_trained = formulaRBM(formula,\n",
    "                         num_epochs=10000,\n",
    "                         device=\"cuda\")\n",
    "\n",
    "print(rbm_trained.W_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.9978, -3.1180],\n",
      "        [-1.1680,  6.0914]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-5.6222, -5.4554], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.9735, -0.0620], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "from models.rbm import clauseRBM\n",
    "\n",
    "rbm_len2 = clauseRBM(2, F_s=-1.0, num_epochs=10000, lr=0.01, device='cuda')\n",
    "\n",
    "print(rbm_len2.W)\n",
    "print(rbm_len2.b)\n",
    "print(rbm_len2.d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2500, -0.7500, -0.7500, -1.2500])\n",
      "tensor([[-3.3325e-08,  1.1686e-08],\n",
      "        [-3.3325e-08,  1.1686e-08]])\n",
      "tensor([0.5000, 0.5000])\n",
      "tensor([-2.0163, -2.0163])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import time\n",
    "from utils import CNFFormula, count_satisfied_clauses, unit_propagation\n",
    "class clauseRBM_symmetric(nn.Module):\n",
    "    def __init__(self, clause_length=2, F_s=-1.0, num_epochs=20000, lr=0.1, verbose=False):\n",
    "        super(clauseRBM_symmetric, self).__init__()\n",
    "        self.n_visible = clause_length\n",
    "        self.n_hidden = clause_length if clause_length <= 3 else clause_length + 1\n",
    "\n",
    "        # self.train_clause_rbm(F_s=F_s, num_epochs=num_epochs, lr=lr)\n",
    "        self.W = torch.Tensor([[-3.3325e-08,  1.1686e-08],[-3.3325e-08,  1.1686e-08]])\n",
    "        self.d = torch.Tensor([0.5 , 0.5])\n",
    "        self.b = torch.Tensor([-2.0163, -2.0163])\n",
    "    \n",
    "    def train_clause_rbm(self, F_s=-1.0, num_epochs=30000, lr=0.1):\n",
    "        \n",
    "        self.W = nn.Parameter(torch.randn(1, self.n_hidden) * 0.1)\n",
    "        self.d = nn.Parameter(torch.zeros(1))\n",
    "        self.b = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.01)\n",
    "\n",
    "        # Generate all possible binary vectors\n",
    "        n_samples = 2 ** self.n_visible\n",
    "        v_vectors = torch.tensor([ [int(x) for x in bin(i)[2:].zfill(self.n_visible)] for i in range(n_samples)], dtype=torch.float32)\n",
    "        # print(v_vectors)\n",
    "        # Target free energies\n",
    "        F_targets = torch.zeros(n_samples)\n",
    "        for i in range(n_samples):\n",
    "            if v_vectors[i].sum() == 0:\n",
    "                F_targets[i] = 0.0\n",
    "            else:\n",
    "                F_targets[i] = F_s\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            optimizer.zero_grad()\n",
    "            F_v = self.free_energy(v_vectors)\n",
    "            loss = torch.mean((F_v - F_targets) ** 2)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (epoch+1) % 1000 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "        \n",
    "    \n",
    "    def free_energy(self, v):\n",
    "        # v: batch_size x n_visible\n",
    "        v_term = torch.sum(self.d * v, dim=1)\n",
    "        # print(v_term)\n",
    "        # print(self.W)\n",
    "        W_expanded = self.W.expand(v.shape[1], self.n_hidden)\n",
    "        # print(W_expanded)\n",
    "        pre_activation = self.b + torch.matmul(v, W_expanded)\n",
    "        # print(pre_activation)\n",
    "        h_term = torch.sum(torch.log1p(torch.exp(pre_activation)), dim=1)\n",
    "        F_v = -v_term - h_term\n",
    "        # print(F_v)\n",
    "        return F_v  # batch_size\n",
    "    \n",
    "rbm = clauseRBM_symmetric()\n",
    "print(rbm.free_energy(torch.tensor([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])))\n",
    "print(rbm.W)\n",
    "print(rbm.d)\n",
    "print(rbm.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class formulaRBM_symmetric:\n",
    "    def __init__(self, \n",
    "                 formula: CNFFormula, \n",
    "                 device='cpu'):\n",
    "\n",
    "        literal_clauses = formula.to_literal_form()\n",
    "        n_visible_total = formula.num_vars\n",
    "        n_hidden_total = 0\n",
    "        for clause in literal_clauses:\n",
    "            clause_length = len(clause)\n",
    "            n_hidden = clause_length if clause_length <= 3 else clause_length + 1\n",
    "            n_hidden_total += n_hidden\n",
    "\n",
    "        self.W_full = torch.zeros(n_visible_total, n_hidden_total, device=device)\n",
    "        self.b_full = torch.zeros(n_hidden_total, device=device)\n",
    "        self.d_full = torch.zeros(n_visible_total, device=device)\n",
    "        \n",
    "        curr_hidden_idx = 0\n",
    "\n",
    "        for clause in literal_clauses:\n",
    "            # clause: list of (var_idx, is_negated)\n",
    "            variable_indices, signs = zip(*clause)\n",
    "            # variable_indices = [var_idx for (var_idx, is_negated) in clause]\n",
    "            # signs = [is_negated for (var_idx, is_negated) in clause]\n",
    "            \n",
    "            clause_length = len(clause)\n",
    "            n_visible = clause_length\n",
    "            n_hidden = n_visible if n_visible <= 3 else n_visible + 1\n",
    "            \n",
    "            # Get pre-trained RBM for this clause length\n",
    "            rbm = clauseRBM_symmetric(clause_length)\n",
    "            \n",
    "            # Adjust W and b according to signs\n",
    "            Lambda = torch.tensor([-1.0 if is_negated else 1.0 for is_negated in signs], device=device)\n",
    "            W = rbm.W.detach()  # size n_visible x n_hidden\n",
    "            b = rbm.b.detach()  # size n_hidden\n",
    "            d = rbm.d.detach()  # size n_visible\n",
    "            \n",
    "            W_prime = Lambda.unsqueeze(1) * W  # size n_visible x n_hidden\n",
    "            b_prime = b + 0.5 * torch.matmul((1 - Lambda), W)\n",
    "            \n",
    "            # Map W_prime into W_full\n",
    "            for local_var_idx, var_idx in enumerate(variable_indices):\n",
    "                self.W_full[var_idx, curr_hidden_idx : curr_hidden_idx + n_hidden] += W_prime[local_var_idx, :]\n",
    "                self.d_full[var_idx] += d[local_var_idx]\n",
    "            # Map b_prime into b_full\n",
    "            self.b_full[curr_hidden_idx : curr_hidden_idx + n_hidden] = b_prime\n",
    "            \n",
    "            curr_hidden_idx += n_hidden\n",
    "\n",
    "        self.n_visible = n_visible_total\n",
    "        self.n_hidden = n_hidden_total\n",
    "        self.device = device\n",
    "        \n",
    "    def sample_h_given_v(self, v):\n",
    "        # v: batch_size x n_visible\n",
    "        p_h_given_v = torch.sigmoid(self.b_full + torch.matmul(v, self.W_full))\n",
    "        h_sample = torch.bernoulli(p_h_given_v)\n",
    "        return h_sample, p_h_given_v\n",
    "    \n",
    "    def sample_v_given_h(self, h):\n",
    "        # h: batch_size x n_hidden\n",
    "        p_v_given_h = torch.sigmoid(self.d_full + torch.matmul(h, self.W_full.t()))\n",
    "        v_sample = torch.bernoulli(p_v_given_h)\n",
    "        return v_sample, p_v_given_h\n",
    "    \n",
    "    def gibbs_sampling(self, v_init, k=1):\n",
    "        v = v_init\n",
    "        for _ in range(k):\n",
    "            h_sample, _ = self.sample_h_given_v(v)\n",
    "            v_sample, _ = self.sample_v_given_h(h_sample)\n",
    "            v = v_sample\n",
    "        return v\n",
    "\n",
    "    def free_energy(self, v):\n",
    "        return torch.sum(v * self.d_full, dim=1) + torch.sum(torch.log(1 + torch.exp(self.b_full + torch.matmul(v, self.W_full))), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100, Best: 787, current: 787, formula size: 1038, Elapsed Time: 3.95s\n",
      "Step 200, Best: 787, current: 787, formula size: 1038, Elapsed Time: 7.69s\n",
      "Step 300, Best: 787, current: 787, formula size: 1038, Elapsed Time: 11.50s\n",
      "Step 400, Best: 787, current: 787, formula size: 1038, Elapsed Time: 15.21s\n",
      "Step 500, Best: 787, current: 787, formula size: 1038, Elapsed Time: 18.94s\n",
      "Step 600, Best: 787, current: 787, formula size: 1038, Elapsed Time: 22.76s\n",
      "Step 700, Best: 787, current: 787, formula size: 1038, Elapsed Time: 26.37s\n",
      "Step 800, Best: 787, current: 787, formula size: 1038, Elapsed Time: 29.79s\n",
      "Step 900, Best: 787, current: 787, formula size: 1038, Elapsed Time: 33.35s\n",
      "Step 1000, Best: 787, current: 787, formula size: 1038, Elapsed Time: 37.06s\n",
      "Step 1100, Best: 787, current: 787, formula size: 1038, Elapsed Time: 40.70s\n",
      "Step 1200, Best: 787, current: 787, formula size: 1038, Elapsed Time: 44.51s\n",
      "Step 1300, Best: 787, current: 787, formula size: 1038, Elapsed Time: 48.07s\n",
      "Step 1400, Best: 787, current: 787, formula size: 1038, Elapsed Time: 51.34s\n",
      "Step 1500, Best: 787, current: 787, formula size: 1038, Elapsed Time: 55.12s\n",
      "Step 1600, Best: 787, current: 787, formula size: 1038, Elapsed Time: 58.86s\n",
      "Time limit reached: 60 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('./wcnfdata/brock200_3.clq.wcnf', 'r') as f:\n",
    "    formula_str = f.read()\n",
    "formula = CNFFormula(formula_str)\n",
    "\n",
    "rbm_trained = formulaRBM_symmetric(formula)\n",
    "batch_size = 1\n",
    "heuristic_interval = 100\n",
    "max_time = 60\n",
    "\n",
    "n_visible = rbm_trained.n_visible\n",
    "\n",
    "# Initialize v to random assignment\n",
    "v = torch.bernoulli(torch.full((batch_size, n_visible), 0.5))\n",
    "\n",
    "best_v, best_num_satisfied = count_satisfied_clauses(formula, v.cpu())\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Unit propagation heuristic: Initialize moving averages of _i\n",
    "nu_i = torch.zeros(n_visible)\n",
    "alpha = 0.9  # Decay factor for moving average\n",
    "step = 0\n",
    "while True:\n",
    "    step += 1\n",
    "    \n",
    "    # Sample h given v\n",
    "    h_sample, _ = rbm_trained.sample_h_given_v(v)\n",
    "    # Sample v given h\n",
    "    v_sample, p_v_given_h = rbm_trained.sample_v_given_h(h_sample)\n",
    "    v = v_sample\n",
    "    \n",
    "    # Update moving averages of _i\n",
    "    rho_i = p_v_given_h.mean(dim=0)  # size n_visible\n",
    "    nu_i = alpha * nu_i + (1 - alpha) * (rho_i * (1 - rho_i))\n",
    "    \n",
    "    # Every heuristic_interval steps, apply heuristic\n",
    "    if step % heuristic_interval == 0:\n",
    "        # Rank variables by _i\n",
    "        _, indices = torch.sort(nu_i)\n",
    "        num_vars_to_unassign = n_visible // 2\n",
    "        vars_to_unassign = indices[:num_vars_to_unassign]\n",
    "        # Set these variables to unassigned (-1)\n",
    "\n",
    "        assignments = []\n",
    "        for i in range(batch_size):\n",
    "            assignment = v[i].clone()\n",
    "            assignment[vars_to_unassign] = -1  # Unassign variables\n",
    "            # Apply unit propagation\n",
    "            assignment = unit_propagation(formula, assignment.cpu())\n",
    "            assignments.append(assignment)\n",
    "        v = torch.stack(assignments, dim=0)  # Shape: (B, N)\n",
    "        # Reset moving averages\n",
    "        nu_i.zero_()\n",
    "    \n",
    "    # Evaluate current assignment\n",
    "    current_best_v, current_best_num_satisfied = count_satisfied_clauses(formula, v)\n",
    "    if current_best_num_satisfied > best_num_satisfied:\n",
    "        best_num_satisfied = current_best_num_satisfied\n",
    "        best_v = current_best_v.clone()\n",
    "    \n",
    "    if best_num_satisfied == formula.num_clauses:\n",
    "        print(f\"Solved in {step} steps\")\n",
    "        break\n",
    "    \n",
    "    # Optional: print progress\n",
    "    if step % 100 == 0:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Step {step}, Best: {best_num_satisfied}, current: {current_best_num_satisfied}, formula size: {formula.num_clauses}, Elapsed Time: {elapsed_time:.2f}s\")\n",
    "    \n",
    "    # Check time limit\n",
    "    if time.time() - start_time >= max_time:\n",
    "        print(f\"Time limit reached: {max_time} seconds\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import CNFFormula\n",
    "from models.rbm import clauseRBM\n",
    "import torch\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "with open(\"wcnfdata/brock200_3.clq.wcnf\", 'r') as f:\n",
    "    cnf_str = f.read()\n",
    "\n",
    "formula = CNFFormula(cnf_str)\n",
    "formula.padding()\n",
    "rbm = clauseRBM(formula.max_clause_length)\n",
    "\n",
    "def gather_and_count(v, Q, polarity):\n",
    "    c = torch.einsum('bv,ckv->bck', v, Q) # (batch_size, num_clause, max_len_clause)\n",
    "    return c, (((polarity + c) == 2) + ((polarity + c) == -1)).any(dim=-1).sum(dim=-1)\n",
    "\n",
    "def time_remaining(start_time, time_limit):\n",
    "    if time.time() - start_time > time_limit:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def construct_Q(T, num_var):\n",
    "    Q = torch.zeros(T.shape[0], T.shape[1], num_var) #(num_clause, len_clause, num_var)\n",
    "    for clause_idx, clause in enumerate(T):\n",
    "        for lit_idx, lit in enumerate(clause):\n",
    "            if lit != 0:\n",
    "                Q[clause_idx, lit_idx, torch.abs(lit)-1] = 1.0\n",
    "    return Q\n",
    "\n",
    "def rbmsat(W_c, b_c, T, B, N, seed, time_limit, upp=100, upw=1, alpha=0.1): # B is batch size, N is the total number of variables\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Initialize variables\n",
    "    s_max = 0 # initially 0 clauses are satisfied\n",
    "    v = torch.bernoulli(torch.ones(B, N, device=W_c.device) * 0.5)  # sample random inits \n",
    "\n",
    "    # Setup tensors\n",
    "    polarity = torch.sign(T)  # strictly {1, -1}, not 0 (-1 2 -3) -> (-1 1 -1)\n",
    "    Q = construct_Q(T, formula.num_vars) # C * K * N    (num_clause, max_len_clause, num_var)\n",
    "    W = torch.einsum('ck,kh->ckh', polarity, W_c) # polarity.unsqueeze(1) * W_c.unsqueeze(2) # element-wise multiplication\n",
    "    b = b_c.repeat([T.shape[0], 1]) + torch.mm((1 - polarity) / 2, W_c)\n",
    "    \n",
    "    # t, d = 1, -1\n",
    "    step = 0\n",
    "    s_max_list = []\n",
    "    best_v = torch.ones(N, device=W_c.device)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    while time_remaining(start_time, time_limit):\n",
    "        step += 1\n",
    "            \n",
    "        c, s = gather_and_count(v, Q, polarity)\n",
    "        s_max_step, idx_max = s.max(dim=0)\n",
    "        if s_max_step.item() > s_max:\n",
    "            s_max = s_max_step.item()\n",
    "            best_v = v[idx_max].clone()\n",
    "            s_max_list.append((step, s_max))\n",
    "            print(f\"new s_max found {s_max} at step {step}\")\n",
    "\n",
    "        h_logits = b + torch.einsum('bck,ckh->bch', c, W) # 'bck,chk->bhk'  bck,ckh->bch\n",
    "        h = torch.bernoulli(torch.sigmoid(h_logits))\n",
    "        ro = torch.sigmoid(torch.einsum('bch,ckh,ckv->bv', h, W, Q)) # 'bhk,chk,ckv->bv'\n",
    "        v = torch.bernoulli(ro)\n",
    "        \n",
    "    return s_max_list, best_v\n",
    "\n",
    "T = torch.tensor(formula.clauses)\n",
    "\n",
    "alpha = 0.1\n",
    "upp = 100\n",
    "upw = 1\n",
    "\n",
    "s_max, best_v = rbmsat(rbm.W, rbm.b, T, 128, formula.num_vars, 42, 300, upp, upw, alpha)\n",
    "print(best_v)\n",
    "x_ax, y_ax = zip(*s_max)\n",
    "plt.step(x_ax,y_ax)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
